{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Welcome to UP42 - the developer platform and marketplace for geospatial data and applications: Access commercial and public geospatial datasets , e.g. satellite, aerial, vector, IoT & more. Construct workflows from modular building blocks for geospatial data, processing & analysis. Bring your own algorithms via custom blocks. Integrates into your own product via the API and Python package. Scale your geospatial applications with the powerful UP42 cloud infrastructure . Related links \u00b6 UP42 Website UP42 Documentation Github Repositories UP42 docs repository UP42 Python package Support \u00b6 Please reach out to us via email - support@up42.com - we are happy to help you with any questions or issues!","title":"Overview"},{"location":"#introduction","text":"Welcome to UP42 - the developer platform and marketplace for geospatial data and applications: Access commercial and public geospatial datasets , e.g. satellite, aerial, vector, IoT & more. Construct workflows from modular building blocks for geospatial data, processing & analysis. Bring your own algorithms via custom blocks. Integrates into your own product via the API and Python package. Scale your geospatial applications with the powerful UP42 cloud infrastructure .","title":"Introduction"},{"location":"#related-links","text":"UP42 Website UP42 Documentation Github Repositories UP42 docs repository UP42 Python package","title":"Related links"},{"location":"#support","text":"Please reach out to us via email - support@up42.com - we are happy to help you with any questions or issues!","title":"Support"},{"location":"01_quickstart/","text":"Quickstart \u00b6 % load_ext autoreload % autoreload 2 import up42 API-Structure \u00b6 The UP42 Python Api uses seven object classes, representing the hierachical structure of UP42: Api > Project > Workflow > Job > JobTask and Catalog & Tools . Each object provides the full functionality at that specific level and can spawn elements of one level below, e.g. workflow = Project().create_workflow() job = workflow.create_and_run_job() Usually the user starts with the Api object, then spawns objects of a lower level (e.g. initializes a project, creates a new workflow, runs a job etc.). To access a lower-level object directly, e.g. a job that was already run on UP42 initialize the object directly via api.initialize_job(job_id='123456789') . 30 seconds example \u00b6 Runs a workflow consisting of Sentinel-2 Streaming and image sharpening. # Authentificate: Gets the the project credentials saved in the config.json file. api = up42 . Api ( cfg_file = \"config.json\" ) project = api . initialize_project () # Create a workflow & add blocks/tasks to the workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) blocks = api . get_blocks ( basic = True ) input_tasks = [ blocks [ 'sobloo-s2-l1c-aoiclipped' ], blocks [ 'sharpening' ]] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow to run it. aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) input_parameters = workflow . construct_parameter ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2020-01-01\" , end_date = \"2020-01-03\" , limit = 1 ) print ( input_parameters ) # Run the workflow as a job job = workflow . create_and_run_job ( input_parameters = input_parameters ) job . track_status () # Plot the scene quicklooks. job . download_quicklook () job . plot_quicklook () # Plot & analyse the results. results_fp = job . download_result () print ( results_fp ) job . plot_result () job . map_result ()","title":"Quickstart"},{"location":"01_quickstart/#quickstart","text":"% load_ext autoreload % autoreload 2 import up42","title":"Quickstart"},{"location":"01_quickstart/#api-structure","text":"The UP42 Python Api uses seven object classes, representing the hierachical structure of UP42: Api > Project > Workflow > Job > JobTask and Catalog & Tools . Each object provides the full functionality at that specific level and can spawn elements of one level below, e.g. workflow = Project().create_workflow() job = workflow.create_and_run_job() Usually the user starts with the Api object, then spawns objects of a lower level (e.g. initializes a project, creates a new workflow, runs a job etc.). To access a lower-level object directly, e.g. a job that was already run on UP42 initialize the object directly via api.initialize_job(job_id='123456789') .","title":"API-Structure"},{"location":"01_quickstart/#30-seconds-example","text":"Runs a workflow consisting of Sentinel-2 Streaming and image sharpening. # Authentificate: Gets the the project credentials saved in the config.json file. api = up42 . Api ( cfg_file = \"config.json\" ) project = api . initialize_project () # Create a workflow & add blocks/tasks to the workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) blocks = api . get_blocks ( basic = True ) input_tasks = [ blocks [ 'sobloo-s2-l1c-aoiclipped' ], blocks [ 'sharpening' ]] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow to run it. aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) input_parameters = workflow . construct_parameter ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2020-01-01\" , end_date = \"2020-01-03\" , limit = 1 ) print ( input_parameters ) # Run the workflow as a job job = workflow . create_and_run_job ( input_parameters = input_parameters ) job . track_status () # Plot the scene quicklooks. job . download_quicklook () job . plot_quicklook () # Plot & analyse the results. results_fp = job . download_result () print ( results_fp ) job . plot_result () job . map_result ()","title":"30 seconds example"},{"location":"02_objects/","text":"Objects \u00b6 This section gives an overview of the the available functionality at each level of the UP42 API, and how to initialize the 7 objects directly (e.g. if you don't want to create a new workflow and job, but want to access an already existing job on UP42). Tools \u00b6 The tools' functionalities can be accessed from each up42 object. Example functions: .read_vector_file , .get_example_aoi , .draw_aoi , plot_coverage , plot_quicklook , plot_result # Can be accessed from each up42 object, e.g. api . get_example_aoi () workflow . get_example_aoi () job . get_example_aoi () Api \u00b6 Example functions: .get_blocks , .get_block_details , .delete_custom_block , .validate_manifest , .initialize_project api = up42 . Api ( cfg_file = \"config.json\" , env = \"dev\" ) api Catalog \u00b6 Example functions: .construct_parameter , .search , .download_quicklooks catalog = api . initialize_catalog () catalog Project \u00b6 Example functions: .get_workflows , .create_workflow , .get_project_settings , .update_project_settings , .update_project_settings , .get_project_api_key UP42_PROJECT_ID = \"8956d18d-33bc-47cb-93bd-0055ff21da8f\" project = api . initialize_project () project Workflow \u00b6 Example functions: .add_workflow_tasks , .get_parameters_info , .construct_parameters , .get_jobs , .create_and_run_job , .get_workflow_tasks , .add_workflow_tasks , .update_workflow , .delete_workflow , .update_name , .delete Alltough most often used from the workflow object, a few generic aoi functions are useable with every object: .get_example_aoi , .draw_aoi , .read_vector_file UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = api . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow Job \u00b6 Example functions: .get_status , .track_status , .cancel_job , .get_results , .get_logs , .get_quicklook , .download_results , .plot_results , .map_results , .upload_results_to_bucket , .get_job_tasks , .get_job_tasks_results UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = api . initialize_job ( job_id = UP42_JOB_ID ) job JobTask \u00b6 Example functions: .get_result_json , .download_results , .get_quicklooks UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = api . initialize_jobtask ( job_task_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) jobtask","title":"Objects"},{"location":"02_objects/#objects","text":"This section gives an overview of the the available functionality at each level of the UP42 API, and how to initialize the 7 objects directly (e.g. if you don't want to create a new workflow and job, but want to access an already existing job on UP42).","title":"Objects"},{"location":"02_objects/#tools","text":"The tools' functionalities can be accessed from each up42 object. Example functions: .read_vector_file , .get_example_aoi , .draw_aoi , plot_coverage , plot_quicklook , plot_result # Can be accessed from each up42 object, e.g. api . get_example_aoi () workflow . get_example_aoi () job . get_example_aoi ()","title":"Tools"},{"location":"02_objects/#api","text":"Example functions: .get_blocks , .get_block_details , .delete_custom_block , .validate_manifest , .initialize_project api = up42 . Api ( cfg_file = \"config.json\" , env = \"dev\" ) api","title":"Api"},{"location":"02_objects/#catalog","text":"Example functions: .construct_parameter , .search , .download_quicklooks catalog = api . initialize_catalog () catalog","title":"Catalog"},{"location":"02_objects/#project","text":"Example functions: .get_workflows , .create_workflow , .get_project_settings , .update_project_settings , .update_project_settings , .get_project_api_key UP42_PROJECT_ID = \"8956d18d-33bc-47cb-93bd-0055ff21da8f\" project = api . initialize_project () project","title":"Project"},{"location":"02_objects/#workflow","text":"Example functions: .add_workflow_tasks , .get_parameters_info , .construct_parameters , .get_jobs , .create_and_run_job , .get_workflow_tasks , .add_workflow_tasks , .update_workflow , .delete_workflow , .update_name , .delete Alltough most often used from the workflow object, a few generic aoi functions are useable with every object: .get_example_aoi , .draw_aoi , .read_vector_file UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = api . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow","title":"Workflow"},{"location":"02_objects/#job","text":"Example functions: .get_status , .track_status , .cancel_job , .get_results , .get_logs , .get_quicklook , .download_results , .plot_results , .map_results , .upload_results_to_bucket , .get_job_tasks , .get_job_tasks_results UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = api . initialize_job ( job_id = UP42_JOB_ID ) job","title":"Job"},{"location":"02_objects/#jobtask","text":"Example functions: .get_result_json , .download_results , .get_quicklooks UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = api . initialize_jobtask ( job_task_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) jobtask","title":"JobTask"},{"location":"03_catalog/","text":"Catalog Search \u00b6 % load_ext autoreload % autoreload 2 import up42 api = up42 . Api ( cfg_file = \"config.json\" ) catalog = api . initialize_catalog () catalog Search available scenes within aoi \u00b6 #aoi = api.read_vector_file(\"data/aoi_berlin.geojson\", # as_dataframe=True) aoi = api . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) aoi catalog = up42 . Catalog ( api = api ) search_paramaters = catalog . construct_parameter ( geometry = aoi , start_date = \"2014-01-01\" , end_date = \"2016-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 4 ) search_results = catalog . search ( search_paramaters = search_paramaters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" ) Quicklooks \u00b6 catalog . download_quicklook ( image_ids = search_results . id . to_list (), provider = \"oneatlas\" , out_dir = None ) catalog . plot_quicklook ( figsize = ( 20 , 20 ))","title":"Catalog Search"},{"location":"03_catalog/#catalog-search","text":"% load_ext autoreload % autoreload 2 import up42 api = up42 . Api ( cfg_file = \"config.json\" ) catalog = api . initialize_catalog () catalog","title":"Catalog Search"},{"location":"03_catalog/#search-available-scenes-within-aoi","text":"#aoi = api.read_vector_file(\"data/aoi_berlin.geojson\", # as_dataframe=True) aoi = api . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) aoi catalog = up42 . Catalog ( api = api ) search_paramaters = catalog . construct_parameter ( geometry = aoi , start_date = \"2014-01-01\" , end_date = \"2016-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 4 ) search_results = catalog . search ( search_paramaters = search_paramaters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" )","title":"Search available scenes within aoi"},{"location":"03_catalog/#quicklooks","text":"catalog . download_quicklook ( image_ids = search_results . id . to_list (), provider = \"oneatlas\" , out_dir = None ) catalog . plot_quicklook ( figsize = ( 20 , 20 ))","title":"Quicklooks"},{"location":"04_typical_usage/","text":"Typical Usage \u00b6 This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps. % load_ext autoreload % autoreload 2 import up42 Authentificate & access project \u00b6 api = up42 . Api ( cfg_file = \"config.json\" ) project = api . initialize_project () Get information about the available blocks to later construct your workflow. api . get_blocks ( basic = True ) Create or access the workflow \u00b6 You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = False ) workflow # Add workflow tasks - simple version input_tasks = [ \"a2daaab4-196d-4226-a018-a810444dcad1\" , \"4ed70368-d4e1-4462-bef6-14e768049471\" ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access the existing workflow the id (has to exist within the accessed project). UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = api . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow Select the aoi \u00b6 There are multiple ways to select an aoi, you can: - Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. - Use .draw_aoi() to draw the aoi and export it as a geojson. - Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. - Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) aoi = workflow . get_example_aoi ( location = \"Berlin\" ) workflow . draw_aoi () Select the workflow parameters \u00b6 There are also multiple ways to construct the workflow input parameters, you can: - Provide the parameters directly in code as a json string. - Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. - Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameter_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameter ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) input_parameters # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters Run the workflow & download results \u00b6 # Run the workflow with the specified parameters. job = workflow . create_and_run_job ( input_parameters = input_parameters , track_status = True ) Download & Display results \u00b6 # Download job result (default downloads to Desktop). Only works after download is finished. results_fp = job . download_result ( out_dir = None ) job . plot_result () job . map_result ()","title":"Typical Usage"},{"location":"04_typical_usage/#typical-usage","text":"This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps. % load_ext autoreload % autoreload 2 import up42","title":"Typical Usage"},{"location":"04_typical_usage/#authentificate-access-project","text":"api = up42 . Api ( cfg_file = \"config.json\" ) project = api . initialize_project () Get information about the available blocks to later construct your workflow. api . get_blocks ( basic = True )","title":"Authentificate &amp; access project"},{"location":"04_typical_usage/#create-or-access-the-workflow","text":"You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = False ) workflow # Add workflow tasks - simple version input_tasks = [ \"a2daaab4-196d-4226-a018-a810444dcad1\" , \"4ed70368-d4e1-4462-bef6-14e768049471\" ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access the existing workflow the id (has to exist within the accessed project). UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = api . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow","title":"Create or access the workflow"},{"location":"04_typical_usage/#select-the-aoi","text":"There are multiple ways to select an aoi, you can: - Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. - Use .draw_aoi() to draw the aoi and export it as a geojson. - Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. - Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) aoi = workflow . get_example_aoi ( location = \"Berlin\" ) workflow . draw_aoi ()","title":"Select the aoi"},{"location":"04_typical_usage/#select-the-workflow-parameters","text":"There are also multiple ways to construct the workflow input parameters, you can: - Provide the parameters directly in code as a json string. - Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. - Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameter_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameter ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) input_parameters # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters","title":"Select the workflow parameters"},{"location":"04_typical_usage/#run-the-workflow-download-results","text":"# Run the workflow with the specified parameters. job = workflow . create_and_run_job ( input_parameters = input_parameters , track_status = True )","title":"Run the workflow &amp; download results"},{"location":"04_typical_usage/#download-display-results","text":"# Download job result (default downloads to Desktop). Only works after download is finished. results_fp = job . download_result ( out_dir = None ) job . plot_result () job . map_result ()","title":"Download &amp; Display results"},{"location":"installation/","text":"Installation \u00b6 Optional : Create a virtual environment: mkvirtualenv --python = $( which python3.7 ) up42-py Install locally with systemlink (code changes are reflected): git clone git@github.com:up42/up42-py.git cd up42-py pip install -r requirements.txt pip install -e . Create a new project on UP42 . Create a config.json file with the project credentials . { \"project_id\" : \"...\" , \"project_api_key\" : \"...\" } Test it in Python! This will authentificate with the UP42 Server and get the project information. import up42 api = up42 . Api ( cfg_file = \"config.json\" ) project = api . initialize_project () print ( project ) Success! Continue with the Getting Started section!","title":"Installation"},{"location":"installation/#installation","text":"Optional : Create a virtual environment: mkvirtualenv --python = $( which python3.7 ) up42-py Install locally with systemlink (code changes are reflected): git clone git@github.com:up42/up42-py.git cd up42-py pip install -r requirements.txt pip install -e . Create a new project on UP42 . Create a config.json file with the project credentials . { \"project_id\" : \"...\" , \"project_api_key\" : \"...\" } Test it in Python! This will authentificate with the UP42 Server and get the project information. import up42 api = up42 . Api ( cfg_file = \"config.json\" ) project = api . initialize_project () print ( project ) Success! Continue with the Getting Started section!","title":"Installation"},{"location":"advanced/airports-parallel/","text":"Parallel jobs \u00b6 Example: Airport monitoring \u00b6 Get a Sentinel-2 clipped image for airports in a country. Run all jobs in parallel Visualize the results % load_ext autoreload % autoreload 2 import pandas as pd import geopandas as gpd from pathlib import Path import up42 10 random airports in Spain \u00b6 https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) airports = airports . sample ( 10 ) # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) # Buffer airport point locations by around 100m airports . geometry = airports . geometry . buffer ( 0.001 ) airports . iloc [ 0 ] . geometry Prepare UP42 workflows \u00b6 # Authentificate api = up42 . Api ( cfg_file = \"config.json\" env = \"dev\" ) project = api . initialize_project () project project . update_project_settings ( max_concurrent_jobs = 10 ) workflow = project . create_workflow ( \"workflow_demo_airplanes\" , use_existing = True ) workflow # Fill the workflow with tasks blocks = api . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ blocks [ selected_block ]]) workflow . get_workflow_tasks ( basic = True ) Run jobs in parallel \u00b6 # Run jobs in parallel jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameter ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . create_and_run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) # Download results: outdir = Path . cwd () out_filepaths = [] for job in jobs : fp = job . download_result ( out_dir = outdir / \"img\" ) out_filepaths . append ( fp [ 0 ]) print ( \"finished\" ) # Visualize downloaded results api . plot_result ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ())","title":"Parallel Jobs"},{"location":"advanced/airports-parallel/#parallel-jobs","text":"","title":"Parallel jobs"},{"location":"advanced/airports-parallel/#example-airport-monitoring","text":"Get a Sentinel-2 clipped image for airports in a country. Run all jobs in parallel Visualize the results % load_ext autoreload % autoreload 2 import pandas as pd import geopandas as gpd from pathlib import Path import up42","title":"Example: Airport monitoring"},{"location":"advanced/airports-parallel/#10-random-airports-in-spain","text":"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) airports = airports . sample ( 10 ) # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) # Buffer airport point locations by around 100m airports . geometry = airports . geometry . buffer ( 0.001 ) airports . iloc [ 0 ] . geometry","title":"10 random airports in Spain"},{"location":"advanced/airports-parallel/#prepare-up42-workflows","text":"# Authentificate api = up42 . Api ( cfg_file = \"config.json\" env = \"dev\" ) project = api . initialize_project () project project . update_project_settings ( max_concurrent_jobs = 10 ) workflow = project . create_workflow ( \"workflow_demo_airplanes\" , use_existing = True ) workflow # Fill the workflow with tasks blocks = api . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ blocks [ selected_block ]]) workflow . get_workflow_tasks ( basic = True )","title":"Prepare UP42 workflows"},{"location":"advanced/airports-parallel/#run-jobs-in-parallel","text":"# Run jobs in parallel jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameter ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . create_and_run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) # Download results: outdir = Path . cwd () out_filepaths = [] for job in jobs : fp = job . download_result ( out_dir = outdir / \"img\" ) out_filepaths . append ( fp [ 0 ]) print ( \"finished\" ) # Visualize downloaded results api . plot_result ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ())","title":"Run jobs in parallel"},{"location":"reference/api/","text":"Api class \u00b6 __init__ ( self , cfg_file = None , project_id = None , project_api_key = None , ** kwargs ) \u00b6 Show source code in up42/api.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , cfg_file : Union [ str , Path ] = None , project_id : str = None , project_api_key : str = None , ** kwargs , ): \"\"\" The Api class handles the authentication with UP42, can create projects, gives access to the blocks on the UP42 marketplace, handles environment settings and other generic UP42 functions. Info: Authentication is possible via the credentials of a specific project (project_id & project_api_key). To get your **project id** and **project api key**, follow the instructions in the docs installation chapter. Public methods: initialize_project, get_blocks, get_block_details, delete_custom_block, get_environments, create_environment, delete_environment, validate_manifest Args: cfg_file: File path to the cfg.json with {project_id: \"...\", project_api_key: \"...\"}. project_id: The unique identifier of the project. project_api_key: The project-specific API key. \"\"\" self . cfg_file = cfg_file self . project_id = project_id self . project_api_key = project_api_key self . env = \"com\" self . authenticate = True self . retry = True if kwargs is not None : try : self . env : str = kwargs [ \"env\" ] except KeyError : pass try : self . authenticate : bool = kwargs [ \"authenticate\" ] except KeyError : pass try : self . retry : bool = kwargs [ \"retry\" ] except KeyError : pass if self . authenticate : self . _find_credentials () self . _get_token () logger . info ( \"Authentication with UP42 successful!\" ) The Api class handles the authentication with UP42, can create projects, gives access to the blocks on the UP42 marketplace, handles environment settings and other generic UP42 functions. Info Authentication is possible via the credentials of a specific project (project_id & project_api_key). To get your project id and project api key , follow the instructions in the docs installation chapter. Public methods: initialize_project, get_blocks, get_block_details, delete_custom_block, get_environments, create_environment, delete_environment, validate_manifest Parameters Name Type Description Default cfg_file Union[str, pathlib.Path] File path to the cfg.json with None project_id str The unique identifier of the project. None project_api_key str The project-specific API key. None create_environment ( self , name , environment_variables = None ) \u00b6 Show source code in up42/api.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 def create_environment ( self , name : str , environment_variables : Dict = None ) -> Dict : \"\"\" Creates a new UP42 environments, used for separating storage of API keys etc. Args: name: Name of the new environment. environment_variables: The variables to add to the environment, see example. Returns: The json info of the newly created environment. Example: ```python environment_variables= {\"username\": \"up42\", \"password\": \"password\"} ``` \"\"\" existing_environment_names = [ env [ \"name\" ] for env in self . get_environments ()] if name in existing_environment_names : raise Exception ( \"An environment with the name %s already exists.\" , name ) payload = { \"name\" : name , \"secrets\" : environment_variables } url = f \" { self . _endpoint () } /environments\" response_json = self . _request ( request_type = \"POST\" , url = url , data = payload ) return response_json [ \"data\" ] Creates a new UP42 environments, used for separating storage of API keys etc. Parameters Name Type Description Default name str Name of the new environment. required environment_variables Dict The variables to add to the environment, see example. None Returns Type Description Dict The json info of the newly created environment. Example environment_variables = { \"username\" : \"up42\" , \"password\" : \"password\" } delete_environment ( self , environment_id ) \u00b6 Show source code in up42/api.py 415 416 417 418 419 420 421 422 423 424 def delete_environment ( self , environment_id : str ) -> None : \"\"\" Deletes a specific environment. Args: environment_id: The id of the environment to delete. See also get_environments. \"\"\" url = f \" { self . _endpoint () } /environments/ { environment_id } \" self . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted environment: %s \" , environment_id ) Deletes a specific environment. Parameters Name Type Description Default environment_id str The id of the environment to delete. See also get_environments. required get_block_details ( self , block_id , as_dataframe = False ) \u00b6 Show source code in up42/api.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific (public or custom) block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" try : url = f \" { self . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . _request ( request_type = \"GET\" , url = url ) except ( requests . exceptions . HTTPError , RetryError ): url = f \" { self . _endpoint () } /users/me/blocks/ { block_id } \" # custom blocks response_json = self . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json Gets the detailed information about a specific (public or custom) block from the server, includes all manifest.json and marketplace.json contents. Parameters Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns Type Description Dict A dict of the block details metadata for the specific block. get_blocks ( self , block_type = None , basic = True , as_dataframe = False ) \u00b6 Show source code in up42/api.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ Dict , List [ Dict ]]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass url = f \" { self . _endpoint () } /blocks\" response_json = self . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting basic information, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json Gets a list of all public blocks on the marketplace. Parameters Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version True as_dataframe Returns a dataframe instead of json (default). False Returns Type Description Union[Dict, List[Dict]] A list of the public blocks and their metadata. Optional a simpler version dict. get_environments ( self , as_dataframe = False ) \u00b6 Show source code in up42/api.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def get_environments ( self , as_dataframe = False ) -> Dict : \"\"\" Gets all existing UP42 environments, used for separating storage of API keys etc. Args: as_dataframe: Returns a dataframe instead of json (default). Returns: The environments as json info. \"\"\" url = f \" { self . _endpoint () } /environments\" response_json = self . _request ( request_type = \"GET\" , url = url ) environments_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame ( environments_json ) else : return environments_json Gets all existing UP42 environments, used for separating storage of API keys etc. Parameters Name Type Description Default as_dataframe Returns a dataframe instead of json (default). False Returns Type Description Dict The environments as json info. initialize_catalog ( self , backend = 'ONE_ATLAS' ) \u00b6 Show source code in up42/api.py 269 270 271 def initialize_catalog ( self , backend : str = \"ONE_ATLAS\" ) -> \"up42.Catalog\" : \"\"\"Directly returns a catalog object.\"\"\" return up42 . Catalog ( api = self , backend = backend ) Directly returns a catalog object. initialize_job ( self , job_id , order_ids = [ '' ]) \u00b6 Show source code in up42/api.py 279 280 281 282 def initialize_job ( self , job_id , order_ids : List [ str ] = [ \"\" ]) -> \"up42.Job\" : \"\"\"Directly returns a Job object (has to exist on UP42).\"\"\" job = up42 . Job ( api = self , job_id = job_id , project_id = self . project_id , order_ids = order_ids ) return job Directly returns a Job object (has to exist on UP42). initialize_jobtask ( self , job_task_id , job_id ) \u00b6 Show source code in up42/api.py 284 285 286 287 288 def initialize_jobtask ( self , job_task_id , job_id ) -> \"up42.Job\" : \"\"\"Directly returns a JobTask object (has to exist on UP42).\"\"\" jobtask = up42 . JobTask ( api = self , job_task_id = job_task_id , job_id = job_id , project_id = self . project_id ) return jobtask Directly returns a JobTask object (has to exist on UP42). initialize_project ( self ) \u00b6 Show source code in up42/api.py 263 264 265 266 267 def initialize_project ( self ) -> \"up42.Project\" : \"\"\"Directly returns the correct project object (has to exist on UP42).\"\"\" project = up42 . Project ( api = self , project_id = self . project_id ) project . _get_info () return project Directly returns the correct project object (has to exist on UP42). initialize_workflow ( self , workflow_id ) \u00b6 Show source code in up42/api.py 273 274 275 276 277 def initialize_workflow ( self , workflow_id ) -> \"up42.Workflow\" : \"\"\"Directly returns a workflow object (has to exist on UP42).\"\"\" workflow = up42 . Workflow ( api = self , workflow_id = workflow_id , project_id = self . project_id ) return workflow Directly returns a workflow object (has to exist on UP42). validate_manifest ( self , path_or_json ) \u00b6 Show source code in up42/api.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation result and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json url = f \" { self . _endpoint () } /validate-schema/block\" response_json = self . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ] Validates the block manifest, input either manifest json string or filepath. Parameters Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns Type Description Dict A dictionary with the validation result and potential validation errors. Example { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } }","title":"Api"},{"location":"reference/api/#api-class","text":"","title":"Api class"},{"location":"reference/api/#up42.api.Api.create_environment","text":"Show source code in up42/api.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 def create_environment ( self , name : str , environment_variables : Dict = None ) -> Dict : \"\"\" Creates a new UP42 environments, used for separating storage of API keys etc. Args: name: Name of the new environment. environment_variables: The variables to add to the environment, see example. Returns: The json info of the newly created environment. Example: ```python environment_variables= {\"username\": \"up42\", \"password\": \"password\"} ``` \"\"\" existing_environment_names = [ env [ \"name\" ] for env in self . get_environments ()] if name in existing_environment_names : raise Exception ( \"An environment with the name %s already exists.\" , name ) payload = { \"name\" : name , \"secrets\" : environment_variables } url = f \" { self . _endpoint () } /environments\" response_json = self . _request ( request_type = \"POST\" , url = url , data = payload ) return response_json [ \"data\" ] Creates a new UP42 environments, used for separating storage of API keys etc. Parameters Name Type Description Default name str Name of the new environment. required environment_variables Dict The variables to add to the environment, see example. None Returns Type Description Dict The json info of the newly created environment. Example environment_variables = { \"username\" : \"up42\" , \"password\" : \"password\" }","title":"create_environment()"},{"location":"reference/api/#up42.api.Api.delete_environment","text":"Show source code in up42/api.py 415 416 417 418 419 420 421 422 423 424 def delete_environment ( self , environment_id : str ) -> None : \"\"\" Deletes a specific environment. Args: environment_id: The id of the environment to delete. See also get_environments. \"\"\" url = f \" { self . _endpoint () } /environments/ { environment_id } \" self . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted environment: %s \" , environment_id ) Deletes a specific environment. Parameters Name Type Description Default environment_id str The id of the environment to delete. See also get_environments. required","title":"delete_environment()"},{"location":"reference/api/#up42.api.Api.get_block_details","text":"Show source code in up42/api.py 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific (public or custom) block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" try : url = f \" { self . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . _request ( request_type = \"GET\" , url = url ) except ( requests . exceptions . HTTPError , RetryError ): url = f \" { self . _endpoint () } /users/me/blocks/ { block_id } \" # custom blocks response_json = self . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json Gets the detailed information about a specific (public or custom) block from the server, includes all manifest.json and marketplace.json contents. Parameters Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns Type Description Dict A dict of the block details metadata for the specific block.","title":"get_block_details()"},{"location":"reference/api/#up42.api.Api.get_blocks","text":"Show source code in up42/api.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ Dict , List [ Dict ]]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass url = f \" { self . _endpoint () } /blocks\" response_json = self . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting basic information, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json Gets a list of all public blocks on the marketplace. Parameters Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version True as_dataframe Returns a dataframe instead of json (default). False Returns Type Description Union[Dict, List[Dict]] A list of the public blocks and their metadata. Optional a simpler version dict.","title":"get_blocks()"},{"location":"reference/api/#up42.api.Api.get_environments","text":"Show source code in up42/api.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def get_environments ( self , as_dataframe = False ) -> Dict : \"\"\" Gets all existing UP42 environments, used for separating storage of API keys etc. Args: as_dataframe: Returns a dataframe instead of json (default). Returns: The environments as json info. \"\"\" url = f \" { self . _endpoint () } /environments\" response_json = self . _request ( request_type = \"GET\" , url = url ) environments_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame ( environments_json ) else : return environments_json Gets all existing UP42 environments, used for separating storage of API keys etc. Parameters Name Type Description Default as_dataframe Returns a dataframe instead of json (default). False Returns Type Description Dict The environments as json info.","title":"get_environments()"},{"location":"reference/api/#up42.api.Api.initialize_catalog","text":"Show source code in up42/api.py 269 270 271 def initialize_catalog ( self , backend : str = \"ONE_ATLAS\" ) -> \"up42.Catalog\" : \"\"\"Directly returns a catalog object.\"\"\" return up42 . Catalog ( api = self , backend = backend ) Directly returns a catalog object.","title":"initialize_catalog()"},{"location":"reference/api/#up42.api.Api.initialize_job","text":"Show source code in up42/api.py 279 280 281 282 def initialize_job ( self , job_id , order_ids : List [ str ] = [ \"\" ]) -> \"up42.Job\" : \"\"\"Directly returns a Job object (has to exist on UP42).\"\"\" job = up42 . Job ( api = self , job_id = job_id , project_id = self . project_id , order_ids = order_ids ) return job Directly returns a Job object (has to exist on UP42).","title":"initialize_job()"},{"location":"reference/api/#up42.api.Api.initialize_jobtask","text":"Show source code in up42/api.py 284 285 286 287 288 def initialize_jobtask ( self , job_task_id , job_id ) -> \"up42.Job\" : \"\"\"Directly returns a JobTask object (has to exist on UP42).\"\"\" jobtask = up42 . JobTask ( api = self , job_task_id = job_task_id , job_id = job_id , project_id = self . project_id ) return jobtask Directly returns a JobTask object (has to exist on UP42).","title":"initialize_jobtask()"},{"location":"reference/api/#up42.api.Api.initialize_project","text":"Show source code in up42/api.py 263 264 265 266 267 def initialize_project ( self ) -> \"up42.Project\" : \"\"\"Directly returns the correct project object (has to exist on UP42).\"\"\" project = up42 . Project ( api = self , project_id = self . project_id ) project . _get_info () return project Directly returns the correct project object (has to exist on UP42).","title":"initialize_project()"},{"location":"reference/api/#up42.api.Api.initialize_workflow","text":"Show source code in up42/api.py 273 274 275 276 277 def initialize_workflow ( self , workflow_id ) -> \"up42.Workflow\" : \"\"\"Directly returns a workflow object (has to exist on UP42).\"\"\" workflow = up42 . Workflow ( api = self , workflow_id = workflow_id , project_id = self . project_id ) return workflow Directly returns a workflow object (has to exist on UP42).","title":"initialize_workflow()"},{"location":"reference/api/#up42.api.Api.validate_manifest","text":"Show source code in up42/api.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation result and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json url = f \" { self . _endpoint () } /validate-schema/block\" response_json = self . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ] Validates the block manifest, input either manifest json string or filepath. Parameters Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns Type Description Dict A dictionary with the validation result and potential validation errors. Example { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } }","title":"validate_manifest()"},{"location":"reference/job/","text":"Job class \u00b6 __init__ ( self , api , project_id , job_id , order_ids = [ '' ]) \u00b6 Show source code in up42/job.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def __init__ ( self , api : up42 . Api , project_id : str , job_id : str , order_ids : List [ str ] = [ \"\" ], ): \"\"\"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklook, get_result_json download_result, upload_result_to_bucket, map_result, get_log, get_job_tasks, get_job_tasks_result_json \"\"\" self . api = api self . project_id = project_id self . job_id = job_id self . order_ids = order_ids if self . api . authenticate : self . info = self . _get_info () The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklook, get_result_json download_result, upload_result_to_bucket, map_result, get_log, get_job_tasks, get_job_tasks_result_json cancel_job ( self ) \u00b6 Show source code in up42/job.py 102 103 104 105 106 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . api . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . api . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id ) Cancels a pending or running job. download_quicklook ( self , out_dir = None ) \u00b6 Show source code in up42/job.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def download_quicklook ( self , out_dir = None ) -> Dict : \"\"\" Conveniance function that downloads the quicklooks of the first/data jobtask. After download, can be plotted via job.plot_quicklook(). \"\"\" if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) # Currently only the first/data task produces quicklooks. data_task = self . get_job_tasks ()[ 0 ] out_paths = data_task . download_quicklook ( out_dir = out_dir ) self . quicklook = out_paths # pylint: disable=attribute-defined-outside-init return out_paths Conveniance function that downloads the quicklooks of the first/data jobtask. After download, can be plotted via job.plot_quicklook(). download_result ( self , out_dir = None ) \u00b6 Show source code in up42/job.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def download_result ( self , out_dir : Union [ str , Path ] = None ,) -> List [ str ]: \"\"\" Downloads and unpacks the job result. Args: out_dir: The output folder. Default download to the Desktop. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument # TODO: Handle other filetypes than tif, Not working for Fullscenes etc. download_url = self . _get_download_url () if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Downloading results of job %s \" , self . job_id ) # Download tgz_file = tempfile . mktemp () with open ( tgz_file , \"wb\" ) as dst_tgz : r = requests . get ( download_url ) dst_tgz . write ( r . content ) # Unpack out_filepaths = [] with tarfile . open ( tgz_file ) as tar : members = tar . getmembers () tif_files = [ i for i in members if i . isfile () and i . name . endswith ( \".tif\" )] for idx , tif_file in enumerate ( tif_files ): f = tar . extractfile ( tif_file ) content = f . read () # TODO: Maybe jobid_sceneid etc? out_fp = out_dir / Path ( f \" { self . job_id } _ { idx } .tif\" ) with open ( out_fp , \"wb\" ) as dst : dst . write ( content ) out_filepaths . append ( str ( out_fp )) logger . info ( \"Download successful of %s files %s \" , len ( out_filepaths ), out_filepaths ) self . result = out_filepaths # pylint: disable=attribute-defined-outside-init return out_filepaths Downloads and unpacks the job result. Parameters Name Type Description Default out_dir Union[str, pathlib.Path] The output folder. Default download to the Desktop. None Returns Type Description List[str] List of the downloaded results' filepaths. get_job_tasks ( self , return_json = False ) \u00b6 Show source code in up42/job.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def get_job_tasks ( self , return_json : bool = False ) -> Union [ \"up42.JobTask\" , Dict ]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) job_tasks_json = response_json [ \"data\" ] job_tasks = [ up42 . JobTask ( api = self . api , project_id = self . project_id , job_id = self . job_id , job_task_id = task [ \"id\" ], ) for task in job_tasks_json ] if return_json : return job_tasks_json else : return job_tasks Get the individual items of the job as JobTask objects or json. Parameters Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns Type Description Union[ForwardRef('up42.JobTask'), Dict] The job task objects in a list. get_job_tasks_result_json ( self ) \u00b6 Show source code in up42/job.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def get_job_tasks_result_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" job_tasks = self . get_job_tasks ( return_json = True ) job_tasks_ids = [ task [ \"id\" ] for task in job_tasks ] job_tasks_results_json = {} for job_task_id in job_tasks_ids : url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { job_task_id } /outputs/data-json\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) job_tasks_results_json [ job_task_id ] = response_json return job_tasks_results_json Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns Type Description Dict The data.json of alle single job tasks. get_log ( self , as_print = True , as_return = False ) \u00b6 Show source code in up42/job.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def get_log ( self , as_print : bool = True , as_return : bool = False ) -> Dict : \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" # TODO: Check if job ended, seems to give error messages when used while still running. # but relevant to get logs while running and possible. just sometimes error. job_tasks = self . get_job_tasks ( return_json = True ) job_tasks_ids = [ task [ \"id\" ] for task in job_tasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( job_tasks_ids ), job_tasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( job_tasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , job_task_id in enumerate ( job_tasks_ids ): url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { job_task_id } /logs\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) job_logs [ job_task_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with job_task_id { job_task_id } : \\n \" ) print ( response_json ) if as_return : return job_logs Convenience function to print or return the logs of all job tasks. Parameters Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns Type Description Dict The log strings (only if as_return was selected). get_result_json ( self , as_dataframe = False ) \u00b6 Show source code in up42/job.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_result_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Job result data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json Gets the Job result data.json. Parameters Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. get_status ( self ) \u00b6 Show source code in up42/job.py 58 59 60 61 62 63 64 65 66 67 68 69 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status Gets the job status. Returns Type Description str The job status. map_result ( self , name_column = None , info_columns = None ) \u00b6 Show source code in up42/job.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def map_result ( self , name_column : str = None , info_columns : List = None ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs name_column: Name of the column that provides the layer name. info_columns: Additional columns that are shown when a feature is clicked. \"\"\" if not is_notebook (): raise ValueError ( \"Only works in Jupyter notebook.\" ) df = self . get_result_json ( as_dataframe = True ) # TODO: centroid of total_bounds centroid = df . iloc [ 0 ] . geometry . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) # Add features from data.json. def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } for index , row in df . iterrows (): try : layer_name = row [ name_column ] except KeyError : layer_name = f \"Layer { index + 1 } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , # ('{}{}'.format(row['dep'], row['dest'])), style_function = _style_function , highlight_function = _highlight_function , ) if not info_columns : folium . Popup ( f \" { layer_name } \" ) . add_to ( f ) else : if not isinstance ( info_columns , list ): raise ValueError ( \"Provide a list!\" ) infos = [ f \" { row [ info_col ] } \\n \" for info_col in info_columns ] infos = \"\" . join ( infos ) folium . Popup ( f \" { layer_name } \\n { infos } \" ) . add_to ( f ) f . add_to ( m ) # Same: folium.GeoJson(df, name=name_column, style_function=style_function, # highlight_function=highlight_function).add_to(map) # TODO: Not ideal, our streaming images are webmercator, folium requires wgs 84.0 # TODO: Switch to ipyleaflet! # This requires reprojecting on the user pc, not via the api. # Reproject raster and add to map dst_crs = 4326 for idx , raster_fp in enumerate ( self . result ): with rasterio . open ( raster_fp ) as src : dst_profile = src . meta . copy () if src . crs != dst_crs : transform , width , height = calculate_default_transform ( src . crs , dst_crs , src . width , src . height , * src . bounds ) dst_profile . update ( { \"crs\" : dst_crs , \"transform\" : transform , \"width\" : width , \"height\" : height , } ) with MemoryFile () as memfile : with memfile . open ( ** dst_profile ) as mem : for i in range ( 1 , src . count + 1 ): reproject ( source = rasterio . band ( src , i ), destination = rasterio . band ( mem , i ), src_transform = src . transform , src_crs = src . crs , dst_transform = transform , dst_crs = dst_crs , resampling = Resampling . nearest , ) # TODO: What if more bands than 3-4? dst_array = mem . read () minx , miny , maxx , maxy = mem . bounds dst_array = np . moveaxis ( np . stack ( dst_array ), 0 , 2 ) m . add_child ( folium . raster_layers . ImageOverlay ( dst_array , bounds = [[ miny , minx ], [ maxy , maxx ]], # andere reihenfolge. name = f \"Image - { idx } - { raster_fp } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) display ( m ) Displays data.json, and if available, one or multiple results geotiffs name_column: Name of the column that provides the layer name. info_columns: Additional columns that are shown when a feature is clicked. track_status ( self , report_time = 30 ) \u00b6 Show source code in up42/job.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def track_status ( self , report_time : int = 30 ) -> None : \"\"\" Continuously gets the job status until job has finished or failed. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" status = None time_asleep = 0 logger . info ( \"Tracking job status every %s seconds\" , report_time ) while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: sleep ( 5 ) time_asleep += 5 if status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully %s - SUCCEEDED!\" , self . job_id ) return status if status in [ \"FAILED\" , \"ERROR\" , \"CANCELLED\" , \"CANCELLING\" ]: self . get_log ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) Continuously gets the job status until job has finished or failed. Parameters Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 upload_result_to_bucket ( self , gs_client , bucket , folder , extension = '.tgz' , version = 'v0' ) \u00b6 Show source code in up42/job.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def upload_result_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the result to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s result with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s result to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" ) Uploads the result to a custom google cloud storage bucket.","title":"Job"},{"location":"reference/job/#job-class","text":"","title":"Job class"},{"location":"reference/job/#up42.job.Job.cancel_job","text":"Show source code in up42/job.py 102 103 104 105 106 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . api . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . api . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id ) Cancels a pending or running job.","title":"cancel_job()"},{"location":"reference/job/#up42.job.Job.download_quicklook","text":"Show source code in up42/job.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def download_quicklook ( self , out_dir = None ) -> Dict : \"\"\" Conveniance function that downloads the quicklooks of the first/data jobtask. After download, can be plotted via job.plot_quicklook(). \"\"\" if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) # Currently only the first/data task produces quicklooks. data_task = self . get_job_tasks ()[ 0 ] out_paths = data_task . download_quicklook ( out_dir = out_dir ) self . quicklook = out_paths # pylint: disable=attribute-defined-outside-init return out_paths Conveniance function that downloads the quicklooks of the first/data jobtask. After download, can be plotted via job.plot_quicklook().","title":"download_quicklook()"},{"location":"reference/job/#up42.job.Job.download_result","text":"Show source code in up42/job.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def download_result ( self , out_dir : Union [ str , Path ] = None ,) -> List [ str ]: \"\"\" Downloads and unpacks the job result. Args: out_dir: The output folder. Default download to the Desktop. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument # TODO: Handle other filetypes than tif, Not working for Fullscenes etc. download_url = self . _get_download_url () if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Downloading results of job %s \" , self . job_id ) # Download tgz_file = tempfile . mktemp () with open ( tgz_file , \"wb\" ) as dst_tgz : r = requests . get ( download_url ) dst_tgz . write ( r . content ) # Unpack out_filepaths = [] with tarfile . open ( tgz_file ) as tar : members = tar . getmembers () tif_files = [ i for i in members if i . isfile () and i . name . endswith ( \".tif\" )] for idx , tif_file in enumerate ( tif_files ): f = tar . extractfile ( tif_file ) content = f . read () # TODO: Maybe jobid_sceneid etc? out_fp = out_dir / Path ( f \" { self . job_id } _ { idx } .tif\" ) with open ( out_fp , \"wb\" ) as dst : dst . write ( content ) out_filepaths . append ( str ( out_fp )) logger . info ( \"Download successful of %s files %s \" , len ( out_filepaths ), out_filepaths ) self . result = out_filepaths # pylint: disable=attribute-defined-outside-init return out_filepaths Downloads and unpacks the job result. Parameters Name Type Description Default out_dir Union[str, pathlib.Path] The output folder. Default download to the Desktop. None Returns Type Description List[str] List of the downloaded results' filepaths.","title":"download_result()"},{"location":"reference/job/#up42.job.Job.get_job_tasks","text":"Show source code in up42/job.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def get_job_tasks ( self , return_json : bool = False ) -> Union [ \"up42.JobTask\" , Dict ]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) job_tasks_json = response_json [ \"data\" ] job_tasks = [ up42 . JobTask ( api = self . api , project_id = self . project_id , job_id = self . job_id , job_task_id = task [ \"id\" ], ) for task in job_tasks_json ] if return_json : return job_tasks_json else : return job_tasks Get the individual items of the job as JobTask objects or json. Parameters Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns Type Description Union[ForwardRef('up42.JobTask'), Dict] The job task objects in a list.","title":"get_job_tasks()"},{"location":"reference/job/#up42.job.Job.get_job_tasks_result_json","text":"Show source code in up42/job.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def get_job_tasks_result_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" job_tasks = self . get_job_tasks ( return_json = True ) job_tasks_ids = [ task [ \"id\" ] for task in job_tasks ] job_tasks_results_json = {} for job_task_id in job_tasks_ids : url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { job_task_id } /outputs/data-json\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) job_tasks_results_json [ job_task_id ] = response_json return job_tasks_results_json Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns Type Description Dict The data.json of alle single job tasks.","title":"get_job_tasks_result_json()"},{"location":"reference/job/#up42.job.Job.get_log","text":"Show source code in up42/job.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def get_log ( self , as_print : bool = True , as_return : bool = False ) -> Dict : \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" # TODO: Check if job ended, seems to give error messages when used while still running. # but relevant to get logs while running and possible. just sometimes error. job_tasks = self . get_job_tasks ( return_json = True ) job_tasks_ids = [ task [ \"id\" ] for task in job_tasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( job_tasks_ids ), job_tasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( job_tasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , job_task_id in enumerate ( job_tasks_ids ): url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { job_task_id } /logs\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) job_logs [ job_task_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with job_task_id { job_task_id } : \\n \" ) print ( response_json ) if as_return : return job_logs Convenience function to print or return the logs of all job tasks. Parameters Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns Type Description Dict The log strings (only if as_return was selected).","title":"get_log()"},{"location":"reference/job/#up42.job.Job.get_result_json","text":"Show source code in up42/job.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def get_result_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Job result data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json Gets the Job result data.json. Parameters Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json.","title":"get_result_json()"},{"location":"reference/job/#up42.job.Job.get_status","text":"Show source code in up42/job.py 58 59 60 61 62 63 64 65 66 67 68 69 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status Gets the job status. Returns Type Description str The job status.","title":"get_status()"},{"location":"reference/job/#up42.job.Job.map_result","text":"Show source code in up42/job.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def map_result ( self , name_column : str = None , info_columns : List = None ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs name_column: Name of the column that provides the layer name. info_columns: Additional columns that are shown when a feature is clicked. \"\"\" if not is_notebook (): raise ValueError ( \"Only works in Jupyter notebook.\" ) df = self . get_result_json ( as_dataframe = True ) # TODO: centroid of total_bounds centroid = df . iloc [ 0 ] . geometry . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) # Add features from data.json. def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } for index , row in df . iterrows (): try : layer_name = row [ name_column ] except KeyError : layer_name = f \"Layer { index + 1 } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , # ('{}{}'.format(row['dep'], row['dest'])), style_function = _style_function , highlight_function = _highlight_function , ) if not info_columns : folium . Popup ( f \" { layer_name } \" ) . add_to ( f ) else : if not isinstance ( info_columns , list ): raise ValueError ( \"Provide a list!\" ) infos = [ f \" { row [ info_col ] } \\n \" for info_col in info_columns ] infos = \"\" . join ( infos ) folium . Popup ( f \" { layer_name } \\n { infos } \" ) . add_to ( f ) f . add_to ( m ) # Same: folium.GeoJson(df, name=name_column, style_function=style_function, # highlight_function=highlight_function).add_to(map) # TODO: Not ideal, our streaming images are webmercator, folium requires wgs 84.0 # TODO: Switch to ipyleaflet! # This requires reprojecting on the user pc, not via the api. # Reproject raster and add to map dst_crs = 4326 for idx , raster_fp in enumerate ( self . result ): with rasterio . open ( raster_fp ) as src : dst_profile = src . meta . copy () if src . crs != dst_crs : transform , width , height = calculate_default_transform ( src . crs , dst_crs , src . width , src . height , * src . bounds ) dst_profile . update ( { \"crs\" : dst_crs , \"transform\" : transform , \"width\" : width , \"height\" : height , } ) with MemoryFile () as memfile : with memfile . open ( ** dst_profile ) as mem : for i in range ( 1 , src . count + 1 ): reproject ( source = rasterio . band ( src , i ), destination = rasterio . band ( mem , i ), src_transform = src . transform , src_crs = src . crs , dst_transform = transform , dst_crs = dst_crs , resampling = Resampling . nearest , ) # TODO: What if more bands than 3-4? dst_array = mem . read () minx , miny , maxx , maxy = mem . bounds dst_array = np . moveaxis ( np . stack ( dst_array ), 0 , 2 ) m . add_child ( folium . raster_layers . ImageOverlay ( dst_array , bounds = [[ miny , minx ], [ maxy , maxx ]], # andere reihenfolge. name = f \"Image - { idx } - { raster_fp } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) display ( m ) Displays data.json, and if available, one or multiple results geotiffs name_column: Name of the column that provides the layer name. info_columns: Additional columns that are shown when a feature is clicked.","title":"map_result()"},{"location":"reference/job/#up42.job.Job.track_status","text":"Show source code in up42/job.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def track_status ( self , report_time : int = 30 ) -> None : \"\"\" Continuously gets the job status until job has finished or failed. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" status = None time_asleep = 0 logger . info ( \"Tracking job status every %s seconds\" , report_time ) while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: sleep ( 5 ) time_asleep += 5 if status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully %s - SUCCEEDED!\" , self . job_id ) return status if status in [ \"FAILED\" , \"ERROR\" , \"CANCELLED\" , \"CANCELLING\" ]: self . get_log ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) Continuously gets the job status until job has finished or failed. Parameters Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30","title":"track_status()"},{"location":"reference/job/#up42.job.Job.upload_result_to_bucket","text":"Show source code in up42/job.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def upload_result_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the result to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s result with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s result to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" ) Uploads the result to a custom google cloud storage bucket.","title":"upload_result_to_bucket()"},{"location":"reference/jobtask/","text":"JobTask class \u00b6 __init__ ( self , api , project_id , job_id , job_task_id ) \u00b6 Show source code in up42/jobtask.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , api : up42 . Api , project_id : str , job_id : str , job_task_id : str , ): \"\"\"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_result_json, download_result, download_quicklook \"\"\" self . api = api self . project_id = project_id self . job_id = job_id self . job_task_id = job_task_id if self . api . authenticate : self . info = self . _get_info () The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_result_json, download_result, download_quicklook download_quicklook ( self , out_dir = None ) \u00b6 Show source code in up42/jobtask.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def download_quicklook ( self , out_dir : Union [ str , Path ] = None ,) -> List [ Path ]: \"\"\" Downloads quicklooks of all job tasks to disk. After download, can be plotted via jobtask.plot_quicklook(). Args: out_dir: Output directory. Returns: The quicklooks filepaths. \"\"\" if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . job_task_id } /outputs/quicklooks/\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) quicklook_ids = response_json [ \"data\" ] out_paths = [] for ql_id in quicklook_ids : out_path = Path ( out_dir ) / f \"quicklook_ { ql_id } .jpg\" out_paths . append ( out_path ) url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . job_task_id } /outputs/quicklooks/ { ql_id } \" ) response = self . api . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklook = out_paths # pylint: disable=attribute-defined-outside-init return out_paths Downloads quicklooks of all job tasks to disk. After download, can be plotted via jobtask.plot_quicklook(). Parameters Name Type Description Default out_dir Union[str, pathlib.Path] Output directory. None Returns Type Description List[pathlib.Path] The quicklooks filepaths. download_result ( self , out_dir = None ) \u00b6 Show source code in up42/jobtask.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def download_result ( self , out_dir : Union [ str , Path ] = None ) -> None : \"\"\" Downloads and unpacks the job task result. Default download to Desktop. Args: out_dir: The output directory for the downloaded files. \"\"\" download_url = self . _get_download_url () # TODO: Add tdqm progress bar if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Downloading results of job %s \" , self . job_id ) tgz_file = tempfile . mktemp () with open ( tgz_file , \"wb\" ) as f : r = requests . get ( download_url ) f . write ( r . content ) with tarfile . open ( tgz_file ) as tgz : files = tgz . getmembers () tif_files = [ i for i in files if i . isfile () and i . name . endswith ( \".tif\" )] out_filepaths = [] for count , f in enumerate ( tif_files ): out = tgz . extractfile ( f ) out_file = out_dir / Path ( f \" { self . job_id } _ { count } .tif\" ) with open ( out_file , \"wb\" ) as o : o . write ( out . read ()) out_filepaths . append ( str ( out_file )) logger . info ( \"Download successful of files %s \" , out_filepaths ) return out_filepaths Downloads and unpacks the job task result. Default download to Desktop. Parameters Name Type Description Default out_dir Union[str, pathlib.Path] The output directory for the downloaded files. None get_result_json ( self , as_dataframe = False ) \u00b6 Show source code in up42/jobtask.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_result_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Jobtask result data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . api . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . job_task_id } /outputs/data-json/\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json Gets the Jobtask result data.json. Parameters Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe.","title":"JobTask"},{"location":"reference/jobtask/#jobtask-class","text":"","title":"JobTask class"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_quicklook","text":"Show source code in up42/jobtask.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def download_quicklook ( self , out_dir : Union [ str , Path ] = None ,) -> List [ Path ]: \"\"\" Downloads quicklooks of all job tasks to disk. After download, can be plotted via jobtask.plot_quicklook(). Args: out_dir: Output directory. Returns: The quicklooks filepaths. \"\"\" if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . job_task_id } /outputs/quicklooks/\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) quicklook_ids = response_json [ \"data\" ] out_paths = [] for ql_id in quicklook_ids : out_path = Path ( out_dir ) / f \"quicklook_ { ql_id } .jpg\" out_paths . append ( out_path ) url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . job_task_id } /outputs/quicklooks/ { ql_id } \" ) response = self . api . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklook = out_paths # pylint: disable=attribute-defined-outside-init return out_paths Downloads quicklooks of all job tasks to disk. After download, can be plotted via jobtask.plot_quicklook(). Parameters Name Type Description Default out_dir Union[str, pathlib.Path] Output directory. None Returns Type Description List[pathlib.Path] The quicklooks filepaths.","title":"download_quicklook()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_result","text":"Show source code in up42/jobtask.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def download_result ( self , out_dir : Union [ str , Path ] = None ) -> None : \"\"\" Downloads and unpacks the job task result. Default download to Desktop. Args: out_dir: The output directory for the downloaded files. \"\"\" download_url = self . _get_download_url () # TODO: Add tdqm progress bar if out_dir is None : out_dir = os . path . join ( os . path . join ( os . path . expanduser ( \"~\" )), \"Desktop\" ) Path ( out_dir ) . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Downloading results of job %s \" , self . job_id ) tgz_file = tempfile . mktemp () with open ( tgz_file , \"wb\" ) as f : r = requests . get ( download_url ) f . write ( r . content ) with tarfile . open ( tgz_file ) as tgz : files = tgz . getmembers () tif_files = [ i for i in files if i . isfile () and i . name . endswith ( \".tif\" )] out_filepaths = [] for count , f in enumerate ( tif_files ): out = tgz . extractfile ( f ) out_file = out_dir / Path ( f \" { self . job_id } _ { count } .tif\" ) with open ( out_file , \"wb\" ) as o : o . write ( out . read ()) out_filepaths . append ( str ( out_file )) logger . info ( \"Download successful of files %s \" , out_filepaths ) return out_filepaths Downloads and unpacks the job task result. Default download to Desktop. Parameters Name Type Description Default out_dir Union[str, pathlib.Path] The output directory for the downloaded files. None","title":"download_result()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.get_result_json","text":"Show source code in up42/jobtask.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_result_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Jobtask result data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . api . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . job_task_id } /outputs/data-json/\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json Gets the Jobtask result data.json. Parameters Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe.","title":"get_result_json()"},{"location":"reference/project/","text":"Project class \u00b6 __init__ ( self , api , project_id ) \u00b6 Show source code in up42/project.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , api : up42 . Api , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_project_settings, update_project_settings \"\"\" self . api = api self . project_id = project_id if self . api . authenticate : self . info = self . _get_info () The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_project_settings, update_project_settings create_workflow ( self , name , description = '' , use_existing = False ) \u00b6 Show source code in up42/project.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"up42.Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logger . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . api . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = up42 . Workflow ( self . api , project_id = self . project_id , workflow_id = workflow_id ) return workflow Creates a new workflow and returns a workflow object. Parameters Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns Type Description up42.Workflow The workflow object. get_project_settings ( self ) \u00b6 Show source code in up42/project.py 111 112 113 114 115 116 117 118 119 120 121 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . api . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings Gets the project settings. Returns Type Description List The project settings. get_workflows ( self , return_json = False ) \u00b6 Show source code in up42/project.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"up42.Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . api . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ up42 . Workflow ( self . api , project_id = self . project_id , workflow_id = work [ \"id\" ] ) for work in workflows_json ] return workflows Gets all workflows in a project as workflow objects or json. Parameters Name Type Description Default return_json bool True returns Workflow Objects. False Returns Type Description Union[List[ForwardRef('up42.Workflow')], Dict] Workflow objects in the project or alternatively json info of the workflows. update_project_settings ( self , max_aoi_size = None , max_concurrent_jobs = None , number_of_images = None ) \u00b6 Show source code in up42/project.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . api . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload ) Updates a project's settings. Parameters Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images None","title":"Project"},{"location":"reference/project/#project-class","text":"","title":"Project class"},{"location":"reference/project/#up42.project.Project.create_workflow","text":"Show source code in up42/project.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"up42.Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logger . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . api . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = up42 . Workflow ( self . api , project_id = self . project_id , workflow_id = workflow_id ) return workflow Creates a new workflow and returns a workflow object. Parameters Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns Type Description up42.Workflow The workflow object.","title":"create_workflow()"},{"location":"reference/project/#up42.project.Project.get_project_settings","text":"Show source code in up42/project.py 111 112 113 114 115 116 117 118 119 120 121 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . api . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings Gets the project settings. Returns Type Description List The project settings.","title":"get_project_settings()"},{"location":"reference/project/#up42.project.Project.get_workflows","text":"Show source code in up42/project.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"up42.Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . api . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ up42 . Workflow ( self . api , project_id = self . project_id , workflow_id = work [ \"id\" ] ) for work in workflows_json ] return workflows Gets all workflows in a project as workflow objects or json. Parameters Name Type Description Default return_json bool True returns Workflow Objects. False Returns Type Description Union[List[ForwardRef('up42.Workflow')], Dict] Workflow objects in the project or alternatively json info of the workflows.","title":"get_workflows()"},{"location":"reference/project/#up42.project.Project.update_project_settings","text":"Show source code in up42/project.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . api . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload ) Updates a project's settings. Parameters Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images None","title":"update_project_settings()"},{"location":"reference/tools/","text":"Tools class \u00b6 __init__ ( self ) \u00b6 Show source code in up42/tools.py 23 24 25 26 27 28 29 30 def __init__ ( self ,): \"\"\" The tools class contains class independent functionality for e.g. aoi handling etc. They can be accessed from every object. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklook \"\"\" The tools class contains class independent functionality for e.g. aoi handling etc. They can be accessed from every object. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklook draw_aoi () (staticmethod) \u00b6 Show source code in up42/tools.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @staticmethod def draw_aoi () -> None : \"\"\" Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. \"\"\" m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) display ( m ) Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. get_example_aoi ( self , location = 'Berlin' , as_dataframe = False ) \u00b6 Show source code in up42/tools.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> FeatureCollection : \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washingtion. as_dataframe: Returns: Feature collection json with the selected aoi. \"\"\" # TODO: Add more geometries. logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { os . path . dirname ( __file__ ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : # TODO pass elif location == \"Tokyo\" : pass else : raise ValueError ( \"Please select one of 'Berlin', 'Washington' or 'Tokyo'!\" ) if as_dataframe : df = gpd . GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi Gets predefined, small, rectangular example aoi for the selected location. Parameters Name Type Description Default location str Location, one of Berlin, Washingtion. 'Berlin' as_dataframe bool False Returns Type Description FeatureCollection Feature collection json with the selected aoi. plot_coverage ( scenes , aoi = None , legend_column = None , figsize = ( 12 , 16 )) (staticmethod) \u00b6 Show source code in up42/tools.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @staticmethod def plot_coverage ( scenes : gpd . GeoDataFrame , aoi : gpd . GeoDataFrame = None , legend_column : str = None , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the result of catalog.search()) Args: scenes: GeoDataFrame of scenes, result of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, e.g. \"scene_id\". If None is provided, no legend is plotted. figsize: Matplotlib figure size. \"\"\" if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show () Plots a coverage map of a dataframe with geometries e.g. the result of catalog.search()) Parameters Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, result of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, e.g. \"scene_id\". If None is provided, no legend is plotted. None figsize Matplotlib figure size. (12, 16) plot_quicklook ( self , figsize = ( 8 , 8 ), filepaths = None ) \u00b6 Show source code in up42/tools.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def plot_quicklook ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. \"\"\" if filepaths is None : if not hasattr ( self , \"quicklook\" ) or self . quicklook is None : raise ValueError ( \"You first need to download the quicklooks via .download_quicklook().\" ) filepaths = self . quicklook # pylint: disable=no-member if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) else : raise ValueError ( \"Only works in Jupyter notebook.\" ) if len ( filepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 2 nrows = int ( math . ceil ( len ( filepaths ) / float ( ncols ))) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( filepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , fp in enumerate ( filepaths ): with rasterio . open ( fp ) as src : show ( src . read (), transform = src . transform , title = Path ( fp ) . stem , ax = axs [ idx ], ) plt . tight_layout () plt . show () Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) plot_result ( self , figsize = ( 8 , 8 ), filepaths = None , titles = None ) \u00b6 Show source code in up42/tools.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def plot_result ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ str ] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the downloaded results. \"\"\" # TODO: Add other fileformats. # TODO: Handle more bands. # TODO: add histogram equalization? But requires skimage dependency. if filepaths is None : if ( not hasattr ( self , \"result\" ) or self . result is None ): # pylint: disable=no-member raise ValueError ( \"You first need to download the results.\" ) filepaths = self . result # pylint: disable=no-member if not titles : titles = [ Path ( fp ) . stem for fp in filepaths ] if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) else : raise ValueError ( \"Only works in Jupyter notebook.\" ) if len ( filepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 3 nrows = int ( math . ceil ( len ( filepaths ) / float ( ncols ))) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( filepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , ( fp , title ) in enumerate ( zip ( filepaths , titles )): with rasterio . open ( fp ) as src : img_array = src . read () show ( img_array , transform = src . transform , title = title , ax = axs [ idx ], ) axs [ idx ] . set_axis_off () plt . tight_layout () plt . show () Plots the downloaded data. Parameters Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[str] Paths to images to plot. Optional, by default picks up the downloaded results. None read_vector_file ( self , filename = 'aoi.geojson' , as_dataframe = False ) \u00b6 Show source code in up42/tools.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> FeatureCollection : \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = gpd . GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs != \"epsg:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__ Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns Type Description FeatureCollection Feature Collection","title":"Tools"},{"location":"reference/tools/#tools-class","text":"","title":"Tools class"},{"location":"reference/tools/#up42.tools.Tools.draw_aoi","text":"Show source code in up42/tools.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @staticmethod def draw_aoi () -> None : \"\"\" Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. \"\"\" m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) display ( m ) Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality.","title":"draw_aoi()"},{"location":"reference/tools/#up42.tools.Tools.get_example_aoi","text":"Show source code in up42/tools.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> FeatureCollection : \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washingtion. as_dataframe: Returns: Feature collection json with the selected aoi. \"\"\" # TODO: Add more geometries. logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { os . path . dirname ( __file__ ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : # TODO pass elif location == \"Tokyo\" : pass else : raise ValueError ( \"Please select one of 'Berlin', 'Washington' or 'Tokyo'!\" ) if as_dataframe : df = gpd . GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi Gets predefined, small, rectangular example aoi for the selected location. Parameters Name Type Description Default location str Location, one of Berlin, Washingtion. 'Berlin' as_dataframe bool False Returns Type Description FeatureCollection Feature collection json with the selected aoi.","title":"get_example_aoi()"},{"location":"reference/tools/#up42.tools.Tools.plot_coverage","text":"Show source code in up42/tools.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @staticmethod def plot_coverage ( scenes : gpd . GeoDataFrame , aoi : gpd . GeoDataFrame = None , legend_column : str = None , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the result of catalog.search()) Args: scenes: GeoDataFrame of scenes, result of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, e.g. \"scene_id\". If None is provided, no legend is plotted. figsize: Matplotlib figure size. \"\"\" if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show () Plots a coverage map of a dataframe with geometries e.g. the result of catalog.search()) Parameters Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, result of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, e.g. \"scene_id\". If None is provided, no legend is plotted. None figsize Matplotlib figure size. (12, 16)","title":"plot_coverage()"},{"location":"reference/tools/#up42.tools.Tools.plot_quicklook","text":"Show source code in up42/tools.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def plot_quicklook ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. \"\"\" if filepaths is None : if not hasattr ( self , \"quicklook\" ) or self . quicklook is None : raise ValueError ( \"You first need to download the quicklooks via .download_quicklook().\" ) filepaths = self . quicklook # pylint: disable=no-member if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) else : raise ValueError ( \"Only works in Jupyter notebook.\" ) if len ( filepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 2 nrows = int ( math . ceil ( len ( filepaths ) / float ( ncols ))) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( filepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , fp in enumerate ( filepaths ): with rasterio . open ( fp ) as src : show ( src . read (), transform = src . transform , title = Path ( fp ) . stem , ax = axs [ idx ], ) plt . tight_layout () plt . show () Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8)","title":"plot_quicklook()"},{"location":"reference/tools/#up42.tools.Tools.plot_result","text":"Show source code in up42/tools.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def plot_result ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ str ] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the downloaded results. \"\"\" # TODO: Add other fileformats. # TODO: Handle more bands. # TODO: add histogram equalization? But requires skimage dependency. if filepaths is None : if ( not hasattr ( self , \"result\" ) or self . result is None ): # pylint: disable=no-member raise ValueError ( \"You first need to download the results.\" ) filepaths = self . result # pylint: disable=no-member if not titles : titles = [ Path ( fp ) . stem for fp in filepaths ] if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) else : raise ValueError ( \"Only works in Jupyter notebook.\" ) if len ( filepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 3 nrows = int ( math . ceil ( len ( filepaths ) / float ( ncols ))) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( filepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , ( fp , title ) in enumerate ( zip ( filepaths , titles )): with rasterio . open ( fp ) as src : img_array = src . read () show ( img_array , transform = src . transform , title = title , ax = axs [ idx ], ) axs [ idx ] . set_axis_off () plt . tight_layout () plt . show () Plots the downloaded data. Parameters Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[str] Paths to images to plot. Optional, by default picks up the downloaded results. None","title":"plot_result()"},{"location":"reference/tools/#up42.tools.Tools.read_vector_file","text":"Show source code in up42/tools.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> FeatureCollection : \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = gpd . GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs != \"epsg:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__ Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns Type Description FeatureCollection Feature Collection","title":"read_vector_file()"},{"location":"reference/workflow/","text":"Workflow class \u00b6 __init__ ( self , api , project_id , workflow_id ) \u00b6 Show source code in up42/workflow.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , api : up42 . Api , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameter_info, construct_parameter, create_and_run_job, get_jobs, update_name, delete \"\"\" self . api = api self . project_id = project_id self . workflow_id = workflow_id if self . api . authenticate : self . info = self . _get_info () The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameter_info, construct_parameter, create_and_run_job, get_jobs, update_name, delete add_workflow_tasks ( self , input_tasks ) \u00b6 Show source code in up42/workflow.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def add_workflow_tasks ( self , input_tasks : Union [ List , List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks. - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. Args: input_tasks: The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # TODO: User be able to only provide block task names or display name. But problematic with custom blocks. # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . api . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks ) Adds or overwrites workflow tasks. Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. API by itself validates if the underlying block for the selected block-id is available. Parameters Name Type Description Default input_tasks Union[List, List[Dict]] The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). required Example input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] Example input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] construct_parameter ( self , geometry = None , geometry_operation = None , handle_multiple_features = 'footprint' , start_date = None , end_date = None , limit = None , scene_ids = None , order_ids = None ) \u00b6 Show source code in up42/workflow.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def construct_parameter ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , List , gpd . GeoDataFrame , shapely . geometry . polygon . Polygon , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.polygon.Polygon. All assume EPSG 4326 and Polygons! geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not create_and_run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In cas of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon]] One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.polygon.Polygon. All assume EPSG 4326 and Polygons! None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns Type Description Dict Dictionary of constructed input parameters. create_and_run_job ( self , input_parameters = None , track_status = False ) \u00b6 Show source code in up42/workflow.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def create_and_run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , ) -> \"up42.Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. Returns: The spawned job object. \"\"\" # TODO: Currently runs both dry & live run at the same time. Make selectable via config in request, see docs. if input_parameters is None : raise ValueError ( \"Select the job_parameters! You can use \" \".get_default_parameters() to get the default parameters \" \"of the workflow.\" ) if isinstance ( input_parameters , ( str , Path )): with open ( input_parameters ) as src : input_parameters = json . load ( src ) logger . info ( \"Loading job parameters from json file.\" ) logger . info ( \"Selected input_parameters: %s .\" , input_parameters ) name = \"_py\" # Enables recognition of python API usage. url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /\" f \"workflows/ { self . workflow_id } /jobs?name= { name } \" ) response_json = self . api . _request ( request_type = \"POST\" , url = url , data = input_parameters ) job_json = response_json [ \"data\" ] logger . info ( \"Created and running new job: %s .\" , job_json [ \"id\" ]) job = up42 . Job ( self . api , job_id = job_json [ \"id\" ], project_id = self . project_id ,) if track_status : job . track_status () return job Creates and runs a new job. Parameters Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False Returns Type Description up42.Job The spawned job object. delete ( self ) \u00b6 Show source code in up42/workflow.py 398 399 400 401 402 403 404 405 406 407 408 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . api . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self Deletes the workflow and sets the Python object to None. get_compatible_blocks ( self ) \u00b6 Show source code in up42/workflow.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True )[ - 1 ] . keys ())[ 0 ] url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. return compatible_blocks Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. get_jobs ( self , return_json = False ) \u00b6 Show source code in up42/workflow.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"up42.Job\" ], Dict ]: \"\"\" Get all jobs in the specific project as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . api . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_json else : jobs = [ up42 . Job ( self . api , job_id = job [ \"id\" ], project_id = self . project_id ) for job in jobs_json ] return jobs Get all jobs in the specific project as job objects or json. Parameters Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns Type Description Union[List[ForwardRef('up42.Job')], Dict] All job objects as a list, or alternatively the jobs info as json. get_parameter_info ( self ) \u00b6 Show source code in up42/workflow.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def get_parameter_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameter info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns Type Description Dict Workflow parameter info json. get_workflow_tasks ( self , basic = False ) \u00b6 Show source code in up42/workflow.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return [{ task [ \"name\" ]: task [ \"id\" ]} for task in tasks ] else : return tasks Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters Name Type Description Default basic bool If selected returns a simplified task-name : task-id version. | False` Returns Type Description Union[List, Dict] The workflow task info. update_name ( self , name = None , description = None ) \u00b6 Show source code in up42/workflow.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . api . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update ) Updates the workflow name and description. Parameters Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None","title":"Workflow"},{"location":"reference/workflow/#workflow-class","text":"","title":"Workflow class"},{"location":"reference/workflow/#up42.workflow.Workflow.add_workflow_tasks","text":"Show source code in up42/workflow.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def add_workflow_tasks ( self , input_tasks : Union [ List , List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks. - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. Args: input_tasks: The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # TODO: User be able to only provide block task names or display name. But problematic with custom blocks. # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . api . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks ) Adds or overwrites workflow tasks. Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. API by itself validates if the underlying block for the selected block-id is available. Parameters Name Type Description Default input_tasks Union[List, List[Dict]] The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). required Example input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] Example input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }]","title":"add_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.construct_parameter","text":"Show source code in up42/workflow.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def construct_parameter ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , List , gpd . GeoDataFrame , shapely . geometry . polygon . Polygon , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.polygon.Polygon. All assume EPSG 4326 and Polygons! geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not create_and_run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In cas of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon]] One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.polygon.Polygon. All assume EPSG 4326 and Polygons! None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns Type Description Dict Dictionary of constructed input parameters.","title":"construct_parameter()"},{"location":"reference/workflow/#up42.workflow.Workflow.create_and_run_job","text":"Show source code in up42/workflow.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 def create_and_run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , ) -> \"up42.Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. Returns: The spawned job object. \"\"\" # TODO: Currently runs both dry & live run at the same time. Make selectable via config in request, see docs. if input_parameters is None : raise ValueError ( \"Select the job_parameters! You can use \" \".get_default_parameters() to get the default parameters \" \"of the workflow.\" ) if isinstance ( input_parameters , ( str , Path )): with open ( input_parameters ) as src : input_parameters = json . load ( src ) logger . info ( \"Loading job parameters from json file.\" ) logger . info ( \"Selected input_parameters: %s .\" , input_parameters ) name = \"_py\" # Enables recognition of python API usage. url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /\" f \"workflows/ { self . workflow_id } /jobs?name= { name } \" ) response_json = self . api . _request ( request_type = \"POST\" , url = url , data = input_parameters ) job_json = response_json [ \"data\" ] logger . info ( \"Created and running new job: %s .\" , job_json [ \"id\" ]) job = up42 . Job ( self . api , job_id = job_json [ \"id\" ], project_id = self . project_id ,) if track_status : job . track_status () return job Creates and runs a new job. Parameters Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False Returns Type Description up42.Job The spawned job object.","title":"create_and_run_job()"},{"location":"reference/workflow/#up42.workflow.Workflow.delete","text":"Show source code in up42/workflow.py 398 399 400 401 402 403 404 405 406 407 408 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . api . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self Deletes the workflow and sets the Python object to None.","title":"delete()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_compatible_blocks","text":"Show source code in up42/workflow.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True )[ - 1 ] . keys ())[ 0 ] url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. return compatible_blocks Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks.","title":"get_compatible_blocks()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_jobs","text":"Show source code in up42/workflow.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"up42.Job\" ], Dict ]: \"\"\" Get all jobs in the specific project as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . api . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . api . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_json else : jobs = [ up42 . Job ( self . api , job_id = job [ \"id\" ], project_id = self . project_id ) for job in jobs_json ] return jobs Get all jobs in the specific project as job objects or json. Parameters Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns Type Description Union[List[ForwardRef('up42.Job')], Dict] All job objects as a list, or alternatively the jobs info as json.","title":"get_jobs()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_parameter_info","text":"Show source code in up42/workflow.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def get_parameter_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameter info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns Type Description Dict Workflow parameter info json.","title":"get_parameter_info()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_workflow_tasks","text":"Show source code in up42/workflow.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . api . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return [{ task [ \"name\" ]: task [ \"id\" ]} for task in tasks ] else : return tasks Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters Name Type Description Default basic bool If selected returns a simplified task-name : task-id version. | False` Returns Type Description Union[List, Dict] The workflow task info.","title":"get_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.update_name","text":"Show source code in up42/workflow.py 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . api . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . api . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update ) Updates the workflow name and description. Parameters Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None","title":"update_name()"}]}