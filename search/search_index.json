{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"30-seconds-example/","text":"30 Seconds Example \u00b6 A new workflow is created and filled with tasks ( Sentinel-2 data , image sharpening ). The area of interest and workflow parameters are defined. After running the job, the results are downloaded and visualized. import up42 up42 . authenticate ( porject_id = 12345 , project_api_key = 12345 ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project # Add blocks/tasks to the workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) blocks = up42 . get_blocks ( basic = True ) input_tasks = [ blocks [ 'sobloo-s2-l1c-aoiclipped' ], blocks [ 'sharpening' ]] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow to run it. aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , limit = 1 ) input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters job = workflow . create_and_run_job ( input_parameters = input_parameters ) job . track_status () job . download_results () job . plot_results ()","title":"30 Seconds Example"},{"location":"30-seconds-example/#30-seconds-example","text":"A new workflow is created and filled with tasks ( Sentinel-2 data , image sharpening ). The area of interest and workflow parameters are defined. After running the job, the results are downloaded and visualized. import up42 up42 . authenticate ( porject_id = 12345 , project_api_key = 12345 ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project # Add blocks/tasks to the workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) blocks = up42 . get_blocks ( basic = True ) input_tasks = [ blocks [ 'sobloo-s2-l1c-aoiclipped' ], blocks [ 'sharpening' ]] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow to run it. aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , limit = 1 ) input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters job = workflow . create_and_run_job ( input_parameters = input_parameters ) job . track_status () job . download_results () job . plot_results ()","title":"30 Seconds Example"},{"location":"authentication/","text":"Authentication \u00b6 In order to use the UP42 Python SDK functionality you need to first authenticate with the UP42 servers via your project credentials. Get your Project credentials Log in to UP42.com and create a new project or select an existing one. You can find the project_id and project_api_key in the project's \"Developer\" section . As arguments \u00b6 Authenticate by passing the project credentials directly as arguments : import up42 up42 . authenticate ( project_id = 123 , project_api_key = 456 ) Use a configuration file \u00b6 Alternatively, create a configuration json file and pass its file path: { \"project_id\" : \"...\" , \"project_api_key\" : \"...\" } import up42 up42 . authenticate ( cfg_file = \"config.json\" )","title":"Authentication"},{"location":"authentication/#authentication","text":"In order to use the UP42 Python SDK functionality you need to first authenticate with the UP42 servers via your project credentials. Get your Project credentials Log in to UP42.com and create a new project or select an existing one. You can find the project_id and project_api_key in the project's \"Developer\" section .","title":"Authentication"},{"location":"authentication/#as-arguments","text":"Authenticate by passing the project credentials directly as arguments : import up42 up42 . authenticate ( project_id = 123 , project_api_key = 456 )","title":"As arguments"},{"location":"authentication/#use-a-configuration-file","text":"Alternatively, create a configuration json file and pass its file path: { \"project_id\" : \"...\" , \"project_api_key\" : \"...\" } import up42 up42 . authenticate ( cfg_file = \"config.json\" )","title":"Use a configuration file"},{"location":"catalog/","text":"Catalog Search \u00b6 import up42 up42 . authenticate ( cfg_file = \"config.json\" ) catalog = up42 . initialize_catalog () catalog Search scenes in aoi \u00b6 #aoi = up42.read_vector_file(\"data/aoi_washington.geojson\", # as_dataframe=False) aoi = up42 . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) aoi search_paramaters = catalog . construct_parameters ( geometry = aoi , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 5 ) search_results = catalog . search ( search_paramaters = search_paramaters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" ) Quicklooks \u00b6 catalog . download_quicklooks ( image_ids = search_results . id . to_list (), provider = \"oneatlas\" ) #oneatlas works catalog . plot_quicklooks ( figsize = ( 20 , 20 ))","title":"Catalog Search"},{"location":"catalog/#catalog-search","text":"import up42 up42 . authenticate ( cfg_file = \"config.json\" ) catalog = up42 . initialize_catalog () catalog","title":"Catalog Search"},{"location":"catalog/#search-scenes-in-aoi","text":"#aoi = up42.read_vector_file(\"data/aoi_washington.geojson\", # as_dataframe=False) aoi = up42 . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) aoi search_paramaters = catalog . construct_parameters ( geometry = aoi , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 5 ) search_results = catalog . search ( search_paramaters = search_paramaters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" )","title":"Search scenes in aoi"},{"location":"catalog/#quicklooks","text":"catalog . download_quicklooks ( image_ids = search_results . id . to_list (), provider = \"oneatlas\" ) #oneatlas works catalog . plot_quicklooks ( figsize = ( 20 , 20 ))","title":"Quicklooks"},{"location":"cli/","text":"Command Line Interface (CLI) \u00b6 The CLI tool allows you to use the UP42 functionality from the command line. It is installed automatically with and based on the Python SDK. To check whether the tool is installed and functioning correctly, type the following on your terminal or command line. This will print out a summary of the available commands. up42 -h To get help on a specific command, use: up42 command -h Authenticate \u00b6 You can authenticate with a PROJECT_ID and PROJECT_API_KEY up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] auth Or using a config.json file: up42 -cfg [ path to config.json ] auth You can make the authentication persistent by storing either the project key pair or the path to the config file as an environment variable. export UP42_PROJECT_ID =[ PROJECT_ID ] export UP42_PROJECT_API_KEY =[ PROJECT_API_KEY ] Or when using config.json . export UP42_CFG_FILE =[ path to config.json ] To save the authentication for future sessions make sure to append these variables to your bash profile file: # Linux export UP42_CFG_FILE =[ path to config.json ] >> ~/.bashrc # MacOS export UP42_CFG_FILE =[ path to config.json ] >> ~/.bash_profile If you want to create a config.json file from a project key pair, you can use the config command. up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] config Workflows \u00b6 up42 workflow -h Create a new workflow: up42 project create-workflow a_test Or check which workflows already exist: up42 project get-workflows And get a workflow by its name: up42 project workflow-from-name -name a_test After running the command to persist the workflow you can get the workflow tasks: up42 workflow get-workflow-tasks You can also add workflow tasks to the workflow via a json file (see typical usage for an example): up42 workflow add-workflow-tasks new_workflow_tasks.json Jobs \u00b6 up42 job -h Create and run a new job with parameters defined in a json file (see typical usage for an example): up42 workflow create-and-run-job input_parameters.json --track After running the command to persist the job you can download the quicklooks from job in current working directory (note that not all data blocks support quicklooks): up42 job download-quicklooks . Or download and unpack the results: up42 job download-results . You can also print out the logs of the job: up42 job get-log Catalog \u00b6 up42 catalog -h With the catalog commands you can easily search the UP42 catalog for data. First, create a parameter configuration: up42 catalog construct-parameters example_aoi.geojson --sensors pleiades --max-cloud-cover 5 Then get the results: up42 catalog search example_search_params.json General tools \u00b6 Get all public blocks in the platform: up42 get-blocks Get block details by name: up42 get-block-details -name oneatlas-pleiades-aoiclipped","title":"Command Line Interface"},{"location":"cli/#command-line-interface-cli","text":"The CLI tool allows you to use the UP42 functionality from the command line. It is installed automatically with and based on the Python SDK. To check whether the tool is installed and functioning correctly, type the following on your terminal or command line. This will print out a summary of the available commands. up42 -h To get help on a specific command, use: up42 command -h","title":"Command Line Interface (CLI)"},{"location":"cli/#authenticate","text":"You can authenticate with a PROJECT_ID and PROJECT_API_KEY up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] auth Or using a config.json file: up42 -cfg [ path to config.json ] auth You can make the authentication persistent by storing either the project key pair or the path to the config file as an environment variable. export UP42_PROJECT_ID =[ PROJECT_ID ] export UP42_PROJECT_API_KEY =[ PROJECT_API_KEY ] Or when using config.json . export UP42_CFG_FILE =[ path to config.json ] To save the authentication for future sessions make sure to append these variables to your bash profile file: # Linux export UP42_CFG_FILE =[ path to config.json ] >> ~/.bashrc # MacOS export UP42_CFG_FILE =[ path to config.json ] >> ~/.bash_profile If you want to create a config.json file from a project key pair, you can use the config command. up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] config","title":"Authenticate"},{"location":"cli/#workflows","text":"up42 workflow -h Create a new workflow: up42 project create-workflow a_test Or check which workflows already exist: up42 project get-workflows And get a workflow by its name: up42 project workflow-from-name -name a_test After running the command to persist the workflow you can get the workflow tasks: up42 workflow get-workflow-tasks You can also add workflow tasks to the workflow via a json file (see typical usage for an example): up42 workflow add-workflow-tasks new_workflow_tasks.json","title":"Workflows"},{"location":"cli/#jobs","text":"up42 job -h Create and run a new job with parameters defined in a json file (see typical usage for an example): up42 workflow create-and-run-job input_parameters.json --track After running the command to persist the job you can download the quicklooks from job in current working directory (note that not all data blocks support quicklooks): up42 job download-quicklooks . Or download and unpack the results: up42 job download-results . You can also print out the logs of the job: up42 job get-log","title":"Jobs"},{"location":"cli/#catalog","text":"up42 catalog -h With the catalog commands you can easily search the UP42 catalog for data. First, create a parameter configuration: up42 catalog construct-parameters example_aoi.geojson --sensors pleiades --max-cloud-cover 5 Then get the results: up42 catalog search example_search_params.json","title":"Catalog"},{"location":"cli/#general-tools","text":"Get all public blocks in the platform: up42 get-blocks Get block details by name: up42 get-block-details -name oneatlas-pleiades-aoiclipped","title":"General tools"},{"location":"index_2/","text":"UP42 Python SDK \u00b6 Welcome to UP42 - the developer platform and marketplace for geospatial data and applications: Access commercial and public geospatial datasets , e.g. satellite, aerial, vector, IoT & more. Construct workflows from modular building blocks for geospatial data, processing & analysis. Bring your own algorithms via custom blocks. Integrates into your own product via the API and Python package. Scale your geospatial applications with the powerful UP42 cloud infrastructure . UP42 Python SDK \u00b6 Python package for easy access to UP42's geospatial datasets & processing workflows For geospatial analysis & product builders! Interactive maps & visualization, ideal with Jupyter notebooks Command Line Interface (CLI) Developer tools for UP42 custom blocks (coming soon)","title":"UP42 Python SDK"},{"location":"index_2/#up42-python-sdk","text":"Welcome to UP42 - the developer platform and marketplace for geospatial data and applications: Access commercial and public geospatial datasets , e.g. satellite, aerial, vector, IoT & more. Construct workflows from modular building blocks for geospatial data, processing & analysis. Bring your own algorithms via custom blocks. Integrates into your own product via the API and Python package. Scale your geospatial applications with the powerful UP42 cloud infrastructure .","title":"UP42 Python SDK"},{"location":"index_2/#up42-python-sdk_1","text":"Python package for easy access to UP42's geospatial datasets & processing workflows For geospatial analysis & product builders! Interactive maps & visualization, ideal with Jupyter notebooks Command Line Interface (CLI) Developer tools for UP42 custom blocks (coming soon)","title":"UP42 Python SDK"},{"location":"installation/","text":"Installation \u00b6 User installation \u00b6 The package requires Python version > 3.6. Install via pip: pip install up42-py If you have an existing installation, update to the newest version via: pip install up42-py --upgrade Test the successful installation by importing the package in Python: import up42 Success! Continue with the Authentication chapter ! Development installation \u00b6 Warning The development installation is only necessary if you want to contribute to up42-py, e.g. to fix a bug. Please see the developer readme for the full installation instructions and further information.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#user-installation","text":"The package requires Python version > 3.6. Install via pip: pip install up42-py If you have an existing installation, update to the newest version via: pip install up42-py --upgrade Test the successful installation by importing the package in Python: import up42 Success! Continue with the Authentication chapter !","title":"User installation"},{"location":"installation/#development-installation","text":"Warning The development installation is only necessary if you want to contribute to up42-py, e.g. to fix a bug. Please see the developer readme for the full installation instructions and further information.","title":"Development installation"},{"location":"structure/","text":"Structure \u00b6 Hierachy \u00b6 The Python SDK uses six object classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask Catalog Tools Each object can spawn elements of one level below , e.g. project = up42.initialize_project() workflow = Project().create_workflow() job = workflow.create_and_run_job() Functionality \u00b6 An overview of the the functionality of each object (also see the code reference ): Available Functionality Project .get_workflows() .create_workflow() .get_project_settings() .update_project_settings() .update_project_settings() Workflow .add_workflow_tasks() .get_parameters_info() .construct_parameters() .get_jobs() .create_and_run_job() .get_workflow_tasks() .add_workflow_tasks() .update_name() .delete() Job .get_status() .track_status() .cancel_job() .get_results() .get_logs() .get_quicklooks() .download_results() .plot_results() .map_results() .upload_results_to_bucket() .get_jobtasks() .get_jobtasks_results() JobTask .get_results_json() .download_results() .get_quicklooks() Catalog .construct_parameters() .search() .download_quicklooks() Tools .read_vector_file() .get_example_aoi() .draw_aoi() .plot_coverage() .plot_quicklooks() .plot_results() .get_blocks() .get_block_details() .validate_manifest() .initialize_project() Object Initialization \u00b6 Here is how initialize each object directly, i.e. when it already exists on UP42 and you want to directly access it: Initialize Object Project up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) project = up42 . initialize_project () Workflow UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) Job UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = up42 . initialize_job ( job_id = UP42_JOB_ID ) JobTask UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = up42 . initialize_jobtask ( jobtask_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) Catalog catalog = up42 . initialize_catalog () Tools The tools' functionalities can be accessed from any of the up42 objects, e.g. up42 . get_example_aoi () # workflow.get_example_aoi() # job.get_example_aoi()","title":"Structure"},{"location":"structure/#structure","text":"","title":"Structure"},{"location":"structure/#hierachy","text":"The Python SDK uses six object classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask Catalog Tools Each object can spawn elements of one level below , e.g. project = up42.initialize_project() workflow = Project().create_workflow() job = workflow.create_and_run_job()","title":"Hierachy"},{"location":"structure/#functionality","text":"An overview of the the functionality of each object (also see the code reference ): Available Functionality Project .get_workflows() .create_workflow() .get_project_settings() .update_project_settings() .update_project_settings() Workflow .add_workflow_tasks() .get_parameters_info() .construct_parameters() .get_jobs() .create_and_run_job() .get_workflow_tasks() .add_workflow_tasks() .update_name() .delete() Job .get_status() .track_status() .cancel_job() .get_results() .get_logs() .get_quicklooks() .download_results() .plot_results() .map_results() .upload_results_to_bucket() .get_jobtasks() .get_jobtasks_results() JobTask .get_results_json() .download_results() .get_quicklooks() Catalog .construct_parameters() .search() .download_quicklooks() Tools .read_vector_file() .get_example_aoi() .draw_aoi() .plot_coverage() .plot_quicklooks() .plot_results() .get_blocks() .get_block_details() .validate_manifest() .initialize_project()","title":"Functionality"},{"location":"structure/#object-initialization","text":"Here is how initialize each object directly, i.e. when it already exists on UP42 and you want to directly access it: Initialize Object Project up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) project = up42 . initialize_project () Workflow UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) Job UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = up42 . initialize_job ( job_id = UP42_JOB_ID ) JobTask UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = up42 . initialize_jobtask ( jobtask_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) Catalog catalog = up42 . initialize_catalog () Tools The tools' functionalities can be accessed from any of the up42 objects, e.g. up42 . get_example_aoi () # workflow.get_example_aoi() # job.get_example_aoi()","title":"Object Initialization"},{"location":"support-faq/","text":"Support & FAQ \u00b6 Contact \u00b6 Please contact us via Email support@up42.com or open a github issue . Related links \u00b6 UP42 Website UP42 Github Repositories UP42 Python package UP42 Documentation UP42 docs repository FAQ \u00b6 Can I contribute to the SDK? \u00b6 Yes, contributions and bug fixes are very welcome. Please see the developer readme for further instructions.","title":"FAQ"},{"location":"support-faq/#support-faq","text":"","title":"Support &amp; FAQ"},{"location":"support-faq/#contact","text":"Please contact us via Email support@up42.com or open a github issue .","title":"Contact"},{"location":"support-faq/#related-links","text":"UP42 Website UP42 Github Repositories UP42 Python package UP42 Documentation UP42 docs repository","title":"Related links"},{"location":"support-faq/#faq","text":"","title":"FAQ"},{"location":"support-faq/#can-i-contribute-to-the-sdk","text":"Yes, contributions and bug fixes are very welcome. Please see the developer readme for further instructions.","title":"Can I contribute to the SDK?"},{"location":"typical-usage/","text":"Typical Usage \u00b6 This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps. Authenticate & access project \u00b6 import up42 up42 . authenticate ( porject_id = 12345 , project_api_key = 12345 ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () Get information about the available blocks to later construct your workflow. up42 . get_blocks ( basic = True ) Create or access the workflow \u00b6 You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = False ) workflow # Add workflow tasks - simple version input_tasks = [ \"a2daaab4-196d-4226-a018-a810444dcad1\" , \"4ed70368-d4e1-4462-bef6-14e768049471\" ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) #workflow.get_jobs() # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access the existing workflow the id (has to exist within the accessed project). UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow Select the aoi \u00b6 There are multiple ways to select an aoi: - Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. - Use .draw_aoi() to draw the aoi and export it as a geojson. - Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. - Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) #aoi = workflow.get_example_aoi(location=\"Berlin\") #aoi #workflow.draw_aoi() Select the workflow parameters \u00b6 There are also multiple ways to construct the workflow input parameters: - Provide the parameters directly in code as a json string. - Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. - Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameters_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) input_parameters # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters Run the workflow & download results \u00b6 # Run the workflow as a test job: Similar to catalog search, only returns whoch images were found for the configuration. query_job = workflow . create_and_run_job ( input_parameters = input_parameters , test_query = True , track_status = True ,) query_job . get_results_json () # Run the actual workflow. job = workflow . create_and_run_job ( input_parameters = input_parameters , track_status = True ) Download & Display results \u00b6 # Download job result (default downloads to Desktop). Only works after download is finished. results_fp = job . download_results () job . plot_results () job . map_results ()","title":"Typical Usage"},{"location":"typical-usage/#typical-usage","text":"This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps.","title":"Typical Usage"},{"location":"typical-usage/#authenticate-access-project","text":"import up42 up42 . authenticate ( porject_id = 12345 , project_api_key = 12345 ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () Get information about the available blocks to later construct your workflow. up42 . get_blocks ( basic = True )","title":"Authenticate &amp; access project"},{"location":"typical-usage/#create-or-access-the-workflow","text":"You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = False ) workflow # Add workflow tasks - simple version input_tasks = [ \"a2daaab4-196d-4226-a018-a810444dcad1\" , \"4ed70368-d4e1-4462-bef6-14e768049471\" ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) #workflow.get_jobs() # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access the existing workflow the id (has to exist within the accessed project). UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow","title":"Create or access the workflow"},{"location":"typical-usage/#select-the-aoi","text":"There are multiple ways to select an aoi: - Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. - Use .draw_aoi() to draw the aoi and export it as a geojson. - Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. - Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) #aoi = workflow.get_example_aoi(location=\"Berlin\") #aoi #workflow.draw_aoi()","title":"Select the aoi"},{"location":"typical-usage/#select-the-workflow-parameters","text":"There are also multiple ways to construct the workflow input parameters: - Provide the parameters directly in code as a json string. - Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. - Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameters_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) input_parameters # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters","title":"Select the workflow parameters"},{"location":"typical-usage/#run-the-workflow-download-results","text":"# Run the workflow as a test job: Similar to catalog search, only returns whoch images were found for the configuration. query_job = workflow . create_and_run_job ( input_parameters = input_parameters , test_query = True , track_status = True ,) query_job . get_results_json () # Run the actual workflow. job = workflow . create_and_run_job ( input_parameters = input_parameters , track_status = True )","title":"Run the workflow &amp; download results"},{"location":"typical-usage/#download-display-results","text":"# Download job result (default downloads to Desktop). Only works after download is finished. results_fp = job . download_results () job . plot_results () job . map_results ()","title":"Download &amp; Display results"},{"location":"examples/airport-monitoring/","text":"Parallel Jobs \u00b6 Example: Airport monitoring \u00b6 Get a Sentinel-2 clipped image for airports in a country. Run all jobs in parallel Visualize the results % load_ext autoreload % autoreload 2 import pandas as pd import geopandas as gpd from pathlib import Path import up42 10 random airports in a Spain \u00b6 https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) airports = airports . sample ( 10 ) # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) # Buffer airport point locations by around 100m airports . geometry = airports . geometry . buffer ( 0.001 ) airports . iloc [ 0 ] . geometry Prepare UP42 workflows \u00b6 # Authentificate api = up42 . authenticate ( cfg_file = \"config.json\" ) project = up42 . initialize_project () project # Only works when you have added your credit card information to the UP42 account. project . update_project_settings ( max_concurrent_jobs = 10 ) workflow = project . create_workflow ( \"workflow_demo_airplanes\" , use_existing = True ) workflow # Fill the workflow with tasks blocks = up42 . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ blocks [ selected_block ]]) workflow . get_workflow_tasks ( basic = True ) Run jobs in parallel \u00b6 Very crude, this will soon be available in the API in one command! # Run jobs in parallel jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameters ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . create_and_run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) # Download results: out_filepaths = [] for job in jobs : fp = job . download_results () out_filepaths . append ( fp [ 0 ]) print ( \"finished\" ) print ( out_filepaths ) # Visualize downloaded results up42 . plot_results ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ())","title":"Airport monitoring"},{"location":"examples/airport-monitoring/#parallel-jobs","text":"","title":"Parallel Jobs"},{"location":"examples/airport-monitoring/#example-airport-monitoring","text":"Get a Sentinel-2 clipped image for airports in a country. Run all jobs in parallel Visualize the results % load_ext autoreload % autoreload 2 import pandas as pd import geopandas as gpd from pathlib import Path import up42","title":"Example: Airport monitoring"},{"location":"examples/airport-monitoring/#10-random-airports-in-a-spain","text":"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) airports = airports . sample ( 10 ) # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) # Buffer airport point locations by around 100m airports . geometry = airports . geometry . buffer ( 0.001 ) airports . iloc [ 0 ] . geometry","title":"10 random airports in a Spain"},{"location":"examples/airport-monitoring/#prepare-up42-workflows","text":"# Authentificate api = up42 . authenticate ( cfg_file = \"config.json\" ) project = up42 . initialize_project () project # Only works when you have added your credit card information to the UP42 account. project . update_project_settings ( max_concurrent_jobs = 10 ) workflow = project . create_workflow ( \"workflow_demo_airplanes\" , use_existing = True ) workflow # Fill the workflow with tasks blocks = up42 . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ blocks [ selected_block ]]) workflow . get_workflow_tasks ( basic = True )","title":"Prepare UP42 workflows"},{"location":"examples/airport-monitoring/#run-jobs-in-parallel","text":"Very crude, this will soon be available in the API in one command! # Run jobs in parallel jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameters ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . create_and_run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) # Download results: out_filepaths = [] for job in jobs : fp = job . download_results () out_filepaths . append ( fp [ 0 ]) print ( \"finished\" ) print ( out_filepaths ) # Visualize downloaded results up42 . plot_results ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ())","title":"Run jobs in parallel"},{"location":"examples/examples-intro/","text":"Examples \u00b6 This section provides some more extensive examples of usage examples and use cases for the UP42 Python SDK. Also see the Jupyter notebooks in the examples folder . Airport monitoring with parallel jobs","title":"Examples"},{"location":"examples/examples-intro/#examples","text":"This section provides some more extensive examples of usage examples and use cases for the UP42 Python SDK. Also see the Jupyter notebooks in the examples folder . Airport monitoring with parallel jobs","title":"Examples"},{"location":"reference/job/","text":"Job class \u00b6 \u00b6 __init__ ( self , auth , project_id , job_id , order_ids = None ) special \u00b6 The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json Source code in up42/job.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , auth : Auth , project_id : str , job_id : str , order_ids : List [ str ] = None , ): \"\"\"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . quicklooks = None self . results = None if order_ids is None : self . order_ids = [ \"\" ] if self . auth . get_info : self . info = self . _get_info () cancel_job ( self ) \u00b6 Cancels a pending or running job. Source code in up42/job.py 113 114 115 116 117 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . auth . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . auth . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id ) download_quicklooks ( self , output_directory = None ) \u00b6 Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). Source code in up42/job.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). \"\"\" # Currently only the first/data task produces quicklooks. data_task = self . get_jobtasks ()[ 0 ] out_paths : List [ str ] = data_task . download_quicklooks ( # type: ignore output_directory = output_directory ) # type: ignore self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths download_results ( self , output_directory = None ) \u00b6 Downloads and unpacks the job results. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/job.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def download_results ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads and unpacks the job results. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of job %s \" , self . job_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths get_jobtasks ( self , return_json = False ) \u00b6 Get the individual items of the job as JobTask objects or json. Parameters: Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns: Type Description Union[List[JobTask], List[Dict]] The job task objects in a list. Source code in up42/job.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def get_jobtasks ( self , return_json : bool = False ) -> Union [ List [ \"JobTask\" ], List [ Dict ]]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_json : List [ Dict ] = response_json [ \"data\" ] jobtasks = [ JobTask ( auth = self . auth , project_id = self . project_id , job_id = self . job_id , jobtask_id = task [ \"id\" ], ) for task in jobtasks_json ] if return_json : return jobtasks_json else : return jobtasks get_jobtasks_results_json ( self ) \u00b6 Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: Type Description Dict The data.json of alle single job tasks. Source code in up42/job.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def get_jobtasks_results_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] jobtasks_results_json = {} for jobtask_id in jobtasks_ids : url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { jobtask_id } /outputs/data-json\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_results_json [ jobtask_id ] = response_json return jobtasks_results_json get_logs ( self , as_print = True , as_return = False ) \u00b6 Convenience function to print or return the logs of all job tasks. Parameters: Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns: Type Description The log strings (only if as_return was selected). Source code in up42/job.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def get_logs ( self , as_print : bool = True , as_return : bool = False ): \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( jobtasks_ids ), jobtasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( jobtasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , jobtask_id in enumerate ( jobtasks_ids ): url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { jobtask_id } /logs\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) job_logs [ jobtask_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with jobtask_id { jobtask_id } : \\n \" ) print ( response_json ) if as_return : return job_logs get_results_json ( self , as_dataframe = False ) \u00b6 Gets the Job results data.json. Parameters: Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. Source code in up42/job.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Job results data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json get_status ( self ) \u00b6 Gets the job status. Returns: Type Description str The job status. Source code in up42/job.py 60 61 62 63 64 65 66 67 68 69 70 71 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status map_results ( self , name_column = None , info_columns = None ) \u00b6 Displays data.json, and if available, one or multiple results geotiffs Additional columns that are shown when a feature is clicked. Source code in up42/job.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def map_results ( self , name_column : str = None , info_columns : List = None ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs name_column: Name of the column that provides the layer name. info_columns: Additional columns that are shown when a feature is clicked. \"\"\" if not is_notebook (): raise ValueError ( \"Only works in Jupyter notebook.\" ) df : gpd . GeoDataFrame = self . get_results_json ( as_dataframe = True ) # type: ignore # TODO: centroid of total_bounds centroid = df . iloc [ 0 ] . geometry . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) # Add features from data.json. def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } for index , row in df . iterrows (): # type: ignore try : layer_name = row [ name_column ] except KeyError : layer_name = f \"Layer { index + 1 } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , # ('{}{}'.format(row['dep'], row['dest'])), style_function = _style_function , highlight_function = _highlight_function , ) if not info_columns : folium . Popup ( f \" { layer_name } \" ) . add_to ( f ) else : if not isinstance ( info_columns , list ): raise ValueError ( \"Provide a list!\" ) infos = [ f \" { row [ info_col ] } \\n \" for info_col in info_columns ] infos = \"\" . join ( infos ) # type: ignore folium . Popup ( f \" { layer_name } \\n { infos } \" ) . add_to ( f ) f . add_to ( m ) # Same: folium.GeoJson(df, name=name_column, style_function=style_function, # highlight_function=highlight_function).add_to(map) # TODO: Not ideal, our streaming images are webmercator, folium requires wgs 84.0 # TODO: Switch to ipyleaflet! # This requires reprojecting on the user pc, not via the api. # Reproject raster and add to map dst_crs = 4326 results : List [ Path ] = self . results for idx , raster_fp in enumerate ( results ): with rasterio . open ( raster_fp ) as src : dst_profile = src . meta . copy () if src . crs != dst_crs : transform , width , height = calculate_default_transform ( src . crs , dst_crs , src . width , src . height , * src . bounds ) dst_profile . update ( { \"crs\" : dst_crs , \"transform\" : transform , \"width\" : width , \"height\" : height , } ) with MemoryFile () as memfile : with memfile . open ( ** dst_profile ) as mem : for i in range ( 1 , src . count + 1 ): reproject ( source = rasterio . band ( src , i ), destination = rasterio . band ( mem , i ), src_transform = src . transform , src_crs = src . crs , dst_transform = transform , dst_crs = dst_crs , resampling = Resampling . nearest , ) # TODO: What if more bands than 3-4? dst_array = mem . read () minx , miny , maxx , maxy = mem . bounds dst_array = np . moveaxis ( np . stack ( dst_array ), 0 , 2 ) m . add_child ( folium . raster_layers . ImageOverlay ( dst_array , bounds = [[ miny , minx ], [ maxy , maxx ]], # andere reihenfolge. name = f \"Image - { idx } - { raster_fp } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement #type: ignore collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) display ( m ) track_status ( self , report_time = 30 ) \u00b6 Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Parameters: Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 Source code in up42/job.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def track_status ( self , report_time : int = 30 ) -> str : \"\"\" Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" logger . info ( \"Tracking job status continuously, reporting every %s seconds...\" , report_time , ) status = \"NOT STARTED\" time_asleep = 0 while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: if time_asleep != 0 and time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) elif status in [ \"FAILED\" , \"ERROR\" ]: logger . info ( \"Job is %s ! - %s - Printing logs ...\" , status , self . job_id ) self . get_logs ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) elif status in [ \"CANCELLED\" , \"CANCELLING\" ]: logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) raise ValueError ( \"Job has been cancelled!\" ) elif status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully! - %s \" , self . job_id ) sleep ( 5 ) time_asleep += 5 return status upload_results_to_bucket ( self , gs_client , bucket , folder , extension = '.tgz' , version = 'v0' ) \u00b6 Uploads the results to a custom google cloud storage bucket. Source code in up42/job.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def upload_results_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the results to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s results with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s results to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" )","title":"Job"},{"location":"reference/job/#job-class","text":"","title":"Job class"},{"location":"reference/job/#up42.job.Job","text":"","title":"up42.job.Job"},{"location":"reference/job/#up42.job.Job.__init__","text":"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json Source code in up42/job.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def __init__ ( self , auth : Auth , project_id : str , job_id : str , order_ids : List [ str ] = None , ): \"\"\"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . quicklooks = None self . results = None if order_ids is None : self . order_ids = [ \"\" ] if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/job/#up42.job.Job.cancel_job","text":"Cancels a pending or running job. Source code in up42/job.py 113 114 115 116 117 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . auth . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . auth . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id )","title":"cancel_job()"},{"location":"reference/job/#up42.job.Job.download_quicklooks","text":"Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). Source code in up42/job.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). \"\"\" # Currently only the first/data task produces quicklooks. data_task = self . get_jobtasks ()[ 0 ] out_paths : List [ str ] = data_task . download_quicklooks ( # type: ignore output_directory = output_directory ) # type: ignore self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/job/#up42.job.Job.download_results","text":"Downloads and unpacks the job results. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/job.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def download_results ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads and unpacks the job results. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of job %s \" , self . job_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths","title":"download_results()"},{"location":"reference/job/#up42.job.Job.get_jobtasks","text":"Get the individual items of the job as JobTask objects or json. Parameters: Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns: Type Description Union[List[JobTask], List[Dict]] The job task objects in a list. Source code in up42/job.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 def get_jobtasks ( self , return_json : bool = False ) -> Union [ List [ \"JobTask\" ], List [ Dict ]]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_json : List [ Dict ] = response_json [ \"data\" ] jobtasks = [ JobTask ( auth = self . auth , project_id = self . project_id , job_id = self . job_id , jobtask_id = task [ \"id\" ], ) for task in jobtasks_json ] if return_json : return jobtasks_json else : return jobtasks","title":"get_jobtasks()"},{"location":"reference/job/#up42.job.Job.get_jobtasks_results_json","text":"Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: Type Description Dict The data.json of alle single job tasks. Source code in up42/job.py 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 def get_jobtasks_results_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] jobtasks_results_json = {} for jobtask_id in jobtasks_ids : url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { jobtask_id } /outputs/data-json\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_results_json [ jobtask_id ] = response_json return jobtasks_results_json","title":"get_jobtasks_results_json()"},{"location":"reference/job/#up42.job.Job.get_logs","text":"Convenience function to print or return the logs of all job tasks. Parameters: Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns: Type Description The log strings (only if as_return was selected). Source code in up42/job.py 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def get_logs ( self , as_print : bool = True , as_return : bool = False ): \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( jobtasks_ids ), jobtasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( jobtasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , jobtask_id in enumerate ( jobtasks_ids ): url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { jobtask_id } /logs\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) job_logs [ jobtask_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with jobtask_id { jobtask_id } : \\n \" ) print ( response_json ) if as_return : return job_logs","title":"get_logs()"},{"location":"reference/job/#up42.job.Job.get_results_json","text":"Gets the Job results data.json. Parameters: Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. Source code in up42/job.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Job results data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"get_results_json()"},{"location":"reference/job/#up42.job.Job.get_status","text":"Gets the job status. Returns: Type Description str The job status. Source code in up42/job.py 60 61 62 63 64 65 66 67 68 69 70 71 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status","title":"get_status()"},{"location":"reference/job/#up42.job.Job.map_results","text":"Displays data.json, and if available, one or multiple results geotiffs Additional columns that are shown when a feature is clicked. Source code in up42/job.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 def map_results ( self , name_column : str = None , info_columns : List = None ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs name_column: Name of the column that provides the layer name. info_columns: Additional columns that are shown when a feature is clicked. \"\"\" if not is_notebook (): raise ValueError ( \"Only works in Jupyter notebook.\" ) df : gpd . GeoDataFrame = self . get_results_json ( as_dataframe = True ) # type: ignore # TODO: centroid of total_bounds centroid = df . iloc [ 0 ] . geometry . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) # Add features from data.json. def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } for index , row in df . iterrows (): # type: ignore try : layer_name = row [ name_column ] except KeyError : layer_name = f \"Layer { index + 1 } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , # ('{}{}'.format(row['dep'], row['dest'])), style_function = _style_function , highlight_function = _highlight_function , ) if not info_columns : folium . Popup ( f \" { layer_name } \" ) . add_to ( f ) else : if not isinstance ( info_columns , list ): raise ValueError ( \"Provide a list!\" ) infos = [ f \" { row [ info_col ] } \\n \" for info_col in info_columns ] infos = \"\" . join ( infos ) # type: ignore folium . Popup ( f \" { layer_name } \\n { infos } \" ) . add_to ( f ) f . add_to ( m ) # Same: folium.GeoJson(df, name=name_column, style_function=style_function, # highlight_function=highlight_function).add_to(map) # TODO: Not ideal, our streaming images are webmercator, folium requires wgs 84.0 # TODO: Switch to ipyleaflet! # This requires reprojecting on the user pc, not via the api. # Reproject raster and add to map dst_crs = 4326 results : List [ Path ] = self . results for idx , raster_fp in enumerate ( results ): with rasterio . open ( raster_fp ) as src : dst_profile = src . meta . copy () if src . crs != dst_crs : transform , width , height = calculate_default_transform ( src . crs , dst_crs , src . width , src . height , * src . bounds ) dst_profile . update ( { \"crs\" : dst_crs , \"transform\" : transform , \"width\" : width , \"height\" : height , } ) with MemoryFile () as memfile : with memfile . open ( ** dst_profile ) as mem : for i in range ( 1 , src . count + 1 ): reproject ( source = rasterio . band ( src , i ), destination = rasterio . band ( mem , i ), src_transform = src . transform , src_crs = src . crs , dst_transform = transform , dst_crs = dst_crs , resampling = Resampling . nearest , ) # TODO: What if more bands than 3-4? dst_array = mem . read () minx , miny , maxx , maxy = mem . bounds dst_array = np . moveaxis ( np . stack ( dst_array ), 0 , 2 ) m . add_child ( folium . raster_layers . ImageOverlay ( dst_array , bounds = [[ miny , minx ], [ maxy , maxx ]], # andere reihenfolge. name = f \"Image - { idx } - { raster_fp } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement #type: ignore collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) display ( m )","title":"map_results()"},{"location":"reference/job/#up42.job.Job.track_status","text":"Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Parameters: Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 Source code in up42/job.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def track_status ( self , report_time : int = 30 ) -> str : \"\"\" Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" logger . info ( \"Tracking job status continuously, reporting every %s seconds...\" , report_time , ) status = \"NOT STARTED\" time_asleep = 0 while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: if time_asleep != 0 and time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) elif status in [ \"FAILED\" , \"ERROR\" ]: logger . info ( \"Job is %s ! - %s - Printing logs ...\" , status , self . job_id ) self . get_logs ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) elif status in [ \"CANCELLED\" , \"CANCELLING\" ]: logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) raise ValueError ( \"Job has been cancelled!\" ) elif status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully! - %s \" , self . job_id ) sleep ( 5 ) time_asleep += 5 return status","title":"track_status()"},{"location":"reference/job/#up42.job.Job.upload_results_to_bucket","text":"Uploads the results to a custom google cloud storage bucket. Source code in up42/job.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def upload_results_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the results to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s results with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s results to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" )","title":"upload_results_to_bucket()"},{"location":"reference/jobtask/","text":"JobTask class \u00b6 \u00b6 __init__ ( self , auth , project_id , job_id , jobtask_id ) special \u00b6 The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks Source code in up42/jobtask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , auth : Auth , project_id : str , job_id : str , jobtask_id : str , ): \"\"\"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . jobtask_id = jobtask_id self . quicklooks = None self . results = None if self . auth . get_info : self . info = self . _get_info () download_quicklooks ( self , output_directory = None ) \u00b6 Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] The quicklooks filepaths. Source code in up42/jobtask.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Args: output_directory: The file output directory, defaults to the current working directory. Returns: The quicklooks filepaths. \"\"\" if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) quicklooks_ids = response_json [ \"data\" ] out_paths : List [ str ] = [] for ql_id in tqdm ( quicklooks_ids ): out_path = output_directory / f \"quicklook_ { ql_id } \" # No suffix required. out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/ { ql_id } \" ) response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths download_results ( self , output_directory = None ) \u00b6 Downloads and unpacks the jobtask results. Default download to Desktop. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/jobtask.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def download_results ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Downloads and unpacks the jobtask results. Default download to Desktop. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of jobtask %s \" , self . jobtask_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths get_results_json ( self , as_dataframe = False ) \u00b6 Gets the Jobtask results data.json. Parameters: Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe. Source code in up42/jobtask.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Jobtask results data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . auth . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"JobTask"},{"location":"reference/jobtask/#jobtask-class","text":"","title":"JobTask class"},{"location":"reference/jobtask/#up42.jobtask.JobTask","text":"","title":"up42.jobtask.JobTask"},{"location":"reference/jobtask/#up42.jobtask.JobTask.__init__","text":"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks Source code in up42/jobtask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , auth : Auth , project_id : str , job_id : str , jobtask_id : str , ): \"\"\"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . jobtask_id = jobtask_id self . quicklooks = None self . results = None if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_quicklooks","text":"Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] The quicklooks filepaths. Source code in up42/jobtask.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Args: output_directory: The file output directory, defaults to the current working directory. Returns: The quicklooks filepaths. \"\"\" if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) quicklooks_ids = response_json [ \"data\" ] out_paths : List [ str ] = [] for ql_id in tqdm ( quicklooks_ids ): out_path = output_directory / f \"quicklook_ { ql_id } \" # No suffix required. out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/ { ql_id } \" ) response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_results","text":"Downloads and unpacks the jobtask results. Default download to Desktop. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/jobtask.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def download_results ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Downloads and unpacks the jobtask results. Default download to Desktop. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of jobtask %s \" , self . jobtask_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths","title":"download_results()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.get_results_json","text":"Gets the Jobtask results data.json. Parameters: Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe. Source code in up42/jobtask.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Gets the Jobtask results data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . auth . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) if as_dataframe : # UP42 results are always in EPSG 4326 df = gpd . GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"get_results_json()"},{"location":"reference/project/","text":"Project class \u00b6 \u00b6 __init__ ( self , auth , project_id ) special \u00b6 The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_project_settings, update_project_settings Source code in up42/project.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , auth : Auth , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_project_settings, update_project_settings \"\"\" self . auth = auth self . project_id = project_id if self . auth . get_info : self . info = self . _get_info () create_workflow ( self , name , description = '' , use_existing = False ) \u00b6 Creates a new workflow and returns a workflow object. Parameters: Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns: Type Description Workflow The workflow object. Source code in up42/project.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . info ( \"Getting existing workflows in project ...\" ) logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = Workflow ( self . auth , project_id = self . project_id , workflow_id = workflow_id ) return workflow get_project_settings ( self ) \u00b6 Gets the project settings. Returns: Type Description List The project settings. Source code in up42/project.py 113 114 115 116 117 118 119 120 121 122 123 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings get_workflows ( self , return_json = False ) \u00b6 Gets all workflows in a project as workflow objects or json. Parameters: Name Type Description Default return_json bool True returns Workflow Objects. False Returns: Type Description Union[List[Workflow], Dict] Workflow objects in the project or alternatively json info of the workflows. Source code in up42/project.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ Workflow ( self . auth , project_id = self . project_id , workflow_id = work [ \"id\" ]) for work in tqdm ( workflows_json ) ] return workflows update_project_settings ( self , max_aoi_size = None , max_concurrent_jobs = None , number_of_images = None ) \u00b6 Updates a project's settings. Parameters: Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images The maximum number of images returned with each job, from 1-20, default 10. None Source code in up42/project.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: The maximum number of images returned with each job, from 1-20, default 10. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . auth . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload )","title":"Project"},{"location":"reference/project/#project-class","text":"","title":"Project class"},{"location":"reference/project/#up42.project.Project","text":"","title":"up42.project.Project"},{"location":"reference/project/#up42.project.Project.__init__","text":"The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_project_settings, update_project_settings Source code in up42/project.py 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , auth : Auth , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_project_settings, update_project_settings \"\"\" self . auth = auth self . project_id = project_id if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/project/#up42.project.Project.create_workflow","text":"Creates a new workflow and returns a workflow object. Parameters: Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns: Type Description Workflow The workflow object. Source code in up42/project.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . info ( \"Getting existing workflows in project ...\" ) logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = Workflow ( self . auth , project_id = self . project_id , workflow_id = workflow_id ) return workflow","title":"create_workflow()"},{"location":"reference/project/#up42.project.Project.get_project_settings","text":"Gets the project settings. Returns: Type Description List The project settings. Source code in up42/project.py 113 114 115 116 117 118 119 120 121 122 123 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings","title":"get_project_settings()"},{"location":"reference/project/#up42.project.Project.get_workflows","text":"Gets all workflows in a project as workflow objects or json. Parameters: Name Type Description Default return_json bool True returns Workflow Objects. False Returns: Type Description Union[List[Workflow], Dict] Workflow objects in the project or alternatively json info of the workflows. Source code in up42/project.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ Workflow ( self . auth , project_id = self . project_id , workflow_id = work [ \"id\" ]) for work in tqdm ( workflows_json ) ] return workflows","title":"get_workflows()"},{"location":"reference/project/#up42.project.Project.update_project_settings","text":"Updates a project's settings. Parameters: Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images The maximum number of images returned with each job, from 1-20, default 10. None Source code in up42/project.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: The maximum number of images returned with each job, from 1-20, default 10. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . auth . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload )","title":"update_project_settings()"},{"location":"reference/tools/","text":"Tools class \u00b6 \u00b6 __init__ ( self , auth = None ) special \u00b6 The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks Source code in up42/tools.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , auth = None ): \"\"\" The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks \"\"\" if auth : self . auth = auth self . quicklooks = None self . results = None draw_aoi ( self ) \u00b6 Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. Source code in up42/tools.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def draw_aoi ( self ) -> None : \"\"\" Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. \"\"\" if not is_notebook (): raise ValueError ( \"Only works in Jupyter notebook.\" ) m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) display ( m ) get_block_details ( self , block_id , as_dataframe = False ) \u00b6 Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Parameters: Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Dict A dict of the block details metadata for the specific block. Source code in up42/tools.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" url = f \" { self . auth . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . auth . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json get_blocks ( self , block_type = None , basic = True , as_dataframe = False ) \u00b6 Gets a list of all public blocks on the marketplace. Parameters: Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version {block_id : block_name} True as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Union[List[Dict], Dict] A list of the public blocks and their metadata. Optional a simpler version dict. Source code in up42/tools.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ List [ Dict ], Dict ]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass url = f \" { self . auth . _endpoint () } /blocks\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting blocks name and id, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json get_example_aoi ( self , location = 'Berlin' , as_dataframe = False ) \u00b6 Gets predefined, small, rectangular example aoi for the selected location. Parameters: Name Type Description Default location str Location, one of Berlin, Washington. 'Berlin' as_dataframe bool Returns a dataframe instead of dict FeatureColletions (default). False Returns: Type Description Union[dict, geopandas.geodataframe.GeoDataFrame] Feature collection json with the selected aoi. Source code in up42/tools.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> Union [ dict , gpd . GeoDataFrame ]: \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washington. as_dataframe: Returns a dataframe instead of dict FeatureColletions (default). Returns: Feature collection json with the selected aoi. \"\"\" logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_washington.geojson\" ) else : raise ValueError ( \"Please select one of 'Berlin' or 'Washington' as the \" \"location!\" ) if as_dataframe : df = gpd . GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi plot_coverage ( scenes , aoi = None , legend_column = 'scene_id' , figsize = ( 12 , 16 )) staticmethod \u00b6 Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Parameters: Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, results of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. 'scene_id' figsize Matplotlib figure size. (12, 16) Source code in up42/tools.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @staticmethod def plot_coverage ( scenes : gpd . GeoDataFrame , aoi : gpd . GeoDataFrame = None , legend_column : str = \"scene_id\" , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Args: scenes: GeoDataFrame of scenes, results of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. figsize: Matplotlib figure size. \"\"\" if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) if legend_column not in scenes . columns : legend_column = None # type: ignore logger . info ( \"Given legend_column name not in scene dataframe, \" \"plotting without legend.\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show () plot_quicklooks ( self , figsize = ( 8 , 8 ), filepaths = None ) \u00b6 Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) Source code in up42/tools.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def plot_quicklooks ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. \"\"\" if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) # TODO: Remove empty axes & give title option. if filepaths is None : if self . quicklooks is None : raise ValueError ( \"You first need to download the quicklooks via .download_quicklooks().\" ) filepaths = self . quicklooks if len ( filepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 2 nrows = int ( math . ceil ( len ( filepaths ) / float ( ncols ))) warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( filepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , fp in enumerate ( filepaths ): with rasterio . open ( fp ) as src : show ( src . read (), transform = src . transform , title = Path ( fp ) . stem , ax = axs [ idx ], ) plt . tight_layout () plt . show () plot_results ( self , figsize = ( 8 , 8 ), filepaths = None , titles = None ) \u00b6 Plots the downloaded data. Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[Union[str, pathlib.Path]] Paths to images to plot. Optional, by default picks up the downloaded results. None Source code in up42/tools.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def plot_results ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ Union [ str , Path ]] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the downloaded results. \"\"\" # TODO: Handle more bands. # TODO: add histogram equalization? But requires skimage dependency. if filepaths is None : if self . results is None : raise ValueError ( \"You first need to download the results.\" ) filepaths = self . results if not isinstance ( filepaths , list ): filepaths = [ filepaths ] filepaths = [ Path ( path ) for path in filepaths ] plot_file_format = [ \".tif\" ] # TODO: Add other fileformats. imagepaths = [ path for path in filepaths if str ( path . suffix ) in plot_file_format # type: ignore ] if not imagepaths : raise ValueError ( f \"Only results of the formats { plot_file_format } can \" \"currently be plotted.\" ) if not titles : titles = [ Path ( fp ) . stem for fp in imagepaths ] if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) else : raise ValueError ( \"Only works in Jupyter notebook.\" ) if len ( imagepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 3 nrows = int ( math . ceil ( len ( imagepaths ) / float ( ncols ))) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( imagepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , ( fp , title ) in enumerate ( zip ( imagepaths , titles )): with rasterio . open ( fp ) as src : img_array = src . read () show ( img_array , transform = src . transform , title = title , ax = axs [ idx ], ) axs [ idx ] . set_axis_off () plt . tight_layout () plt . show () read_vector_file ( self , filename = 'aoi.geojson' , as_dataframe = False ) \u00b6 Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters: Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Feature Collection Source code in up42/tools.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = gpd . GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs != \"epsg:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__ validate_manifest ( self , path_or_json ) \u00b6 Validates the block manifest, input either manifest json string or filepath. Parameters: Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns: Type Description Dict A dictionary with the validation results and potential validation errors. Example { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } } Source code in up42/tools.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation results and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json url = f \" { self . auth . _endpoint () } /validate-schema/block\" response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ]","title":"Tools"},{"location":"reference/tools/#tools-class","text":"","title":"Tools class"},{"location":"reference/tools/#up42.tools.Tools","text":"","title":"up42.tools.Tools"},{"location":"reference/tools/#up42.tools.Tools.__init__","text":"The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks Source code in up42/tools.py 23 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , auth = None ): \"\"\" The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks \"\"\" if auth : self . auth = auth self . quicklooks = None self . results = None","title":"__init__()"},{"location":"reference/tools/#up42.tools.Tools.draw_aoi","text":"Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. Source code in up42/tools.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def draw_aoi ( self ) -> None : \"\"\" Opens a interactive map to draw an aoi by hand, export via the export button. Then read in via read_aoi_file(). Currently no way to get the drawn geometry via a callback in Python, as not supported by folium. And ipyleaflet misses raster vizualization & folium plugins functionality. \"\"\" if not is_notebook (): raise ValueError ( \"Only works in Jupyter notebook.\" ) m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) display ( m )","title":"draw_aoi()"},{"location":"reference/tools/#up42.tools.Tools.get_block_details","text":"Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Parameters: Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Dict A dict of the block details metadata for the specific block. Source code in up42/tools.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" url = f \" { self . auth . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . auth . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json","title":"get_block_details()"},{"location":"reference/tools/#up42.tools.Tools.get_blocks","text":"Gets a list of all public blocks on the marketplace. Parameters: Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version {block_id : block_name} True as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Union[List[Dict], Dict] A list of the public blocks and their metadata. Optional a simpler version dict. Source code in up42/tools.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ List [ Dict ], Dict ]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass url = f \" { self . auth . _endpoint () } /blocks\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting blocks name and id, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json","title":"get_blocks()"},{"location":"reference/tools/#up42.tools.Tools.get_example_aoi","text":"Gets predefined, small, rectangular example aoi for the selected location. Parameters: Name Type Description Default location str Location, one of Berlin, Washington. 'Berlin' as_dataframe bool Returns a dataframe instead of dict FeatureColletions (default). False Returns: Type Description Union[dict, geopandas.geodataframe.GeoDataFrame] Feature collection json with the selected aoi. Source code in up42/tools.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> Union [ dict , gpd . GeoDataFrame ]: \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washington. as_dataframe: Returns a dataframe instead of dict FeatureColletions (default). Returns: Feature collection json with the selected aoi. \"\"\" logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_washington.geojson\" ) else : raise ValueError ( \"Please select one of 'Berlin' or 'Washington' as the \" \"location!\" ) if as_dataframe : df = gpd . GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi","title":"get_example_aoi()"},{"location":"reference/tools/#up42.tools.Tools.plot_coverage","text":"Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Parameters: Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, results of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. 'scene_id' figsize Matplotlib figure size. (12, 16) Source code in up42/tools.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 @staticmethod def plot_coverage ( scenes : gpd . GeoDataFrame , aoi : gpd . GeoDataFrame = None , legend_column : str = \"scene_id\" , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Args: scenes: GeoDataFrame of scenes, results of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. figsize: Matplotlib figure size. \"\"\" if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) if legend_column not in scenes . columns : legend_column = None # type: ignore logger . info ( \"Given legend_column name not in scene dataframe, \" \"plotting without legend.\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show ()","title":"plot_coverage()"},{"location":"reference/tools/#up42.tools.Tools.plot_quicklooks","text":"Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) Source code in up42/tools.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def plot_quicklooks ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. \"\"\" if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) # TODO: Remove empty axes & give title option. if filepaths is None : if self . quicklooks is None : raise ValueError ( \"You first need to download the quicklooks via .download_quicklooks().\" ) filepaths = self . quicklooks if len ( filepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 2 nrows = int ( math . ceil ( len ( filepaths ) / float ( ncols ))) warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( filepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , fp in enumerate ( filepaths ): with rasterio . open ( fp ) as src : show ( src . read (), transform = src . transform , title = Path ( fp ) . stem , ax = axs [ idx ], ) plt . tight_layout () plt . show ()","title":"plot_quicklooks()"},{"location":"reference/tools/#up42.tools.Tools.plot_results","text":"Plots the downloaded data. Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[Union[str, pathlib.Path]] Paths to images to plot. Optional, by default picks up the downloaded results. None Source code in up42/tools.py 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def plot_results ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ Union [ str , Path ]] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the downloaded results. \"\"\" # TODO: Handle more bands. # TODO: add histogram equalization? But requires skimage dependency. if filepaths is None : if self . results is None : raise ValueError ( \"You first need to download the results.\" ) filepaths = self . results if not isinstance ( filepaths , list ): filepaths = [ filepaths ] filepaths = [ Path ( path ) for path in filepaths ] plot_file_format = [ \".tif\" ] # TODO: Add other fileformats. imagepaths = [ path for path in filepaths if str ( path . suffix ) in plot_file_format # type: ignore ] if not imagepaths : raise ValueError ( f \"Only results of the formats { plot_file_format } can \" \"currently be plotted.\" ) if not titles : titles = [ Path ( fp ) . stem for fp in imagepaths ] if is_notebook (): get_ipython () . run_line_magic ( \"matplotlib\" , \"inline\" ) else : raise ValueError ( \"Only works in Jupyter notebook.\" ) if len ( imagepaths ) < 2 : nrows , ncols = 1 , 1 else : ncols = 3 nrows = int ( math . ceil ( len ( imagepaths ) / float ( ncols ))) fig , axs = plt . subplots ( nrows = nrows , ncols = ncols , figsize = figsize ) if len ( imagepaths ) > 1 : axs = axs . ravel () else : axs = [ axs ] for idx , ( fp , title ) in enumerate ( zip ( imagepaths , titles )): with rasterio . open ( fp ) as src : img_array = src . read () show ( img_array , transform = src . transform , title = title , ax = axs [ idx ], ) axs [ idx ] . set_axis_off () plt . tight_layout () plt . show ()","title":"plot_results()"},{"location":"reference/tools/#up42.tools.Tools.read_vector_file","text":"Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters: Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Feature Collection Source code in up42/tools.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> Union [ Dict , gpd . GeoDataFrame ]: \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = gpd . GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs != \"epsg:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__","title":"read_vector_file()"},{"location":"reference/tools/#up42.tools.Tools.validate_manifest","text":"Validates the block manifest, input either manifest json string or filepath. Parameters: Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns: Type Description Dict A dictionary with the validation results and potential validation errors. Example { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } } Source code in up42/tools.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation results and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json url = f \" { self . auth . _endpoint () } /validate-schema/block\" response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ]","title":"validate_manifest()"},{"location":"reference/workflow/","text":"Workflow class \u00b6 \u00b6 __init__ ( self , auth , project_id , workflow_id ) special \u00b6 The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, create_and_run_job, get_jobs, update_name, delete Source code in up42/workflow.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , auth : Auth , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, create_and_run_job, get_jobs, update_name, delete \"\"\" self . auth = auth self . project_id = project_id self . workflow_id = workflow_id if self . auth . get_info : self . info = self . _get_info () add_workflow_tasks ( self , input_tasks ) \u00b6 Adds or overwrites workflow tasks in a workflow on UP42. Parameters: Name Type Description Default input_tasks Union[List, List[Dict]] The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. required Example input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] Example input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] Source code in up42/workflow.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def add_workflow_tasks ( self , input_tasks : Union [ List , List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks in a workflow on UP42. Args: input_tasks: The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # TODO: User should be able to only provide block task names or display name. # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . auth . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks ) construct_parameters ( self , geometry = None , geometry_operation = None , handle_multiple_features = 'footprint' , start_date = None , end_date = None , limit = None , scene_ids = None , order_ids = None ) \u00b6 Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters: Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, geojson.geometry.Polygon, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon, shapely.geometry.point.Point]] One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns: Type Description Dict Dictionary of constructed input parameters. Source code in up42/workflow.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def construct_parameters ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , geojson . Polygon , List , gpd . GeoDataFrame , Polygon , Point , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not create_and_run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In case of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , # type: ignore squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters create_and_run_job ( self , input_parameters = None , test_query = False , track_status = False ) \u00b6 Creates and runs a new job. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None test_query bool If set, runs a test query (search for available imagery based on your data parameters). False track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False Returns: Type Description Job The spawned job object. Source code in up42/workflow.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def create_and_run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , test_query : bool = False , track_status : bool = False , ) -> \"Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. test_query: If set, runs a test query (search for available imagery based on your data parameters). track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. Returns: The spawned job object. \"\"\" if input_parameters is None : raise ValueError ( \"Select the job_parameters, use workflow.construct_parameters()!\" ) if isinstance ( input_parameters , ( str , Path )): with open ( input_parameters ) as src : input_parameters = json . load ( src ) logger . info ( \"Loading job parameters from json file.\" ) logger . info ( \"Selected input_parameters: %s .\" , input_parameters ) if test_query : # TODO: Fix type union input_parameters = input_parameters . copy () # type: ignore input_parameters . update ({ \"config\" : { \"mode\" : \"DRY_RUN\" }}) # type: ignore logger . info ( \"+++++++++++++++++++++++++++++++++\" ) logger . info ( \"Running this job as Test Query...\" ) logger . info ( \"+++++++++++++++++++++++++++++++++\" ) name = \"_py\" # Enables recognition of python API usage. url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /\" f \"workflows/ { self . workflow_id } /jobs?name= { name } \" ) response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = input_parameters ) job_json = response_json [ \"data\" ] logger . info ( \"Created and running new job: %s .\" , job_json [ \"id\" ]) job = Job ( self . auth , job_id = job_json [ \"id\" ], project_id = self . project_id ,) if track_status : job . track_status () return job delete ( self ) \u00b6 Deletes the workflow and sets the Python object to None. Source code in up42/workflow.py 411 412 413 414 415 416 417 418 419 420 421 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self get_compatible_blocks ( self ) \u00b6 Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. Source code in up42/workflow.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True ) . keys ())[ - 1 ] # type: ignore url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. compatible_blocks = { block [ \"name\" ]: block [ \"blockId\" ] for block in compatible_blocks } return compatible_blocks get_jobs ( self , return_json = False ) \u00b6 Get all jobs in the specific project as job objects or json. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/workflow.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs in the specific project as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_json ) ] return jobs get_parameters_info ( self ) \u00b6 Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Type Description Dict Workflow parameters info json. Source code in up42/workflow.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def get_parameters_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameters info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info get_workflow_tasks ( self , basic = False ) \u00b6 Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters: Name Type Description Default basic bool If selected returns a simplified task-name : task-id` version. False Returns: Type Description Union[List, Dict] The workflow task info. Source code in up42/workflow.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return { task [ \"name\" ]: task [ \"id\" ] for task in tasks } else : return tasks update_name ( self , name = None , description = None ) \u00b6 Updates the workflow name and description. Parameters: Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None Source code in up42/workflow.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update )","title":"Workflow"},{"location":"reference/workflow/#workflow-class","text":"","title":"Workflow class"},{"location":"reference/workflow/#up42.workflow.Workflow","text":"","title":"up42.workflow.Workflow"},{"location":"reference/workflow/#up42.workflow.Workflow.__init__","text":"The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, create_and_run_job, get_jobs, update_name, delete Source code in up42/workflow.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , auth : Auth , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, create_and_run_job, get_jobs, update_name, delete \"\"\" self . auth = auth self . project_id = project_id self . workflow_id = workflow_id if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/workflow/#up42.workflow.Workflow.add_workflow_tasks","text":"Adds or overwrites workflow tasks in a workflow on UP42. Parameters: Name Type Description Default input_tasks Union[List, List[Dict]] The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. required Example input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] Example input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] Source code in up42/workflow.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def add_workflow_tasks ( self , input_tasks : Union [ List , List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks in a workflow on UP42. Args: input_tasks: The input tasks, can be provided in the simplified (list of block ids, is automatically transformed to the full version) or full version (dict of block id, block name and parent block name). - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # TODO: User should be able to only provide block task names or display name. # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . auth . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks )","title":"add_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.construct_parameters","text":"Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters: Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, geojson.geometry.Polygon, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon, shapely.geometry.point.Point]] One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns: Type Description Dict Dictionary of constructed input parameters. Source code in up42/workflow.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def construct_parameters ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , geojson . Polygon , List , gpd . GeoDataFrame , Polygon , Point , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, gpd.GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not create_and_run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In case of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , # type: ignore squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters","title":"construct_parameters()"},{"location":"reference/workflow/#up42.workflow.Workflow.create_and_run_job","text":"Creates and runs a new job. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None test_query bool If set, runs a test query (search for available imagery based on your data parameters). False track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False Returns: Type Description Job The spawned job object. Source code in up42/workflow.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def create_and_run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , test_query : bool = False , track_status : bool = False , ) -> \"Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. test_query: If set, runs a test query (search for available imagery based on your data parameters). track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. Returns: The spawned job object. \"\"\" if input_parameters is None : raise ValueError ( \"Select the job_parameters, use workflow.construct_parameters()!\" ) if isinstance ( input_parameters , ( str , Path )): with open ( input_parameters ) as src : input_parameters = json . load ( src ) logger . info ( \"Loading job parameters from json file.\" ) logger . info ( \"Selected input_parameters: %s .\" , input_parameters ) if test_query : # TODO: Fix type union input_parameters = input_parameters . copy () # type: ignore input_parameters . update ({ \"config\" : { \"mode\" : \"DRY_RUN\" }}) # type: ignore logger . info ( \"+++++++++++++++++++++++++++++++++\" ) logger . info ( \"Running this job as Test Query...\" ) logger . info ( \"+++++++++++++++++++++++++++++++++\" ) name = \"_py\" # Enables recognition of python API usage. url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /\" f \"workflows/ { self . workflow_id } /jobs?name= { name } \" ) response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = input_parameters ) job_json = response_json [ \"data\" ] logger . info ( \"Created and running new job: %s .\" , job_json [ \"id\" ]) job = Job ( self . auth , job_id = job_json [ \"id\" ], project_id = self . project_id ,) if track_status : job . track_status () return job","title":"create_and_run_job()"},{"location":"reference/workflow/#up42.workflow.Workflow.delete","text":"Deletes the workflow and sets the Python object to None. Source code in up42/workflow.py 411 412 413 414 415 416 417 418 419 420 421 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self","title":"delete()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_compatible_blocks","text":"Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. Source code in up42/workflow.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True ) . keys ())[ - 1 ] # type: ignore url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. compatible_blocks = { block [ \"name\" ]: block [ \"blockId\" ] for block in compatible_blocks } return compatible_blocks","title":"get_compatible_blocks()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_jobs","text":"Get all jobs in the specific project as job objects or json. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/workflow.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs in the specific project as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_json ) ] return jobs","title":"get_jobs()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_parameters_info","text":"Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Type Description Dict Workflow parameters info json. Source code in up42/workflow.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def get_parameters_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameters info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info","title":"get_parameters_info()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_workflow_tasks","text":"Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters: Name Type Description Default basic bool If selected returns a simplified task-name : task-id` version. False Returns: Type Description Union[List, Dict] The workflow task info. Source code in up42/workflow.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return { task [ \"name\" ]: task [ \"id\" ] for task in tasks } else : return tasks","title":"get_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.update_name","text":"Updates the workflow name and description. Parameters: Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None Source code in up42/workflow.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update )","title":"update_name()"}]}