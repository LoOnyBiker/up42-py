{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"30-second-example/","text":"30 Second Example \u00b6 The UP42 Python package uses six classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask and Catalog & Tools A new workflow consisting of Sentinel-2 data and Image Sharpening is created. The area of interest and workflow parameters are defined. After running the job , the results are downloaded and visualized. import up42 up42 . authenticate ( project_id = \"12345\" , project_api_key = \"12345\" ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) # Add data and processing blocks to the workflow. print ( up42 . get_blocks ( basic = True )) input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow. aoi = workflow . get_example_aoi ( as_dataframe = True ) #aoi = workflow.read_vector_file(\"data/aoi_berlin.geojson\", as_dataframe=True) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , limit = 1 ) input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) print ( input_parameters ) # Run a test job to check data availability and configuration. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual job. job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) job . download_results () job . plot_results ()","title":"30 Second Example"},{"location":"30-second-example/#30-second-example","text":"The UP42 Python package uses six classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask and Catalog & Tools A new workflow consisting of Sentinel-2 data and Image Sharpening is created. The area of interest and workflow parameters are defined. After running the job , the results are downloaded and visualized. import up42 up42 . authenticate ( project_id = \"12345\" , project_api_key = \"12345\" ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) # Add data and processing blocks to the workflow. print ( up42 . get_blocks ( basic = True )) input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow. aoi = workflow . get_example_aoi ( as_dataframe = True ) #aoi = workflow.read_vector_file(\"data/aoi_berlin.geojson\", as_dataframe=True) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , limit = 1 ) input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) print ( input_parameters ) # Run a test job to check data availability and configuration. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual job. job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) job . download_results () job . plot_results ()","title":"30 Second Example"},{"location":"CHANGELOG/","text":"Release notes \u00b6 Upgrading \u00b6 To upgrade to the latest version of up42-py use pip : pip install up42-py --upgrade You can determine your currently installed version using this command: pip show up42-py Versions \u00b6 0.9.2 (2020-07-04) \u00b6 Fix inconsistency with job.map_results selecting the json instead of the image 0.9.1 (2020-06-25) \u00b6 Fixes typo in catalog search parameters 0.9.0 (2020-05-07) \u00b6 Enable block_name and block_display_name for workflow.add_workflow_tasks Replace requirement to specify provider by sensor for catalog.download_quicklooks Add option to disable logging in up42.settings Add project.get_jobs and limit workflow.get_jobs to jobs in the workflow. Fix download of all output files Job name selectabable in workflow.test_job and workflow.run_job (with added suffix _py) Fix crs issues in make job.map_results , make plotting functionalities more robust 0.8.3 (2020-04-30) \u00b6 Pin geopandas to 0.7.0, package requires new crs convention 0.8.2 (2020-04-27) \u00b6 Removed job.create_and_run_job , now split into job.test_job and job.run_job","title":"Releases"},{"location":"CHANGELOG/#release-notes","text":"","title":"Release notes"},{"location":"CHANGELOG/#upgrading","text":"To upgrade to the latest version of up42-py use pip : pip install up42-py --upgrade You can determine your currently installed version using this command: pip show up42-py","title":"Upgrading"},{"location":"CHANGELOG/#versions","text":"","title":"Versions"},{"location":"CHANGELOG/#092-2020-07-04","text":"Fix inconsistency with job.map_results selecting the json instead of the image","title":"0.9.2 (2020-07-04)"},{"location":"CHANGELOG/#091-2020-06-25","text":"Fixes typo in catalog search parameters","title":"0.9.1 (2020-06-25)"},{"location":"CHANGELOG/#090-2020-05-07","text":"Enable block_name and block_display_name for workflow.add_workflow_tasks Replace requirement to specify provider by sensor for catalog.download_quicklooks Add option to disable logging in up42.settings Add project.get_jobs and limit workflow.get_jobs to jobs in the workflow. Fix download of all output files Job name selectabable in workflow.test_job and workflow.run_job (with added suffix _py) Fix crs issues in make job.map_results , make plotting functionalities more robust","title":"0.9.0 (2020-05-07)"},{"location":"CHANGELOG/#083-2020-04-30","text":"Pin geopandas to 0.7.0, package requires new crs convention","title":"0.8.3 (2020-04-30)"},{"location":"CHANGELOG/#082-2020-04-27","text":"Removed job.create_and_run_job , now split into job.test_job and job.run_job","title":"0.8.2 (2020-04-27)"},{"location":"authentication/","text":"Authentication \u00b6 To create and run workflow you first need to authenticate with UP42 via your project credentials . Get your Project credentials \u00b6 Log in to UP42.com and create a new project or select an existing one. In the project's Developers section you can find the project_id and project_api_key . Authenticate \u00b6 Authenticate by passing the project credentials directly as arguments : import up42 up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) Alternatively, create a configuration json file and pass its file path: { \"project_id\" : \"123\" , \"project_api_key\" : \"456\" } import up42 up42 . authenticate ( cfg_file = \"config.json\" )","title":"Authentication"},{"location":"authentication/#authentication","text":"To create and run workflow you first need to authenticate with UP42 via your project credentials .","title":"Authentication"},{"location":"authentication/#get-your-project-credentials","text":"Log in to UP42.com and create a new project or select an existing one. In the project's Developers section you can find the project_id and project_api_key .","title":"Get your Project credentials"},{"location":"authentication/#authenticate","text":"Authenticate by passing the project credentials directly as arguments : import up42 up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) Alternatively, create a configuration json file and pass its file path: { \"project_id\" : \"123\" , \"project_api_key\" : \"456\" } import up42 up42 . authenticate ( cfg_file = \"config.json\" )","title":"Authenticate"},{"location":"catalog/","text":"Catalog Search \u00b6 Check data availability & download image preview quicklooks via the catalog search. You can filter by various parameters e.g. time period, area of interest, cloud cover etc. Currently the following sensors are supported: Pleiades, Spot, Sentinel1, Sentinel2, Sentinel3, Sentinel5p . Initialize Catalog \u00b6 import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) up42 . authenticate ( cfg_file = \"config.json\" ) catalog = up42 . initialize_catalog () Search scenes in aoi \u00b6 aoi = up42 . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) #aoi = up42.read_vector_file(\"data/aoi_washington.geojson\", # as_dataframe=False) search_parameters = catalog . construct_parameters ( geometry = aoi , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 5 ) search_results = catalog . search ( search_parameters = search_parameters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" ) Download & visualize quicklooks \u00b6 catalog . download_quicklooks ( image_ids = search_results . id . to_list (), sensor = \"pleiades\" ) catalog . plot_quicklooks ( figsize = ( 20 , 20 ))","title":"Catalog Search"},{"location":"catalog/#catalog-search","text":"Check data availability & download image preview quicklooks via the catalog search. You can filter by various parameters e.g. time period, area of interest, cloud cover etc. Currently the following sensors are supported: Pleiades, Spot, Sentinel1, Sentinel2, Sentinel3, Sentinel5p .","title":"Catalog Search"},{"location":"catalog/#initialize-catalog","text":"import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) up42 . authenticate ( cfg_file = \"config.json\" ) catalog = up42 . initialize_catalog ()","title":"Initialize Catalog"},{"location":"catalog/#search-scenes-in-aoi","text":"aoi = up42 . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) #aoi = up42.read_vector_file(\"data/aoi_washington.geojson\", # as_dataframe=False) search_parameters = catalog . construct_parameters ( geometry = aoi , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 5 ) search_results = catalog . search ( search_parameters = search_parameters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" )","title":"Search scenes in aoi"},{"location":"catalog/#download-visualize-quicklooks","text":"catalog . download_quicklooks ( image_ids = search_results . id . to_list (), sensor = \"pleiades\" ) catalog . plot_quicklooks ( figsize = ( 20 , 20 ))","title":"Download &amp; visualize quicklooks"},{"location":"cli/","text":"Command Line Interface (CLI) \u00b6 The CLI tool allows you to use the UP42 functionality from the command line. It is installed automatically with and based on the Python SDK. To check whether the tool is installed and functioning correctly, type the following on your terminal or command line. This will print out a summary of the available commands. up42 -h To get help on a specific command, use: up42 command -h Authenticate \u00b6 You can authenticate with a PROJECT_ID and PROJECT_API_KEY up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] auth Or using a config.json file: up42 -cfg [ path to config.json ] auth You can make the authentication persistent by storing either the project key pair or the path to the config file as an environment variable. export UP42_PROJECT_ID =[ PROJECT_ID ] export UP42_PROJECT_API_KEY =[ PROJECT_API_KEY ] Or when using config.json . export UP42_CFG_FILE =[ path to config.json ] To save the authentication for future sessions make sure to append these variables to your bash profile file: # Linux export UP42_CFG_FILE =[ path to config.json ] >> ~/.bashrc # MacOS export UP42_CFG_FILE =[ path to config.json ] >> ~/.bash_profile If you want to create a config.json file from a project key pair, you can use the config command. up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] config Workflows \u00b6 up42 workflow -h Create a new workflow: up42 project create-workflow a_test Or check which workflows already exist: up42 project get-workflows And get a workflow by its name: up42 project workflow-from-name -name a_test After running the command to persist the workflow you can get the workflow tasks: up42 workflow get-workflow-tasks You can also add workflow tasks to the workflow via a json file (see typical usage for an example): up42 workflow add-workflow-tasks new_workflow_tasks.json Jobs \u00b6 up42 job -h First create and run a new test job with parameters defined in a json file (see typical usage for an example): up42 workflow test-job input_parameters.json --track Then run the actual job with parameters: up42 workflow run-job input_parameters.json --track After running the command to persist the job you can download the quicklooks from job in current working directory (note that not all data blocks support quicklooks): up42 job download-quicklooks . Or download and unpack the results: up42 job download-results . You can also print out the logs of the job: up42 job get-log Catalog \u00b6 up42 catalog -h With the catalog commands you can easily search the UP42 catalog for data. First, create a parameter configuration: up42 catalog construct-parameters example_aoi.geojson --sensors pleiades --max-cloud-cover 5 Then get the results: up42 catalog search example_search_params.json General tools \u00b6 Get all public blocks in the platform: up42 get-blocks Get block details by name: up42 get-block-details -name oneatlas-pleiades-aoiclipped","title":"Command Line Interface (CLI)"},{"location":"cli/#command-line-interface-cli","text":"The CLI tool allows you to use the UP42 functionality from the command line. It is installed automatically with and based on the Python SDK. To check whether the tool is installed and functioning correctly, type the following on your terminal or command line. This will print out a summary of the available commands. up42 -h To get help on a specific command, use: up42 command -h","title":"Command Line Interface (CLI)"},{"location":"cli/#authenticate","text":"You can authenticate with a PROJECT_ID and PROJECT_API_KEY up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] auth Or using a config.json file: up42 -cfg [ path to config.json ] auth You can make the authentication persistent by storing either the project key pair or the path to the config file as an environment variable. export UP42_PROJECT_ID =[ PROJECT_ID ] export UP42_PROJECT_API_KEY =[ PROJECT_API_KEY ] Or when using config.json . export UP42_CFG_FILE =[ path to config.json ] To save the authentication for future sessions make sure to append these variables to your bash profile file: # Linux export UP42_CFG_FILE =[ path to config.json ] >> ~/.bashrc # MacOS export UP42_CFG_FILE =[ path to config.json ] >> ~/.bash_profile If you want to create a config.json file from a project key pair, you can use the config command. up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] config","title":"Authenticate"},{"location":"cli/#workflows","text":"up42 workflow -h Create a new workflow: up42 project create-workflow a_test Or check which workflows already exist: up42 project get-workflows And get a workflow by its name: up42 project workflow-from-name -name a_test After running the command to persist the workflow you can get the workflow tasks: up42 workflow get-workflow-tasks You can also add workflow tasks to the workflow via a json file (see typical usage for an example): up42 workflow add-workflow-tasks new_workflow_tasks.json","title":"Workflows"},{"location":"cli/#jobs","text":"up42 job -h First create and run a new test job with parameters defined in a json file (see typical usage for an example): up42 workflow test-job input_parameters.json --track Then run the actual job with parameters: up42 workflow run-job input_parameters.json --track After running the command to persist the job you can download the quicklooks from job in current working directory (note that not all data blocks support quicklooks): up42 job download-quicklooks . Or download and unpack the results: up42 job download-results . You can also print out the logs of the job: up42 job get-log","title":"Jobs"},{"location":"cli/#catalog","text":"up42 catalog -h With the catalog commands you can easily search the UP42 catalog for data. First, create a parameter configuration: up42 catalog construct-parameters example_aoi.geojson --sensors pleiades --max-cloud-cover 5 Then get the results: up42 catalog search example_search_params.json","title":"Catalog"},{"location":"cli/#general-tools","text":"Get all public blocks in the platform: up42 get-blocks Get block details by name: up42 get-block-details -name oneatlas-pleiades-aoiclipped","title":"General tools"},{"location":"detailed-example/","text":"Detailed Example \u00b6 This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps. Authenticate & access project \u00b6 import up42 up42 . authenticate ( project_id = \"12345\" , project_api_key = \"12345\" ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project Get information about the available blocks to later construct your workflow. up42 . get_blocks ( basic = True ) Create or access the workflow \u00b6 You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) workflow # Add workflow tasks - simple version. See above .get_blocks() result. input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control # about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access an existing workflow within the project by its workflow_id UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow Select the aoi \u00b6 There are multiple ways to select an aoi: Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. Use .draw_aoi() to draw the aoi and export it as a geojson. Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) #aoi = workflow.get_example_aoi(location=\"Berlin\") #workflow.draw_aoi() Select the workflow parameters \u00b6 There are also multiple ways to construct the workflow input parameters: * Provide the parameters directly in code as a json string. * Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. * Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameters_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters Test & Run the workflow & download results \u00b6 # Run a test job to query data availability and check the configuration. # With this test query you will not be charged with any data or processing # credits, but have a preview of the job result. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual workflow. job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) Download & Display results \u00b6 # Download job result. results_fp = job . download_results () job . plot_results () job . map_results ()","title":"Detailed Example"},{"location":"detailed-example/#detailed-example","text":"This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps.","title":"Detailed Example"},{"location":"detailed-example/#authenticate-access-project","text":"import up42 up42 . authenticate ( project_id = \"12345\" , project_api_key = \"12345\" ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project Get information about the available blocks to later construct your workflow. up42 . get_blocks ( basic = True )","title":"Authenticate &amp; access project"},{"location":"detailed-example/#create-or-access-the-workflow","text":"You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) workflow # Add workflow tasks - simple version. See above .get_blocks() result. input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control # about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access an existing workflow within the project by its workflow_id UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow","title":"Create or access the workflow"},{"location":"detailed-example/#select-the-aoi","text":"There are multiple ways to select an aoi: Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. Use .draw_aoi() to draw the aoi and export it as a geojson. Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) #aoi = workflow.get_example_aoi(location=\"Berlin\") #workflow.draw_aoi()","title":"Select the aoi"},{"location":"detailed-example/#select-the-workflow-parameters","text":"There are also multiple ways to construct the workflow input parameters: * Provide the parameters directly in code as a json string. * Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. * Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameters_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters","title":"Select the workflow parameters"},{"location":"detailed-example/#test-run-the-workflow-download-results","text":"# Run a test job to query data availability and check the configuration. # With this test query you will not be charged with any data or processing # credits, but have a preview of the job result. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual workflow. job = workflow . run_job ( input_parameters = input_parameters , track_status = True )","title":"Test &amp; Run the workflow &amp; download results"},{"location":"detailed-example/#download-display-results","text":"# Download job result. results_fp = job . download_results () job . plot_results () job . map_results ()","title":"Download &amp; Display results"},{"location":"examples-intro/","text":"Examples \u00b6 This section provides more extensive use cases for the UP42 Python SDK. All examples are provided as Jupyter notebooks in the examples folder . Radar Processing Airport monitoring with parallel jobs Flood Mapping \ud83d\uddbc\ufe0f Image mosaicing","title":"Examples"},{"location":"examples-intro/#examples","text":"This section provides more extensive use cases for the UP42 Python SDK. All examples are provided as Jupyter notebooks in the examples folder . Radar Processing Airport monitoring with parallel jobs Flood Mapping \ud83d\uddbc\ufe0f Image mosaicing","title":"Examples"},{"location":"installation/","text":"Installation \u00b6 User installation \u00b6 Install via pip . The package requires Python version > 3.6. pip install up42-py Update an existing installation to the newest version via: pip install up42-py --upgrade Optional: Install Jupyter Lab The UP42 Python SDK is even more comfortable to use in a Jupyter notebook ! To install Jupyter Lab: pip install jupyterlab Test the installation \u00b6 To test the successful installation, import it in Python: import up42 Success! Continue with the Authentication chapter ! Development installation \u00b6 The development installation is only necessary if you want to contribute to up42-py or its documentation. Please see the developer readme for the full installation instructions and further information.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#user-installation","text":"Install via pip . The package requires Python version > 3.6. pip install up42-py Update an existing installation to the newest version via: pip install up42-py --upgrade Optional: Install Jupyter Lab The UP42 Python SDK is even more comfortable to use in a Jupyter notebook ! To install Jupyter Lab: pip install jupyterlab","title":"User installation"},{"location":"installation/#test-the-installation","text":"To test the successful installation, import it in Python: import up42 Success! Continue with the Authentication chapter !","title":"Test the installation"},{"location":"installation/#development-installation","text":"The development installation is only necessary if you want to contribute to up42-py or its documentation. Please see the developer readme for the full installation instructions and further information.","title":"Development installation"},{"location":"structure/","text":"Structure \u00b6 Hierachy \u00b6 The Python SDK uses six object classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask Catalog Tools Each object can spawn elements of one level below , e.g. project = up42.initialize_project() workflow = Project().create_workflow() job = workflow.run_job() Functionality \u00b6 An overview of the functionality of each object (also see the code reference ): Available Functionality up42 .initialize_project() .initalize_workflow() .initalize_job() .initalize_jobtask() .initalize_catalog() Project .get_workflows() .create_workflow() .get_jobs() .get_project_settings() .update_project_settings() Workflow .get_jobs() .test_job() .run_job() .get_workflow_tasks() `.get_compatible_blocks() .add_workflow_tasks() .get_parameters_info() .construct_parameters() .update_name() .delete() Job .download_results() .plot_results() .map_results() .get_status() .track_status() .cancel_job() .get_results_json() .get_logs() .download_quicklooks() .upload_results_to_bucket() .get_jobtasks() .get_jobtasks_results() JobTask .get_results_json() .download_results() .download_quicklooks() Catalog .construct_parameters() .search() .download_quicklooks() Tools .read_vector_file() .get_example_aoi() .draw_aoi() .plot_coverage() .plot_quicklooks() .plot_results() .get_blocks() .get_block_details() .validate_manifest() Object Initialization \u00b6 If a workflow etc. already exists on UP42, you can initialize and access it directly using its id : Initialize Object Project up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) project = up42 . initialize_project () Workflow UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) Job UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = up42 . initialize_job ( job_id = UP42_JOB_ID ) JobTask UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = up42 . initialize_jobtask ( jobtask_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) Catalog catalog = up42 . initialize_catalog () Tools The tools' functionalities can be accessed from any of the up42 objects, e.g. up42 . get_example_aoi () # workflow.get_example_aoi() # job.get_example_aoi()","title":"Structure"},{"location":"structure/#structure","text":"","title":"Structure"},{"location":"structure/#hierachy","text":"The Python SDK uses six object classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask Catalog Tools Each object can spawn elements of one level below , e.g. project = up42.initialize_project() workflow = Project().create_workflow() job = workflow.run_job()","title":"Hierachy"},{"location":"structure/#functionality","text":"An overview of the functionality of each object (also see the code reference ): Available Functionality up42 .initialize_project() .initalize_workflow() .initalize_job() .initalize_jobtask() .initalize_catalog() Project .get_workflows() .create_workflow() .get_jobs() .get_project_settings() .update_project_settings() Workflow .get_jobs() .test_job() .run_job() .get_workflow_tasks() `.get_compatible_blocks() .add_workflow_tasks() .get_parameters_info() .construct_parameters() .update_name() .delete() Job .download_results() .plot_results() .map_results() .get_status() .track_status() .cancel_job() .get_results_json() .get_logs() .download_quicklooks() .upload_results_to_bucket() .get_jobtasks() .get_jobtasks_results() JobTask .get_results_json() .download_results() .download_quicklooks() Catalog .construct_parameters() .search() .download_quicklooks() Tools .read_vector_file() .get_example_aoi() .draw_aoi() .plot_coverage() .plot_quicklooks() .plot_results() .get_blocks() .get_block_details() .validate_manifest()","title":"Functionality"},{"location":"structure/#object-initialization","text":"If a workflow etc. already exists on UP42, you can initialize and access it directly using its id : Initialize Object Project up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) project = up42 . initialize_project () Workflow UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) Job UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = up42 . initialize_job ( job_id = UP42_JOB_ID ) JobTask UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = up42 . initialize_jobtask ( jobtask_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) Catalog catalog = up42 . initialize_catalog () Tools The tools' functionalities can be accessed from any of the up42 objects, e.g. up42 . get_example_aoi () # workflow.get_example_aoi() # job.get_example_aoi()","title":"Object Initialization"},{"location":"support-faq/","text":"Support & FAQ \u00b6 Contact \u00b6 Please contact us via Email support@up42.com or open a github issue . Related links \u00b6 UP42 Website UP42 Python SDK Repository UP42 Github Repositories UP42 Documentation UP42 blockutils - Developer tools to easily create custom UP42 data & processing blocks UP42 mosaicing - Scripts to create image mosaics using UP42 FAQ \u00b6 Can I contribute to the SDK? \u00b6 Yes, contributions and bug fixes are very welcome. Please see the developer readme for further instructions.","title":"FAQ & Support"},{"location":"support-faq/#support-faq","text":"","title":"Support &amp; FAQ"},{"location":"support-faq/#contact","text":"Please contact us via Email support@up42.com or open a github issue .","title":"Contact"},{"location":"support-faq/#related-links","text":"UP42 Website UP42 Python SDK Repository UP42 Github Repositories UP42 Documentation UP42 blockutils - Developer tools to easily create custom UP42 data & processing blocks UP42 mosaicing - Scripts to create image mosaics using UP42","title":"Related links"},{"location":"support-faq/#faq","text":"","title":"FAQ"},{"location":"support-faq/#can-i-contribute-to-the-sdk","text":"Yes, contributions and bug fixes are very welcome. Please see the developer readme for further instructions.","title":"Can I contribute to the SDK?"},{"location":"examples/airports-parallel/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Parallel Jobs \u00b6 Example: Airport monitoring \u00b6 Get a Sentinel-2 clipped image for 10 airports in a country. Run all jobs in parallel Visualize the results In [2]: import up42 import pandas as pd import geopandas as gpd from pathlib import Path Random airports in Spain \u00b6 Airport locations scrapped from: https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat In [31]: country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uid airport country lat lon geometry 1180 1211 Albacete-Los Llanos Airport Spain 38.948502 -1.863520 POINT (-1.86352 38.94850) 1181 1212 Alicante International Airport Spain 38.282200 -0.558156 POINT (-0.55816 38.28220) 1182 1213 Almer\u00eda International Airport Spain 36.843899 -2.370100 POINT (-2.37010 36.84390) 1183 1214 Asturias Airport Spain 43.563599 -6.034620 POINT (-6.03462 43.56360) 1184 1215 C\u00f3rdoba Airport Spain 37.841999 -4.848880 POINT (-4.84888 37.84200) In [32]: airports = airports . sample ( 7 ) In [33]: # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) Out[33]: <matplotlib.axes._subplots.AxesSubplot at 0x128aa6d50> In [34]: # Buffer airport point locations by roughly 100m airports . geometry = airports . geometry . buffer ( 0.001 ) Prepare UP42 workflows \u00b6 Create a new project on UP42 or use an existing one. In [3]: # Authenticate with UP42 up42 . authenticate ( project_id = \"123456\" , project_api_key = \"123456\" ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () 2020-05-14 13:50:17,691 - up42.auth - INFO - Authentication with UP42 successful! 2020-05-14 13:50:17,692 - up42 - INFO - Working on Project with project_id d3ea9123-2b89-4975-be16-e978fed8329c In [4]: # Increase the parallel job limit for the project. # Only works when you have added your credit card information to the UP42 account. project . update_project_settings ( max_concurrent_jobs = 10 ) 2020-05-14 13:50:38,293 - up42.project - INFO - Updated project settings: [{'name': 'JOB_QUERY_MAX_AOI_SIZE', 'value': '100'}, {'name': 'MAX_CONCURRENT_JOBS', 'value': '10'}, {'name': 'JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE', 'value': '10'}] In [5]: workflow = project . create_workflow ( \"workflow_airports\" , use_existing = True ) 2020-05-14 13:50:39,219 - up42.project - INFO - Getting existing workflows in project ... 2020-05-14 13:50:39,673 - up42.project - INFO - Got 9 workflows for project d3ea9123-2b89-4975-be16-e978fed8329c. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:03<00:00, 2.34it/s] 2020-05-14 13:50:43,539 - up42.project - INFO - Using existing workflow: workflow_airports, ecda9673-1c39-4404-948d-e707af7780c9. In [6]: # Fill the workflow with tasks blocks = up42 . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ selected_block ]) workflow . get_workflow_tasks ( basic = True ) 2020-05-14 13:50:46,096 - up42.tools - INFO - Getting blocks name and id, use basic=False for all block details. 2020-05-14 13:50:47,349 - up42.workflow - INFO - Added tasks to workflow: [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}] 2020-05-14 13:50:47,880 - up42.workflow - INFO - Got 1 tasks/blocks in workflow ecda9673-1c39-4404-948d-e707af7780c9. Out[6]: {'sobloo-s2-l1c-aoiclipped:1': '156fa04b-7632-4e1b-b7eb-d6c77b46a284'} Run jobs in parallel \u00b6 Queries & downloads one image per airport in parallel. Crude, this will soon be available in the API in one simple command! In [39]: # Run jobs in parallel up42 . settings ( log = False ) jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameters ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) In [49]: # Download results: out_filepaths = [] for job in jobs : fp = job . download_results () out_filepaths . append ( fp [ 0 ]) 121it [00:00, 72939.18it/s] 135it [00:00, 75922.64it/s] 111it [00:00, 60104.28it/s] 202it [00:00, 98106.69it/s] 168it [00:00, 130441.15it/s] 183it [00:00, 91441.22it/s] 89it [00:00, 59765.14it/s] In [48]: # Visualize downloaded results up42 . plot_results ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ()) In [ ]:","title":"Airport monitoring"},{"location":"examples/airports-parallel/#parallel-jobs","text":"","title":"Parallel Jobs"},{"location":"examples/airports-parallel/#example-airport-monitoring","text":"Get a Sentinel-2 clipped image for 10 airports in a country. Run all jobs in parallel Visualize the results In [2]: import up42 import pandas as pd import geopandas as gpd from pathlib import Path","title":"Example: Airport monitoring"},{"location":"examples/airports-parallel/#random-airports-in-spain","text":"Airport locations scrapped from: https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat In [31]: country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } uid airport country lat lon geometry 1180 1211 Albacete-Los Llanos Airport Spain 38.948502 -1.863520 POINT (-1.86352 38.94850) 1181 1212 Alicante International Airport Spain 38.282200 -0.558156 POINT (-0.55816 38.28220) 1182 1213 Almer\u00eda International Airport Spain 36.843899 -2.370100 POINT (-2.37010 36.84390) 1183 1214 Asturias Airport Spain 43.563599 -6.034620 POINT (-6.03462 43.56360) 1184 1215 C\u00f3rdoba Airport Spain 37.841999 -4.848880 POINT (-4.84888 37.84200) In [32]: airports = airports . sample ( 7 ) In [33]: # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) Out[33]: <matplotlib.axes._subplots.AxesSubplot at 0x128aa6d50> In [34]: # Buffer airport point locations by roughly 100m airports . geometry = airports . geometry . buffer ( 0.001 )","title":"Random airports in Spain"},{"location":"examples/airports-parallel/#prepare-up42-workflows","text":"Create a new project on UP42 or use an existing one. In [3]: # Authenticate with UP42 up42 . authenticate ( project_id = \"123456\" , project_api_key = \"123456\" ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () 2020-05-14 13:50:17,691 - up42.auth - INFO - Authentication with UP42 successful! 2020-05-14 13:50:17,692 - up42 - INFO - Working on Project with project_id d3ea9123-2b89-4975-be16-e978fed8329c In [4]: # Increase the parallel job limit for the project. # Only works when you have added your credit card information to the UP42 account. project . update_project_settings ( max_concurrent_jobs = 10 ) 2020-05-14 13:50:38,293 - up42.project - INFO - Updated project settings: [{'name': 'JOB_QUERY_MAX_AOI_SIZE', 'value': '100'}, {'name': 'MAX_CONCURRENT_JOBS', 'value': '10'}, {'name': 'JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE', 'value': '10'}] In [5]: workflow = project . create_workflow ( \"workflow_airports\" , use_existing = True ) 2020-05-14 13:50:39,219 - up42.project - INFO - Getting existing workflows in project ... 2020-05-14 13:50:39,673 - up42.project - INFO - Got 9 workflows for project d3ea9123-2b89-4975-be16-e978fed8329c. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:03<00:00, 2.34it/s] 2020-05-14 13:50:43,539 - up42.project - INFO - Using existing workflow: workflow_airports, ecda9673-1c39-4404-948d-e707af7780c9. In [6]: # Fill the workflow with tasks blocks = up42 . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ selected_block ]) workflow . get_workflow_tasks ( basic = True ) 2020-05-14 13:50:46,096 - up42.tools - INFO - Getting blocks name and id, use basic=False for all block details. 2020-05-14 13:50:47,349 - up42.workflow - INFO - Added tasks to workflow: [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}] 2020-05-14 13:50:47,880 - up42.workflow - INFO - Got 1 tasks/blocks in workflow ecda9673-1c39-4404-948d-e707af7780c9. Out[6]: {'sobloo-s2-l1c-aoiclipped:1': '156fa04b-7632-4e1b-b7eb-d6c77b46a284'}","title":"Prepare UP42 workflows"},{"location":"examples/airports-parallel/#run-jobs-in-parallel","text":"Queries & downloads one image per airport in parallel. Crude, this will soon be available in the API in one simple command! In [39]: # Run jobs in parallel up42 . settings ( log = False ) jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameters ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) In [49]: # Download results: out_filepaths = [] for job in jobs : fp = job . download_results () out_filepaths . append ( fp [ 0 ]) 121it [00:00, 72939.18it/s] 135it [00:00, 75922.64it/s] 111it [00:00, 60104.28it/s] 202it [00:00, 98106.69it/s] 168it [00:00, 130441.15it/s] 183it [00:00, 91441.22it/s] 89it [00:00, 59765.14it/s] In [48]: # Visualize downloaded results up42 . plot_results ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ()) In [ ]:","title":"Run jobs in parallel"},{"location":"examples/flood_mapping/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Flood Impact Mapping with UP42 \u00b6 In the following tutorial we will map the impact of the flooding in urban area using UP42 python SDK and OpenStreetMap data. This notebook is intended to show how your existing GIS analysis and workflows can seemlessly be integrated with UP42 Python SDK in a few lines of code. The notebook is divided in following sections: Download Sentinel-2 AOI clipped GeoTiff with UP42 data-block Calculate Modified Normalized Water Index (MNDWI) Convert MNDWI raster mask to vector mask Extract building footprints polygons from OSM (with awesome osmnx) library Plot the impacted buildings with Folium Additional Package Requirement(s) $ pip install osmnx In [ ]: # !pip install -U osmnx In [2]: # imports import os from functools import partial import geopandas as gpd import matplotlib.pyplot as plt import numpy as np import pandas as pd import pyproj import rasterio as rio from rasterio import features from rasterio.plot import reshape_as_raster , show from shapely.geometry import LineString , MultiPolygon , Point , Polygon , box from shapely.geometry import shape as shapely_shp from shapely.ops import cascaded_union , transform import folium import osmnx as ox import up42 In [3]: # allows for ignoring errors + division by zero np . seterr ( divide = 'ignore' , invalid = 'ignore' ) Out[3]: {'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'} Set-up data directory to work with. This is optional though. The SDK creates a directory in the project folder for you! In [4]: data_dir = './tmp' Download data from UP42 platform \u00b6 In [5]: # authenticate up42 . authenticate ( cfg_file = \"flood_config.json\" ) 2020-05-13 16:08:25,244 - up42.auth - INFO - Got credentials from config file. 2020-05-13 16:08:25,706 - up42.auth - INFO - Authentication with UP42 successful! In [6]: project = up42 . initialize_project () # init workflow workflow = project . create_workflow ( name = \"flooding\" , use_existing = True ) 2020-05-13 16:08:25,711 - up42 - INFO - Working on Project with project_id 28f674f9-0886-4fab-8774-c9ee4d54eea1 2020-05-13 16:08:26,150 - up42.project - INFO - Getting existing workflows in project ... 2020-05-13 16:08:26,607 - up42.project - INFO - Got 1 workflows for project 28f674f9-0886-4fab-8774-c9ee4d54eea1. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.12it/s] 2020-05-13 16:08:27,081 - up42.project - INFO - Using existing workflow: flooding, 89d31c7e-8f46-4c4a-9d26-ef90496250a4. In [7]: def get_data ( wf_name , inp_tasks , aoi_gjson , geom_op , start , end , data_dir = data_dir , limit = 1 ): # init project project = up42 . initialize_project () # init workflow workflow = project . create_workflow ( name = wf_name , use_existing = True ) input_tasks = inp_tasks workflow . add_workflow_tasks ( input_tasks = input_tasks ) aoi = workflow . read_vector_file ( aoi_gjson , as_dataframe = True ) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = geom_op , start_date = start , end_date = end , limit = limit ) # Run the actual job job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) job . download_results ( data_dir ) job . download_quicklooks ( data_dir ) return job Following will fetch the data from UP42 platform corresponding the AOI and parameters we passed into the function. The area of interest is based on 2019 flooding events in Mid-West, USA in 2019. Bellevue In [8]: job = get_data ( wf_name = \"flooding\" , inp_tasks = [ 'sentinelhub-s2-aoiclipped' ], aoi_gjson = \"./data/aoi_bellevue_US.geojson\" , geom_op = \"contains\" , start = \"2019-03-21\" , end = \"2019-03-21\" ) 2020-05-13 16:08:27,095 - up42 - INFO - Working on Project with project_id 28f674f9-0886-4fab-8774-c9ee4d54eea1 2020-05-13 16:08:27,543 - up42.project - INFO - Getting existing workflows in project ... 2020-05-13 16:08:27,994 - up42.project - INFO - Got 1 workflows for project 28f674f9-0886-4fab-8774-c9ee4d54eea1. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.09it/s] 2020-05-13 16:08:28,475 - up42.project - INFO - Using existing workflow: flooding, 89d31c7e-8f46-4c4a-9d26-ef90496250a4. 2020-05-13 16:08:29,911 - up42.workflow - INFO - Added tasks to workflow: [{'name': 'sentinelhub-s2-aoiclipped:1', 'parentName': None, 'blockId': '018dfb34-fc19-4334-8125-14fd7535f979'}] 2020-05-13 16:08:30,474 - up42.workflow - INFO - Selected input_parameters: {'sentinelhub-s2-aoiclipped:1': {'time': '2019-03-21T00:00:00Z/2019-03-21T00:00:00Z', 'limit': 1, 'zoom_level': 14, 'contains': {'type': 'Polygon', 'coordinates': (((-95.929012, 41.114021), (-95.928326, 41.124884), (-95.922832, 41.132642), (-95.906868, 41.123978), (-95.900517, 41.116995), (-95.88747, 41.119452), (-95.878887, 41.119064), (-95.875969, 41.110787), (-95.880947, 41.106519), (-95.901203, 41.102121), (-95.916824, 41.106649), (-95.929012, 41.114021)),)}}}. 2020-05-13 16:08:31,829 - up42.workflow - INFO - Created and running new job: 50ca4d17-7b86-420c-8f0c-d86d700b36c9. 2020-05-13 16:08:32,280 - up42.job - INFO - Tracking job status continuously, reporting every 30 seconds... 2020-05-13 16:08:55,103 - up42.job - INFO - Job finished successfully! - 50ca4d17-7b86-420c-8f0c-d86d700b36c9 2020-05-13 16:09:00,109 - up42.job - INFO - Downloading results of job 50ca4d17-7b86-420c-8f0c-d86d700b36c9 2020-05-13 16:09:00,111 - up42.job - INFO - Download directory: tmp 4244it [00:00, 352285.34it/s] 2020-05-13 16:09:02,526 - up42.utils - INFO - Download successful of 2 files to output_directory 'tmp': ['496a6bc2-2af2-4e5b-b11d-d14c336e6855.tif', 'data.json'] 2020-05-13 16:09:03,875 - up42.jobtask - INFO - Download directory: tmp 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1.70it/s] You should see a new folder named tmp being created in the current working directory with .tif and meatadata file data.json . Additionally, we downloaded quicklooks .jpg with job.download_quicklooks() . This will come handy later while visualising on a folium map. NOTE: The SDK automatically creates the download directory if not already exists. In [9]: # store results and quicklook paths to separate variables raster_path = [ i for i in job . results if i . endswith ( '.tif' )][ 0 ] metadata_path = [ i for i in job . results if i . endswith ( '.json' )][ 0 ] In [10]: # similarly store path for quicklook ql_path = job . quicklooks [ 0 ] Now, that we have downloaded the necessary data let's move on to the next steps! In [11]: raster_path , metadata_path Out[11]: ('tmp/496a6bc2-2af2-4e5b-b11d-d14c336e6855.tif', 'tmp/data.json') Calculate MNDWI \u00b6 The Modified Normalized Difference Water Index (MNDWI) uses green and SWIR bands for the enhancement of open water features. It also diminishes built-up area features that are often correlated with open water in other indices. MNDWI = (Green - SWIR) / (Green + SWIR) Green = pixel values from the green band SWIR = pixel values from the short-wave infrared band Reference : http://space4water.org/taxonomy/term/1246 In [12]: # read data with rasterio dataset = rio . open ( raster_path ) In [13]: # read bands # B03 -> green # B11 -> SWIR green = dataset . read ( 3 ) swir = dataset . read ( 11 ) In [14]: def normalize ( array ): \"\"\"Normalizes numpy arrays into scale 0.0 - 1.0\"\"\" array_min , array_max = array . min (), array . max () return (( array - array_min ) / ( array_max - array_min )) In [15]: def calc_mndwi ( green_band , swir_band ): mndwi = ( green_band - swir_band ) / ( green_band + swir_band ) return mndwi . astype ( np . float32 ) In [16]: # normalize green_n = normalize ( green ) swir_n = normalize ( swir ) In [17]: mndwi = calc_mndwi ( green_band = green_n , swir_band = swir_n ) Let's quickly plot and see how it looks like In [18]: plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( mndwi , cmap = 'terrain_r' ) # Add colorbar to show the index plt . colorbar (); As we can see the water area is depicted in blue where as land area is brown(-ish) color In [19]: # close the dataset dataset . close () Create Vector Mask \u00b6 Now that we have computed MNDWI, next step is to isolate boundaries of flooded area from whole image. This means, Apply a deterministic decision boundary / threshold to NDWI values Translate the threshold values into boolean value mask (1/0) Convert raster into vector boundary (multipolygon) According the NDWI-wikipedia : For the second variant of the NDWI, another threshold can also be found in that avoids creating false alarms in urban areas: < 0.3 - Non-water >= 0.3 - Water Meaning all the values that are >= 0.3 will be mapped to value true and < 0.3 to false respectively. In [20]: # create numpy mask mndwi_msk = ( mndwi >= 0.3 ) Pixel to shapely geom \u00b6 In [21]: mypoly = [] for vec , val in features . shapes ( source = mndwi_msk . astype ( np . float32 ), transform = dataset . transform ): mypoly . append ({ 'geom' : shapely_shp ( vec ), 'value' : val }) Now, let's convert this to GeoDataFrame as it will be easy to do vector based operations as well as to create viz. In [22]: # to gdf submerged = gpd . GeoDataFrame ( mypoly , crs = dataset . crs , geometry = 'geom' ) In [23]: # value counts submerged [ 'value' ] . value_counts () Out[23]: 1.0 203 0.0 138 Name: value, dtype: int64 We see that submerged have two unique values (0.0, 1.0). However, 1.0 corresponds to water so it makes sense to filter out land-area from gdf. In [24]: # filter by water area submerged = submerged [ submerged [ 'value' ] > 0 ] In [25]: # shape should be 203 based on value count cell above submerged . shape [ 0 ] Out[25]: 203 In [26]: # quick plot submerged . plot ( figsize = ( 15 , 10 )); Now that we have vectorized boundaries of flooded area which we will use to intersect with building footprint polygons from OSM. It is better to collapse all individual polygons into a single Multipolygon geometry. Also, note the x and y axis values!! ( HINT: crs) In [27]: boundary = cascaded_union ( list ( submerged [ 'geom' ])) Retrieve Building Footprints from OSM \u00b6 For this we will use, OSMNx library by Geoff Boeing. This library comes with readily available function to extract building footprints given bounding box. Caveat(s): OSMNx expects input in WGS84 ( EPSG:4326 ) whereas out data is in psuedo-mercator projection ( EPSG:3857 ) We need some coordinate system transformation here, for both, OSM data retrieval as well as plotting In [28]: # transform crs crs_project = partial ( pyproj . transform , pyproj . Proj ( init = 'epsg:3857' ), # source coordinate system pyproj . Proj ( init = 'epsg:4326' ) # destination coordinate system ) In [29]: # read bounds from dataset and transform to wgs84 because osmnx expects epsg:4326 :/ osm_poly = transform ( crs_project , box ( * dataset . bounds )) Now, we have everything we need to extract data from OSM In [30]: gdf = ox . footprints_from_polygon ( polygon = osm_poly ) In [31]: gdf . plot ( figsize = ( 15 , 10 )) Out[31]: <matplotlib.axes._subplots.AxesSubplot at 0x7fb6f86b3cd0> Notice that the coordinates, here, are in (lat, lon). Now, that data is extracted, we can transform it to the crs of the dataset. Doing this right now prevents any inconsistencies (with plotting, intersecting etc.) at later point as well as ensures sanity!! In [32]: gdf_proj = ox . project_gdf ( gdf , to_crs = dataset . crs ) In [33]: gdf_proj . head ( 1 ) Out[33]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nodes building layer geometry ref ref:source source name access amenity ... ele gnis:county_id gnis:created gnis:feature_id gnis:state_id barrier smoking lit members type 32811010 [369730571, 2007636330, 2007636643, 2007635719... hangar 1 POLYGON ((-10675793.142 5028506.241, -10675786... NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows \u00d7 74 columns Extract Flood Impacted buildings \u00b6 Until this point, we set the stage for actual task. Basically, the building footprints that intersects the Multipolygon (one we created from MNDWI) are effected building and those that are not are not-effected buildings! In [34]: effected = gdf_proj [ gdf_proj [ 'geometry' ] . intersects ( boundary )] # change coordinate system for plotting effected . to_crs ( crs = 'EPSG:4326' , inplace = True ) /Users/prayag.thakkar/Envs/up42-py/lib/python3.7/site-packages/geopandas/geodataframe.py:183: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy frame[geo_column_name] = level In [35]: not_effected = gdf_proj [ ~ gdf_proj [ 'geometry' ] . intersects ( boundary )] # change coordinate system for plotting not_effected . to_crs ( crs = 'EPSG:4326' , inplace = True ) /Users/prayag.thakkar/Envs/up42-py/lib/python3.7/site-packages/geopandas/geodataframe.py:183: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy frame[geo_column_name] = level In [36]: # quick sanity check gdf_proj . shape [ 0 ] == ( effected . shape [ 0 ] + not_effected . shape [ 0 ]) Out[36]: True It's time for plotting. We will use Folium library for displaying our results. Of course, it requires a bit of massaging before we get to the plotting Plotting \u00b6 In [37]: # bbox centroid serves as the center point for the folium map bbox_centroid = list (( osm_poly . centroid ) . coords [:][ 0 ]) bbox_centroid = [ bbox_centroid [ - 1 ], bbox_centroid [ 0 ]] bbox_centroid Out[37]: [41.12074089370247, -95.899658203125] In [38]: # extracts bounds for image overlay lon_min , lat_min , lon_max , lat_max = osm_poly . bounds In [39]: style1 = { 'fillColor' : '#228B22' , 'color' : 'red' } style2 = { 'fillColor' : '#00FFFFFF' , 'color' : '#00FFFFFF' } In [40]: # init folium map m = folium . Map ( bbox_centroid , zoom_start = 15 ) # add effected buildings folium . GeoJson ( effected . to_json (), style_function = lambda x : style1 ) . add_to ( m ); # add not_effected buildings folium . GeoJson ( not_effected . to_json (), style_function = lambda x : style2 ) . add_to ( m ); # add raster png quicklook folium . raster_layers . ImageOverlay ( image = ql_path , bounds = [[ lat_min , lon_min ], [ lat_max , lon_max ]], opacity = 0.8 ) . add_to ( m ); In [41]: folium . LayerControl () . add_to ( m ) m Out[41]: Make this Notebook Trusted to load map: File -> Trust Notebook As we can see the buildings impacted by flooding in red and those that are not impacted are in blue. It should be noted that success of the analysis depends on the availability of the data in OSM!!","title":"Flood mapping"},{"location":"examples/flood_mapping/#flood-impact-mapping-with-up42","text":"In the following tutorial we will map the impact of the flooding in urban area using UP42 python SDK and OpenStreetMap data. This notebook is intended to show how your existing GIS analysis and workflows can seemlessly be integrated with UP42 Python SDK in a few lines of code. The notebook is divided in following sections: Download Sentinel-2 AOI clipped GeoTiff with UP42 data-block Calculate Modified Normalized Water Index (MNDWI) Convert MNDWI raster mask to vector mask Extract building footprints polygons from OSM (with awesome osmnx) library Plot the impacted buildings with Folium Additional Package Requirement(s) $ pip install osmnx In [ ]: # !pip install -U osmnx In [2]: # imports import os from functools import partial import geopandas as gpd import matplotlib.pyplot as plt import numpy as np import pandas as pd import pyproj import rasterio as rio from rasterio import features from rasterio.plot import reshape_as_raster , show from shapely.geometry import LineString , MultiPolygon , Point , Polygon , box from shapely.geometry import shape as shapely_shp from shapely.ops import cascaded_union , transform import folium import osmnx as ox import up42 In [3]: # allows for ignoring errors + division by zero np . seterr ( divide = 'ignore' , invalid = 'ignore' ) Out[3]: {'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'} Set-up data directory to work with. This is optional though. The SDK creates a directory in the project folder for you! In [4]: data_dir = './tmp'","title":"Flood Impact Mapping with UP42"},{"location":"examples/flood_mapping/#download-data-from-up42-platform","text":"In [5]: # authenticate up42 . authenticate ( cfg_file = \"flood_config.json\" ) 2020-05-13 16:08:25,244 - up42.auth - INFO - Got credentials from config file. 2020-05-13 16:08:25,706 - up42.auth - INFO - Authentication with UP42 successful! In [6]: project = up42 . initialize_project () # init workflow workflow = project . create_workflow ( name = \"flooding\" , use_existing = True ) 2020-05-13 16:08:25,711 - up42 - INFO - Working on Project with project_id 28f674f9-0886-4fab-8774-c9ee4d54eea1 2020-05-13 16:08:26,150 - up42.project - INFO - Getting existing workflows in project ... 2020-05-13 16:08:26,607 - up42.project - INFO - Got 1 workflows for project 28f674f9-0886-4fab-8774-c9ee4d54eea1. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.12it/s] 2020-05-13 16:08:27,081 - up42.project - INFO - Using existing workflow: flooding, 89d31c7e-8f46-4c4a-9d26-ef90496250a4. In [7]: def get_data ( wf_name , inp_tasks , aoi_gjson , geom_op , start , end , data_dir = data_dir , limit = 1 ): # init project project = up42 . initialize_project () # init workflow workflow = project . create_workflow ( name = wf_name , use_existing = True ) input_tasks = inp_tasks workflow . add_workflow_tasks ( input_tasks = input_tasks ) aoi = workflow . read_vector_file ( aoi_gjson , as_dataframe = True ) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = geom_op , start_date = start , end_date = end , limit = limit ) # Run the actual job job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) job . download_results ( data_dir ) job . download_quicklooks ( data_dir ) return job Following will fetch the data from UP42 platform corresponding the AOI and parameters we passed into the function. The area of interest is based on 2019 flooding events in Mid-West, USA in 2019. Bellevue In [8]: job = get_data ( wf_name = \"flooding\" , inp_tasks = [ 'sentinelhub-s2-aoiclipped' ], aoi_gjson = \"./data/aoi_bellevue_US.geojson\" , geom_op = \"contains\" , start = \"2019-03-21\" , end = \"2019-03-21\" ) 2020-05-13 16:08:27,095 - up42 - INFO - Working on Project with project_id 28f674f9-0886-4fab-8774-c9ee4d54eea1 2020-05-13 16:08:27,543 - up42.project - INFO - Getting existing workflows in project ... 2020-05-13 16:08:27,994 - up42.project - INFO - Got 1 workflows for project 28f674f9-0886-4fab-8774-c9ee4d54eea1. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2.09it/s] 2020-05-13 16:08:28,475 - up42.project - INFO - Using existing workflow: flooding, 89d31c7e-8f46-4c4a-9d26-ef90496250a4. 2020-05-13 16:08:29,911 - up42.workflow - INFO - Added tasks to workflow: [{'name': 'sentinelhub-s2-aoiclipped:1', 'parentName': None, 'blockId': '018dfb34-fc19-4334-8125-14fd7535f979'}] 2020-05-13 16:08:30,474 - up42.workflow - INFO - Selected input_parameters: {'sentinelhub-s2-aoiclipped:1': {'time': '2019-03-21T00:00:00Z/2019-03-21T00:00:00Z', 'limit': 1, 'zoom_level': 14, 'contains': {'type': 'Polygon', 'coordinates': (((-95.929012, 41.114021), (-95.928326, 41.124884), (-95.922832, 41.132642), (-95.906868, 41.123978), (-95.900517, 41.116995), (-95.88747, 41.119452), (-95.878887, 41.119064), (-95.875969, 41.110787), (-95.880947, 41.106519), (-95.901203, 41.102121), (-95.916824, 41.106649), (-95.929012, 41.114021)),)}}}. 2020-05-13 16:08:31,829 - up42.workflow - INFO - Created and running new job: 50ca4d17-7b86-420c-8f0c-d86d700b36c9. 2020-05-13 16:08:32,280 - up42.job - INFO - Tracking job status continuously, reporting every 30 seconds... 2020-05-13 16:08:55,103 - up42.job - INFO - Job finished successfully! - 50ca4d17-7b86-420c-8f0c-d86d700b36c9 2020-05-13 16:09:00,109 - up42.job - INFO - Downloading results of job 50ca4d17-7b86-420c-8f0c-d86d700b36c9 2020-05-13 16:09:00,111 - up42.job - INFO - Download directory: tmp 4244it [00:00, 352285.34it/s] 2020-05-13 16:09:02,526 - up42.utils - INFO - Download successful of 2 files to output_directory 'tmp': ['496a6bc2-2af2-4e5b-b11d-d14c336e6855.tif', 'data.json'] 2020-05-13 16:09:03,875 - up42.jobtask - INFO - Download directory: tmp 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1.70it/s] You should see a new folder named tmp being created in the current working directory with .tif and meatadata file data.json . Additionally, we downloaded quicklooks .jpg with job.download_quicklooks() . This will come handy later while visualising on a folium map. NOTE: The SDK automatically creates the download directory if not already exists. In [9]: # store results and quicklook paths to separate variables raster_path = [ i for i in job . results if i . endswith ( '.tif' )][ 0 ] metadata_path = [ i for i in job . results if i . endswith ( '.json' )][ 0 ] In [10]: # similarly store path for quicklook ql_path = job . quicklooks [ 0 ] Now, that we have downloaded the necessary data let's move on to the next steps! In [11]: raster_path , metadata_path Out[11]: ('tmp/496a6bc2-2af2-4e5b-b11d-d14c336e6855.tif', 'tmp/data.json')","title":"Download data from UP42 platform"},{"location":"examples/flood_mapping/#calculate-mndwi","text":"The Modified Normalized Difference Water Index (MNDWI) uses green and SWIR bands for the enhancement of open water features. It also diminishes built-up area features that are often correlated with open water in other indices. MNDWI = (Green - SWIR) / (Green + SWIR) Green = pixel values from the green band SWIR = pixel values from the short-wave infrared band Reference : http://space4water.org/taxonomy/term/1246 In [12]: # read data with rasterio dataset = rio . open ( raster_path ) In [13]: # read bands # B03 -> green # B11 -> SWIR green = dataset . read ( 3 ) swir = dataset . read ( 11 ) In [14]: def normalize ( array ): \"\"\"Normalizes numpy arrays into scale 0.0 - 1.0\"\"\" array_min , array_max = array . min (), array . max () return (( array - array_min ) / ( array_max - array_min )) In [15]: def calc_mndwi ( green_band , swir_band ): mndwi = ( green_band - swir_band ) / ( green_band + swir_band ) return mndwi . astype ( np . float32 ) In [16]: # normalize green_n = normalize ( green ) swir_n = normalize ( swir ) In [17]: mndwi = calc_mndwi ( green_band = green_n , swir_band = swir_n ) Let's quickly plot and see how it looks like In [18]: plt . figure ( figsize = ( 10 , 7 )) plt . imshow ( mndwi , cmap = 'terrain_r' ) # Add colorbar to show the index plt . colorbar (); As we can see the water area is depicted in blue where as land area is brown(-ish) color In [19]: # close the dataset dataset . close ()","title":"Calculate MNDWI"},{"location":"examples/flood_mapping/#create-vector-mask","text":"Now that we have computed MNDWI, next step is to isolate boundaries of flooded area from whole image. This means, Apply a deterministic decision boundary / threshold to NDWI values Translate the threshold values into boolean value mask (1/0) Convert raster into vector boundary (multipolygon) According the NDWI-wikipedia : For the second variant of the NDWI, another threshold can also be found in that avoids creating false alarms in urban areas: < 0.3 - Non-water >= 0.3 - Water Meaning all the values that are >= 0.3 will be mapped to value true and < 0.3 to false respectively. In [20]: # create numpy mask mndwi_msk = ( mndwi >= 0.3 )","title":"Create Vector Mask"},{"location":"examples/flood_mapping/#pixel-to-shapely-geom","text":"In [21]: mypoly = [] for vec , val in features . shapes ( source = mndwi_msk . astype ( np . float32 ), transform = dataset . transform ): mypoly . append ({ 'geom' : shapely_shp ( vec ), 'value' : val }) Now, let's convert this to GeoDataFrame as it will be easy to do vector based operations as well as to create viz. In [22]: # to gdf submerged = gpd . GeoDataFrame ( mypoly , crs = dataset . crs , geometry = 'geom' ) In [23]: # value counts submerged [ 'value' ] . value_counts () Out[23]: 1.0 203 0.0 138 Name: value, dtype: int64 We see that submerged have two unique values (0.0, 1.0). However, 1.0 corresponds to water so it makes sense to filter out land-area from gdf. In [24]: # filter by water area submerged = submerged [ submerged [ 'value' ] > 0 ] In [25]: # shape should be 203 based on value count cell above submerged . shape [ 0 ] Out[25]: 203 In [26]: # quick plot submerged . plot ( figsize = ( 15 , 10 )); Now that we have vectorized boundaries of flooded area which we will use to intersect with building footprint polygons from OSM. It is better to collapse all individual polygons into a single Multipolygon geometry. Also, note the x and y axis values!! ( HINT: crs) In [27]: boundary = cascaded_union ( list ( submerged [ 'geom' ]))","title":"Pixel to shapely geom"},{"location":"examples/flood_mapping/#retrieve-building-footprints-from-osm","text":"For this we will use, OSMNx library by Geoff Boeing. This library comes with readily available function to extract building footprints given bounding box. Caveat(s): OSMNx expects input in WGS84 ( EPSG:4326 ) whereas out data is in psuedo-mercator projection ( EPSG:3857 ) We need some coordinate system transformation here, for both, OSM data retrieval as well as plotting In [28]: # transform crs crs_project = partial ( pyproj . transform , pyproj . Proj ( init = 'epsg:3857' ), # source coordinate system pyproj . Proj ( init = 'epsg:4326' ) # destination coordinate system ) In [29]: # read bounds from dataset and transform to wgs84 because osmnx expects epsg:4326 :/ osm_poly = transform ( crs_project , box ( * dataset . bounds )) Now, we have everything we need to extract data from OSM In [30]: gdf = ox . footprints_from_polygon ( polygon = osm_poly ) In [31]: gdf . plot ( figsize = ( 15 , 10 )) Out[31]: <matplotlib.axes._subplots.AxesSubplot at 0x7fb6f86b3cd0> Notice that the coordinates, here, are in (lat, lon). Now, that data is extracted, we can transform it to the crs of the dataset. Doing this right now prevents any inconsistencies (with plotting, intersecting etc.) at later point as well as ensures sanity!! In [32]: gdf_proj = ox . project_gdf ( gdf , to_crs = dataset . crs ) In [33]: gdf_proj . head ( 1 ) Out[33]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } nodes building layer geometry ref ref:source source name access amenity ... ele gnis:county_id gnis:created gnis:feature_id gnis:state_id barrier smoking lit members type 32811010 [369730571, 2007636330, 2007636643, 2007635719... hangar 1 POLYGON ((-10675793.142 5028506.241, -10675786... NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 rows \u00d7 74 columns","title":"Retrieve Building Footprints from OSM"},{"location":"examples/flood_mapping/#extract-flood-impacted-buildings","text":"Until this point, we set the stage for actual task. Basically, the building footprints that intersects the Multipolygon (one we created from MNDWI) are effected building and those that are not are not-effected buildings! In [34]: effected = gdf_proj [ gdf_proj [ 'geometry' ] . intersects ( boundary )] # change coordinate system for plotting effected . to_crs ( crs = 'EPSG:4326' , inplace = True ) /Users/prayag.thakkar/Envs/up42-py/lib/python3.7/site-packages/geopandas/geodataframe.py:183: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy frame[geo_column_name] = level In [35]: not_effected = gdf_proj [ ~ gdf_proj [ 'geometry' ] . intersects ( boundary )] # change coordinate system for plotting not_effected . to_crs ( crs = 'EPSG:4326' , inplace = True ) /Users/prayag.thakkar/Envs/up42-py/lib/python3.7/site-packages/geopandas/geodataframe.py:183: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy frame[geo_column_name] = level In [36]: # quick sanity check gdf_proj . shape [ 0 ] == ( effected . shape [ 0 ] + not_effected . shape [ 0 ]) Out[36]: True It's time for plotting. We will use Folium library for displaying our results. Of course, it requires a bit of massaging before we get to the plotting","title":"Extract Flood Impacted buildings"},{"location":"examples/flood_mapping/#plotting","text":"In [37]: # bbox centroid serves as the center point for the folium map bbox_centroid = list (( osm_poly . centroid ) . coords [:][ 0 ]) bbox_centroid = [ bbox_centroid [ - 1 ], bbox_centroid [ 0 ]] bbox_centroid Out[37]: [41.12074089370247, -95.899658203125] In [38]: # extracts bounds for image overlay lon_min , lat_min , lon_max , lat_max = osm_poly . bounds In [39]: style1 = { 'fillColor' : '#228B22' , 'color' : 'red' } style2 = { 'fillColor' : '#00FFFFFF' , 'color' : '#00FFFFFF' } In [40]: # init folium map m = folium . Map ( bbox_centroid , zoom_start = 15 ) # add effected buildings folium . GeoJson ( effected . to_json (), style_function = lambda x : style1 ) . add_to ( m ); # add not_effected buildings folium . GeoJson ( not_effected . to_json (), style_function = lambda x : style2 ) . add_to ( m ); # add raster png quicklook folium . raster_layers . ImageOverlay ( image = ql_path , bounds = [[ lat_min , lon_min ], [ lat_max , lon_max ]], opacity = 0.8 ) . add_to ( m ); In [41]: folium . LayerControl () . add_to ( m ) m Out[41]: Make this Notebook Trusted to load map: File -> Trust Notebook As we can see the buildings impacted by flooding in red and those that are not impacted are in blue. It should be noted that success of the analysis depends on the availability of the data in OSM!!","title":"Plotting"},{"location":"examples/radar_processing_1/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Example: Radar processing \u00b6 The processing of radar data - often referred to as SAR data, which stands for Synthetice Aperture Radar - is a topic of its own and quite different from optical data that uses Red, Green, Blue, NIR and similar bands. This is the first of a series of jupyter notebooks where example SAR workflows will be explained that can be run on UP42. All of the examples are based on a specific type of analysis which is called polarimetry and processes one image at a time. In comparison to polarimetric there also is interferometric analysis which leverages the information encoded in suitable pairs of radar image. In this notebook we want to have a look at the Sentinel-1 GRD full scene block, followed by SNAP Sentinel-1 Polarimetric Processing. Polarimetric SAR Processing turns a Sentinel-1 GRD image into a multichannel GeoTIFF that is ready for analysis. Prepare workflow Define aoi and input parameters Run Job Visualize results In [1]: import up42 In [3]: # Authenticate with UP42 up42 . authenticate ( cfg_file = \"config.json\" ) #up42.authenticate(project_id=12345, project_api_key=12345) 2020-05-26 14:19:56,546 - up42.auth - INFO - Got credentials from config file. 2020-05-26 14:19:57,155 - up42.auth - INFO - Authentication with UP42 successful! Prepare UP42 workflows \u00b6 Create a new project on UP42 or use an existing one. In [5]: S1_SNAP_project = up42 . initialize_project () 2020-05-26 14:20:10,960 - up42 - INFO - Working on Project with project_id d3ea9123-2b89-4975-be16-e978fed8329c Create workflow and check available blocks and data In [6]: workflow = S1_SNAP_project . create_workflow ( name = \"S1-GRD_SNAP\" , use_existing = True ) print ( up42 . get_blocks ( basic = True )) 2020-05-26 14:20:14,765 - up42.project - INFO - Getting existing workflows in project ... 2020-05-26 14:20:15,434 - up42.project - INFO - Got 12 workflows for project d3ea9123-2b89-4975-be16-e978fed8329c. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:07<00:00, 1.52it/s] 2020-05-26 14:20:23,341 - up42.project - INFO - Using existing workflow: S1-GRD_SNAP, 1585a205-3907-4905-bd02-057a17c3cf84. 2020-05-26 14:20:24,728 - up42.tools - INFO - Getting blocks name and id, use basic=False for all block details. {'tiling': 'd350aa0b-ac31-4021-bbe6-fd8da366740a', 'oneatlas-spot-aoiclipped': '0f15e07f-efcc-4598-939b-18aade349c57', 'oneatlas-pleiades-aoiclipped': 'f026874d-e95e-4293-b811-7667130e054d', 'sobloo-s1-grd-fullscene': '4524e2de-c780-488d-9818-fe68dad9f095', 'sobloo-s2-l1c-fullscene': '604988cb-8252-4161-bf28-f6fb63d7371c', 'snap-polarimetric': '320158d6-8f93-4484-a828-e1fb64f677ff', 'sentinelhub-s2-aoiclipped': 'c4758545-4b74-4318-ae1f-d5ba72f234ca', 'sentinelhub-landsat8-aoiclipped': 'e0b133ae-7b9c-435c-99ac-c4527cc8d9cf', 'sobloo-s1-slc-fullscene': 'cf822545-c73c-467b-8f43-5311dbefe03f', 'nasa-modis': '61279eb8-02e1-4b7a-ac3d-1f62d19d3484', 's2-superresolution': '4872fef8-aec8-4dec-adcb-560ee4430a2b', 'oneatlas-pleiades-fullscene': '8487adcd-a4d7-4cb7-b826-75a533e1f330', 'oneatlas-spot-fullscene': 'aa62113f-0dd1-40a3-a004-954c9d087071', 'data-conversion': '470eedda-5f62-433c-8562-98eb8783af87', 'pansharpen': '2f24c662-c129-409f-a7c3-afa16a4c78cb', 'sobloo-s1-grd-aoiclipped': 'a956166f-c0ed-4670-8a43-87bed8d222f3', 'hello-world': '77899bd0-40de-4aa7-b67e-513a5655afcb', 'ndvi': 'aecb81ef-1c92-4b2e-aa25-55ccebe2f90a', 'sobloo-s2-l1c-aoiclipped': 'a2daaab4-196d-4226-a018-a810444dcad1', 'sobloo-s3': 'fee13ec1-a067-4d6a-95dc-a4fef458f4f4', 'sobloo-s5p': 'cba7c59b-548d-48bf-8920-c20d7d316dfd', 'kmeans-clustering': 'adf21e0a-98bf-41a9-a2cc-a59898a461ba', 'vectorising': '295bf286-0748-474f-aa38-c1ac35151204', 'crs-conversion': '18e6772f-cf33-4955-b7e4-61df8a108fd9', 'sharpening': '4ed70368-d4e1-4462-bef6-14e768049471', 'zonal-statistics': 'a5b8b938-6fd6-4bac-92cd-dffd7f3169aa', 'superresolution': '6c914299-7203-40ad-9b89-de0b4e827bd0', 's5p-lvl3': '9a593e06-eca0-49e0-8b8c-6fe95f699f9d', 'hexagon-aerial-30cm': '0b04d9f7-3a8e-4467-9fb8-1e8343c9469a', 'hexagon-aerial-15cm': '36fe7d3a-4671-424b-bd54-918dd21cfde1', 'ship-identification': '20a3bd1e-3f27-40cf-9915-8fa3d5024ade', 'ais-hvp': '7394287a-2458-4204-be62-36b6d264bcfe', 'ais-hvt': '67eb1763-abeb-4188-b135-f6a0d669d759', 'meteomatics': 'ed0beedb-111b-4285-aa2d-f876a4c16a32', 'oneatlas-pleiades-primary': 'd1e5e0de-71fa-4488-9c0e-3f22ac74a2b6', 'tiled-k-means': '45c0284a-5cd7-4bc0-9d6c-db05f7271036', 'data-conversion-netcdf': 'c358d5af-7819-4ecf-b8b3-629d5d3ba319', 'data-conversion-dimap': '25f42430-d108-4ea4-a81d-2c2b3fff7d11', 'aws-s2-l2a': '73f1b9e0-83de-4cbd-addf-5e0d23ed65d4', 'nextmapone-3m': '56a0ac08-69a5-491a-993d-63e3b9da315c', 'nextmapone-6m': 'f87dd9f3-fc14-48b1-b77d-d695fb65fcc4', 'nextmapone-1m': 'bfd43fbc-b662-4874-9147-658a55bf9edc'} In [7]: # Fill the workflow with tasks input_tasks = [ 'sobloo-s1-grd-fullscene' , 'snap-polarimetric' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) workflow . get_parameters_info () 2020-05-26 14:21:03,273 - up42.workflow - INFO - Added tasks to workflow: [{'name': 'sobloo-s1-grd-fullscene:1', 'parentName': None, 'blockId': '4524e2de-c780-488d-9818-fe68dad9f095'}, {'name': 'snap-polarimetric:1', 'parentName': 'sobloo-s1-grd-fullscene:1', 'blockId': '320158d6-8f93-4484-a828-e1fb64f677ff'}] 2020-05-26 14:21:04,000 - up42.workflow - INFO - Got 2 tasks/blocks in workflow 1585a205-3907-4905-bd02-057a17c3cf84. Out[7]: {'sobloo-s1-grd-fullscene:1': {'ids': {'type': 'array', 'default': None}, 'bbox': {'type': 'array', 'default': None}, 'time': {'type': 'dateRange', 'default': '2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00'}, 'limit': {'type': 'integer', 'default': 1, 'minimum': 1}, 'contains': {'type': 'geometry'}, 'intersects': {'type': 'geometry'}, 'time_series': {'type': 'array', 'default': None}, 'orbit_direction': {'type': 'string', 'default': None}, 'acquisition_mode': {'type': 'string', 'default': None}}, 'snap-polarimetric:1': {'bbox': {'type': 'array', 'default': None}, 'mask': {'type': 'array', 'items': {'enum': ['land', 'sea'], 'type': 'string'}, 'default': None}, 'contains': {'type': 'geometry', 'default': None}, 'intersects': {'type': 'geometry', 'default': None}, 'clip_to_aoi': {'type': 'boolean', 'default': False}, 'tcorrection': {'type': 'boolean', 'default': True}, 'linear_to_db': {'type': 'boolean', 'default': True}, 'polarisations': {'type': 'array', 'items': {'enum': ['VV', 'VH'], 'type': 'string'}, 'default': ['VV'], 'required': False, 'description': 'Requested polarisations for the output'}, 'speckle_filter': {'type': 'boolean', 'default': True}, 'calibration_band': {'type': 'array', 'default': ['sigma']}}} Define aoi and input parameters \u00b6 The S1 GRD block always delivers the complete images as they are delivered in SAFE format which cannot be clipped. The image can be clipped though by the SNAP polarimetric processing blocks. For this it is necessary to supply the same geometry (in this case a bbox) to that block as well. In [8]: input_parameters = { \"sobloo-s1-grd-fullscene:1\" : { \"bbox\" : [ 13.371037 , 52.512799 , 13.382624 , 52.518747 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , }, \"snap-polarimetric:1\" : { \"bbox\" : [ 13.371037 , 52.512799 , 13.382624 , 52.518747 ], \"mask\" : None , \"contains\" : None , \"intersects\" : None , \"clip_to_aoi\" : True , \"tcorrection\" : True , \"linear_to_db\" : True , \"polarisations\" : [ \"VV\" ], \"speckle_filter\" : True , \"calibration_band\" : [ \"sigma\" ] } } Run test query \u00b6 Run a test job to query data availability and check the configuration. In [9]: test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) 2020-05-26 14:21:14,155 - up42.workflow - INFO - +++++++++++++++++++++++++++++++++ 2020-05-26 14:21:14,157 - up42.workflow - INFO - Running this job as Test Query... 2020-05-26 14:21:14,157 - up42.workflow - INFO - +++++++++++++++++++++++++++++++++ 2020-05-26 14:21:14,158 - up42.workflow - INFO - Selected input_parameters: {'sobloo-s1-grd-fullscene:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'ids': None, 'time': '2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00', 'limit': 1, 'zoom_level': 14}, 'snap-polarimetric:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'mask': None, 'contains': None, 'intersects': None, 'clip_to_aoi': True, 'tcorrection': True, 'linear_to_db': True, 'polarisations': ['VV'], 'speckle_filter': True, 'calibration_band': ['sigma']}, 'config': {'mode': 'DRY_RUN'}}. 2020-05-26 14:21:15,659 - up42.workflow - INFO - Created and running new job: 28c72c74-a5dc-46e3-bb70-f6efad3f910c. 2020-05-26 14:21:16,144 - up42.job - INFO - Tracking job status continuously, reporting every 30 seconds... 2020-05-26 14:21:39,235 - up42.job - INFO - Job finished successfully! - 28c72c74-a5dc-46e3-bb70-f6efad3f910c 2020-05-26 14:21:44,956 - up42.job - INFO - Retrieved 1 features. {'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'bbox': [12.306028, 52.168652, 16.66515, 54.078037], 'id': '5a0bcc6b-8316-45c9-9106-8de8e27e5753', 'geometry': {'type': 'Polygon', 'coordinates': [[[16.151102, 52.168652], [16.66515, 53.660217], [12.685362, 54.078037], [12.306028, 52.5835], [16.151102, 52.168652]]]}, 'properties': {'attachments': [], 'visibility': {'deleted': False}, 'illumination': {}, 'production': {'levelCode': 'L1', 'ongoing': False, 'timeliness': 'Fast-24h'}, 'archive': {'offLine': False, 'filename': 'S1A_IW_GRDH_1SDV_20200525T051659_20200525T051724_032717_03CA2F_DB50.SAFE', 'size': 1689, 'format': 'SAFE', 'onLine': False}, 'spatialCoverage': {'verticality': {}, 'geometry': {'geographicBoundingPolygon': {'coordinates': [[[16.151102, 52.168652], [16.66515, 53.660217], [12.685362, 54.078037], [12.306028, 52.5835], [16.151102, 52.168652]]], 'type': 'Polygon'}, 'global': False, 'centerPoint': {'lon': 14.45556570632881, 'lat': 53.12652491313295}}}, 'quality': {'qualified': False}, 'target': {}, 'timeStamp': 1590383819131, 'uid': '5a0bcc6b-8316-45c9-9106-8de8e27e5753', 'identification': {'profile': 'Image', 'externalId': 'S1A_IW_GRDH_1SDV_20200525T051659_20200525T051724_032717_03CA2F_DB50', 'collection': 'Sentinel-1', 'type': 'GRD', 'dataset': {}}, 'transmission': {}, 'contentDescription': {}, 'provider': {}, 'acquisition': {'endViewingDate': 1590383844129, 'mission': 'Sentinel-1', 'missionId': 'A', 'missionCode': 'S1A', 'beginViewingDate': 1590383819131, 'missionName': 'Sentinel-1A', 'polarization': 'VV VH', 'type': 'NOMINAL', 'sensorMode': 'IW', 'sensorId': 'SAR-C SAR'}, 'orbit': {'relativeNumber': 95, 'lastRelativeNumber': 95, 'lastNumber': 32717, 'direction': 'DESCENDING'}, 'state': {'resources': {'thumbnail': True, 'quicklook': True}, 'services': {'wmts': False, 'download': 'internal', 'wcs': False, 'wms': False}, 'insertionDate': 1590395388355}, 'attitude': {}}}]} Run the job \u00b6 In [10]: job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) 2020-05-26 14:21:44,963 - up42.workflow - INFO - Selected input_parameters: {'sobloo-s1-grd-fullscene:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'ids': None, 'time': '2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00', 'limit': 1, 'zoom_level': 14}, 'snap-polarimetric:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'mask': None, 'contains': None, 'intersects': None, 'clip_to_aoi': True, 'tcorrection': True, 'linear_to_db': True, 'polarisations': ['VV'], 'speckle_filter': True, 'calibration_band': ['sigma']}}. 2020-05-26 14:21:46,310 - up42.workflow - INFO - Created and running new job: 2e0e9cf0-8d80-439a-bc50-179543641ea9. 2020-05-26 14:21:46,734 - up42.job - INFO - Tracking job status continuously, reporting every 30 seconds... 2020-05-26 14:22:21,006 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:23:06,480 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:23:40,185 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:24:13,848 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:24:47,455 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:25:21,147 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:25:54,825 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:26:28,527 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:27:02,484 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:27:36,284 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:28:09,935 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:28:42,827 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:29:15,409 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:29:48,015 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:30:20,694 - up42.job - INFO - Job finished successfully! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 Visualize the results \u00b6 In [12]: # Download results: job . download_results () # Visualize downloaded results job . plot_results () Next processing steps \u00b6 The output of the workflow is a GeoTIFF file which can be processed by different algorithms. It is for example possible to apply raster tiling and then do some machine learning based analysis at the end.","title":"Radar processing"},{"location":"examples/radar_processing_1/#example-radar-processing","text":"The processing of radar data - often referred to as SAR data, which stands for Synthetice Aperture Radar - is a topic of its own and quite different from optical data that uses Red, Green, Blue, NIR and similar bands. This is the first of a series of jupyter notebooks where example SAR workflows will be explained that can be run on UP42. All of the examples are based on a specific type of analysis which is called polarimetry and processes one image at a time. In comparison to polarimetric there also is interferometric analysis which leverages the information encoded in suitable pairs of radar image. In this notebook we want to have a look at the Sentinel-1 GRD full scene block, followed by SNAP Sentinel-1 Polarimetric Processing. Polarimetric SAR Processing turns a Sentinel-1 GRD image into a multichannel GeoTIFF that is ready for analysis. Prepare workflow Define aoi and input parameters Run Job Visualize results In [1]: import up42 In [3]: # Authenticate with UP42 up42 . authenticate ( cfg_file = \"config.json\" ) #up42.authenticate(project_id=12345, project_api_key=12345) 2020-05-26 14:19:56,546 - up42.auth - INFO - Got credentials from config file. 2020-05-26 14:19:57,155 - up42.auth - INFO - Authentication with UP42 successful!","title":"Example: Radar processing"},{"location":"examples/radar_processing_1/#prepare-up42-workflows","text":"Create a new project on UP42 or use an existing one. In [5]: S1_SNAP_project = up42 . initialize_project () 2020-05-26 14:20:10,960 - up42 - INFO - Working on Project with project_id d3ea9123-2b89-4975-be16-e978fed8329c Create workflow and check available blocks and data In [6]: workflow = S1_SNAP_project . create_workflow ( name = \"S1-GRD_SNAP\" , use_existing = True ) print ( up42 . get_blocks ( basic = True )) 2020-05-26 14:20:14,765 - up42.project - INFO - Getting existing workflows in project ... 2020-05-26 14:20:15,434 - up42.project - INFO - Got 12 workflows for project d3ea9123-2b89-4975-be16-e978fed8329c. 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:07<00:00, 1.52it/s] 2020-05-26 14:20:23,341 - up42.project - INFO - Using existing workflow: S1-GRD_SNAP, 1585a205-3907-4905-bd02-057a17c3cf84. 2020-05-26 14:20:24,728 - up42.tools - INFO - Getting blocks name and id, use basic=False for all block details. {'tiling': 'd350aa0b-ac31-4021-bbe6-fd8da366740a', 'oneatlas-spot-aoiclipped': '0f15e07f-efcc-4598-939b-18aade349c57', 'oneatlas-pleiades-aoiclipped': 'f026874d-e95e-4293-b811-7667130e054d', 'sobloo-s1-grd-fullscene': '4524e2de-c780-488d-9818-fe68dad9f095', 'sobloo-s2-l1c-fullscene': '604988cb-8252-4161-bf28-f6fb63d7371c', 'snap-polarimetric': '320158d6-8f93-4484-a828-e1fb64f677ff', 'sentinelhub-s2-aoiclipped': 'c4758545-4b74-4318-ae1f-d5ba72f234ca', 'sentinelhub-landsat8-aoiclipped': 'e0b133ae-7b9c-435c-99ac-c4527cc8d9cf', 'sobloo-s1-slc-fullscene': 'cf822545-c73c-467b-8f43-5311dbefe03f', 'nasa-modis': '61279eb8-02e1-4b7a-ac3d-1f62d19d3484', 's2-superresolution': '4872fef8-aec8-4dec-adcb-560ee4430a2b', 'oneatlas-pleiades-fullscene': '8487adcd-a4d7-4cb7-b826-75a533e1f330', 'oneatlas-spot-fullscene': 'aa62113f-0dd1-40a3-a004-954c9d087071', 'data-conversion': '470eedda-5f62-433c-8562-98eb8783af87', 'pansharpen': '2f24c662-c129-409f-a7c3-afa16a4c78cb', 'sobloo-s1-grd-aoiclipped': 'a956166f-c0ed-4670-8a43-87bed8d222f3', 'hello-world': '77899bd0-40de-4aa7-b67e-513a5655afcb', 'ndvi': 'aecb81ef-1c92-4b2e-aa25-55ccebe2f90a', 'sobloo-s2-l1c-aoiclipped': 'a2daaab4-196d-4226-a018-a810444dcad1', 'sobloo-s3': 'fee13ec1-a067-4d6a-95dc-a4fef458f4f4', 'sobloo-s5p': 'cba7c59b-548d-48bf-8920-c20d7d316dfd', 'kmeans-clustering': 'adf21e0a-98bf-41a9-a2cc-a59898a461ba', 'vectorising': '295bf286-0748-474f-aa38-c1ac35151204', 'crs-conversion': '18e6772f-cf33-4955-b7e4-61df8a108fd9', 'sharpening': '4ed70368-d4e1-4462-bef6-14e768049471', 'zonal-statistics': 'a5b8b938-6fd6-4bac-92cd-dffd7f3169aa', 'superresolution': '6c914299-7203-40ad-9b89-de0b4e827bd0', 's5p-lvl3': '9a593e06-eca0-49e0-8b8c-6fe95f699f9d', 'hexagon-aerial-30cm': '0b04d9f7-3a8e-4467-9fb8-1e8343c9469a', 'hexagon-aerial-15cm': '36fe7d3a-4671-424b-bd54-918dd21cfde1', 'ship-identification': '20a3bd1e-3f27-40cf-9915-8fa3d5024ade', 'ais-hvp': '7394287a-2458-4204-be62-36b6d264bcfe', 'ais-hvt': '67eb1763-abeb-4188-b135-f6a0d669d759', 'meteomatics': 'ed0beedb-111b-4285-aa2d-f876a4c16a32', 'oneatlas-pleiades-primary': 'd1e5e0de-71fa-4488-9c0e-3f22ac74a2b6', 'tiled-k-means': '45c0284a-5cd7-4bc0-9d6c-db05f7271036', 'data-conversion-netcdf': 'c358d5af-7819-4ecf-b8b3-629d5d3ba319', 'data-conversion-dimap': '25f42430-d108-4ea4-a81d-2c2b3fff7d11', 'aws-s2-l2a': '73f1b9e0-83de-4cbd-addf-5e0d23ed65d4', 'nextmapone-3m': '56a0ac08-69a5-491a-993d-63e3b9da315c', 'nextmapone-6m': 'f87dd9f3-fc14-48b1-b77d-d695fb65fcc4', 'nextmapone-1m': 'bfd43fbc-b662-4874-9147-658a55bf9edc'} In [7]: # Fill the workflow with tasks input_tasks = [ 'sobloo-s1-grd-fullscene' , 'snap-polarimetric' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) workflow . get_parameters_info () 2020-05-26 14:21:03,273 - up42.workflow - INFO - Added tasks to workflow: [{'name': 'sobloo-s1-grd-fullscene:1', 'parentName': None, 'blockId': '4524e2de-c780-488d-9818-fe68dad9f095'}, {'name': 'snap-polarimetric:1', 'parentName': 'sobloo-s1-grd-fullscene:1', 'blockId': '320158d6-8f93-4484-a828-e1fb64f677ff'}] 2020-05-26 14:21:04,000 - up42.workflow - INFO - Got 2 tasks/blocks in workflow 1585a205-3907-4905-bd02-057a17c3cf84. Out[7]: {'sobloo-s1-grd-fullscene:1': {'ids': {'type': 'array', 'default': None}, 'bbox': {'type': 'array', 'default': None}, 'time': {'type': 'dateRange', 'default': '2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00'}, 'limit': {'type': 'integer', 'default': 1, 'minimum': 1}, 'contains': {'type': 'geometry'}, 'intersects': {'type': 'geometry'}, 'time_series': {'type': 'array', 'default': None}, 'orbit_direction': {'type': 'string', 'default': None}, 'acquisition_mode': {'type': 'string', 'default': None}}, 'snap-polarimetric:1': {'bbox': {'type': 'array', 'default': None}, 'mask': {'type': 'array', 'items': {'enum': ['land', 'sea'], 'type': 'string'}, 'default': None}, 'contains': {'type': 'geometry', 'default': None}, 'intersects': {'type': 'geometry', 'default': None}, 'clip_to_aoi': {'type': 'boolean', 'default': False}, 'tcorrection': {'type': 'boolean', 'default': True}, 'linear_to_db': {'type': 'boolean', 'default': True}, 'polarisations': {'type': 'array', 'items': {'enum': ['VV', 'VH'], 'type': 'string'}, 'default': ['VV'], 'required': False, 'description': 'Requested polarisations for the output'}, 'speckle_filter': {'type': 'boolean', 'default': True}, 'calibration_band': {'type': 'array', 'default': ['sigma']}}}","title":"Prepare UP42 workflows"},{"location":"examples/radar_processing_1/#define-aoi-and-input-parameters","text":"The S1 GRD block always delivers the complete images as they are delivered in SAFE format which cannot be clipped. The image can be clipped though by the SNAP polarimetric processing blocks. For this it is necessary to supply the same geometry (in this case a bbox) to that block as well. In [8]: input_parameters = { \"sobloo-s1-grd-fullscene:1\" : { \"bbox\" : [ 13.371037 , 52.512799 , 13.382624 , 52.518747 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , }, \"snap-polarimetric:1\" : { \"bbox\" : [ 13.371037 , 52.512799 , 13.382624 , 52.518747 ], \"mask\" : None , \"contains\" : None , \"intersects\" : None , \"clip_to_aoi\" : True , \"tcorrection\" : True , \"linear_to_db\" : True , \"polarisations\" : [ \"VV\" ], \"speckle_filter\" : True , \"calibration_band\" : [ \"sigma\" ] } }","title":"Define aoi and input parameters"},{"location":"examples/radar_processing_1/#run-test-query","text":"Run a test job to query data availability and check the configuration. In [9]: test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) 2020-05-26 14:21:14,155 - up42.workflow - INFO - +++++++++++++++++++++++++++++++++ 2020-05-26 14:21:14,157 - up42.workflow - INFO - Running this job as Test Query... 2020-05-26 14:21:14,157 - up42.workflow - INFO - +++++++++++++++++++++++++++++++++ 2020-05-26 14:21:14,158 - up42.workflow - INFO - Selected input_parameters: {'sobloo-s1-grd-fullscene:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'ids': None, 'time': '2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00', 'limit': 1, 'zoom_level': 14}, 'snap-polarimetric:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'mask': None, 'contains': None, 'intersects': None, 'clip_to_aoi': True, 'tcorrection': True, 'linear_to_db': True, 'polarisations': ['VV'], 'speckle_filter': True, 'calibration_band': ['sigma']}, 'config': {'mode': 'DRY_RUN'}}. 2020-05-26 14:21:15,659 - up42.workflow - INFO - Created and running new job: 28c72c74-a5dc-46e3-bb70-f6efad3f910c. 2020-05-26 14:21:16,144 - up42.job - INFO - Tracking job status continuously, reporting every 30 seconds... 2020-05-26 14:21:39,235 - up42.job - INFO - Job finished successfully! - 28c72c74-a5dc-46e3-bb70-f6efad3f910c 2020-05-26 14:21:44,956 - up42.job - INFO - Retrieved 1 features. {'type': 'FeatureCollection', 'features': [{'type': 'Feature', 'bbox': [12.306028, 52.168652, 16.66515, 54.078037], 'id': '5a0bcc6b-8316-45c9-9106-8de8e27e5753', 'geometry': {'type': 'Polygon', 'coordinates': [[[16.151102, 52.168652], [16.66515, 53.660217], [12.685362, 54.078037], [12.306028, 52.5835], [16.151102, 52.168652]]]}, 'properties': {'attachments': [], 'visibility': {'deleted': False}, 'illumination': {}, 'production': {'levelCode': 'L1', 'ongoing': False, 'timeliness': 'Fast-24h'}, 'archive': {'offLine': False, 'filename': 'S1A_IW_GRDH_1SDV_20200525T051659_20200525T051724_032717_03CA2F_DB50.SAFE', 'size': 1689, 'format': 'SAFE', 'onLine': False}, 'spatialCoverage': {'verticality': {}, 'geometry': {'geographicBoundingPolygon': {'coordinates': [[[16.151102, 52.168652], [16.66515, 53.660217], [12.685362, 54.078037], [12.306028, 52.5835], [16.151102, 52.168652]]], 'type': 'Polygon'}, 'global': False, 'centerPoint': {'lon': 14.45556570632881, 'lat': 53.12652491313295}}}, 'quality': {'qualified': False}, 'target': {}, 'timeStamp': 1590383819131, 'uid': '5a0bcc6b-8316-45c9-9106-8de8e27e5753', 'identification': {'profile': 'Image', 'externalId': 'S1A_IW_GRDH_1SDV_20200525T051659_20200525T051724_032717_03CA2F_DB50', 'collection': 'Sentinel-1', 'type': 'GRD', 'dataset': {}}, 'transmission': {}, 'contentDescription': {}, 'provider': {}, 'acquisition': {'endViewingDate': 1590383844129, 'mission': 'Sentinel-1', 'missionId': 'A', 'missionCode': 'S1A', 'beginViewingDate': 1590383819131, 'missionName': 'Sentinel-1A', 'polarization': 'VV VH', 'type': 'NOMINAL', 'sensorMode': 'IW', 'sensorId': 'SAR-C SAR'}, 'orbit': {'relativeNumber': 95, 'lastRelativeNumber': 95, 'lastNumber': 32717, 'direction': 'DESCENDING'}, 'state': {'resources': {'thumbnail': True, 'quicklook': True}, 'services': {'wmts': False, 'download': 'internal', 'wcs': False, 'wms': False}, 'insertionDate': 1590395388355}, 'attitude': {}}}]}","title":"Run test query"},{"location":"examples/radar_processing_1/#run-the-job","text":"In [10]: job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) 2020-05-26 14:21:44,963 - up42.workflow - INFO - Selected input_parameters: {'sobloo-s1-grd-fullscene:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'ids': None, 'time': '2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00', 'limit': 1, 'zoom_level': 14}, 'snap-polarimetric:1': {'bbox': [13.371037, 52.512799, 13.382624, 52.518747], 'mask': None, 'contains': None, 'intersects': None, 'clip_to_aoi': True, 'tcorrection': True, 'linear_to_db': True, 'polarisations': ['VV'], 'speckle_filter': True, 'calibration_band': ['sigma']}}. 2020-05-26 14:21:46,310 - up42.workflow - INFO - Created and running new job: 2e0e9cf0-8d80-439a-bc50-179543641ea9. 2020-05-26 14:21:46,734 - up42.job - INFO - Tracking job status continuously, reporting every 30 seconds... 2020-05-26 14:22:21,006 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:23:06,480 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:23:40,185 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:24:13,848 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:24:47,455 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:25:21,147 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:25:54,825 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:26:28,527 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:27:02,484 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:27:36,284 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:28:09,935 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:28:42,827 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:29:15,409 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:29:48,015 - up42.job - INFO - Job is RUNNING! - 2e0e9cf0-8d80-439a-bc50-179543641ea9 2020-05-26 14:30:20,694 - up42.job - INFO - Job finished successfully! - 2e0e9cf0-8d80-439a-bc50-179543641ea9","title":"Run the job"},{"location":"examples/radar_processing_1/#visualize-the-results","text":"In [12]: # Download results: job . download_results () # Visualize downloaded results job . plot_results ()","title":"Visualize the results"},{"location":"examples/radar_processing_1/#next-processing-steps","text":"The output of the workflow is a GeoTIFF file which can be processed by different algorithms. It is for example possible to apply raster tiling and then do some machine learning based analysis at the end.","title":"Next processing steps"},{"location":"reference/catalog/","text":"Catalog class \u00b6 \u00b6 __init__ ( self , auth ) special \u00b6 The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. Source code in up42/catalog.py 54 55 56 57 58 59 60 def __init__ ( self , auth : Auth ): \"\"\" The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. \"\"\" self . auth = auth self . quicklooks = None construct_parameters ( geometry , start_date = '2020-01-01' , end_date = '2020-01-30' , sensors = [ 'pleiades' , 'spot' , 'sentinel1' , 'sentinel2' , 'sentinel3' , 'sentinel5p' ], limit = 1 , max_cloudcover = 100 , sortby = 'cloudCoverage' , ascending = True ) staticmethod \u00b6 Follows STAC principles and property names. Parameters: Name Type Description Default geometry Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.point.Point, shapely.geometry.polygon.Polygon] The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. required start_date str Query period starting day, format \"2020-01-01\". '2020-01-01' end_date str Query period ending day, format \"2020-01-01\". '2020-01-30' sensors List[str] The satellite sensors to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] ['pleiades', 'spot', 'sentinel1', 'sentinel2', 'sentinel3', 'sentinel5p'] limit int The maximum number of search results to return. 1 max_cloudcover float Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. 100 sortby str The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" 'cloudCoverage' ascending bool Ascending sort order by default, descending if False. True Returns: Type Description Dict The constructed parameters dictionary. Source code in up42/catalog.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @staticmethod def construct_parameters ( geometry : Union [ Dict , Feature , FeatureCollection , List , GeoDataFrame , Point , Polygon , ], start_date : str = \"2020-01-01\" , end_date : str = \"2020-01-30\" , sensors : List [ str ] = [ \"pleiades\" , \"spot\" , \"sentinel1\" , \"sentinel2\" , \"sentinel3\" , \"sentinel5p\" , ], limit : int = 1 , max_cloudcover : float = 100 , sortby : str = \"cloudCoverage\" , ascending : bool = True , ) -> Dict : \"\"\" Follows STAC principles and property names. Args: geometry: The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". sensors: The satellite sensors to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] limit: The maximum number of search results to return. max_cloudcover: Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. sortby: The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" ascending: Ascending sort order by default, descending if False. Returns: The constructed parameters dictionary. \"\"\" datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" block_filters : List [ str ] = [] for sensor in sensors : if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) block_filters . extend ( supported_sensors [ sensor ][ \"blocks\" ]) query_filters = { \"cloudCoverage\" : { \"lte\" : max_cloudcover }, \"dataBlock\" : { \"in\" : block_filters }, } if ascending : sort_order = \"asc\" else : sort_order = \"desc\" aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_geometry = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = \"intersects\" , squash_multiple_features = \"footprint\" , ) # TODO: cc also contains nan with sentinel 1 etc. ignore? search_parameters = { \"datetime\" : datetime , \"intersects\" : aoi_geometry , \"limit\" : limit , \"query\" : query_filters , \"sortby\" : [{ \"field\" : f \"properties. { sortby } \" , \"direction\" : sort_order }], } return search_parameters download_quicklooks ( self , image_ids , sensor , output_directory = None ) \u00b6 Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Parameters: Name Type Description Default image_ids List[str] provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" required sensor str The satellite sensor of the image_ids, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". required output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of quicklook image output file paths. Source code in up42/catalog.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def download_quicklooks ( self , image_ids : List [ str ], sensor : str , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Args: image_ids: provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" sensor: The satellite sensor of the image_ids, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". output_directory: The file output directory, defaults to the current working directory. Returns: List of quicklook image output file paths. \"\"\" if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) provider = supported_sensors [ sensor ][ \"provider\" ] logger . info ( \"Getting quicklooks from provider %s for image_ids: %s \" , provider , image_ids ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / \"catalog\" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) if isinstance ( image_ids , str ): image_ids = [ image_ids ] out_paths : List [ str ] = [] for image_id in tqdm ( image_ids ): out_path = output_directory / f \"quicklook_ { image_id } .jpg\" out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /catalog/ { provider } /image/ { image_id } /quicklook\" ) try : response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) response . raise_for_status () except HTTPError as err : raise SystemExit ( err ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths search ( self , search_parameters , as_dataframe = True ) \u00b6 Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Parameters: Name Type Description Default search_parameters Dict The catalog search parameters, see example. required as_dataframe bool return type, GeoDataFrame if True (default), FeatureCollection if False. True Returns: Type Description Union[geopandas.geodataframe.GeoDataFrame, Dict] The search results as a GeoDataFrame, optionally as json dict. Examples: search_parameters = { \"datetime\" : \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\" , \"intersects\" : { \"type\" : \"Polygon\" , \"coordinates\" : [[[ 13.32113746 , 52.73971768 ],[ 13.15981158 , 52.2092959 ], [ 13.62204483 , 52.15632025 ],[ 13.78859517 , 52.68655119 ],[ 13.32113746 , 52.73971768 ]]]}, \"limit\" : 1 , \"sortby\" : [{ \"field\" : \"properties.cloudCoverage\" , \"direction\" : \"asc\" }] } Source code in up42/catalog.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def search ( self , search_parameters : Dict , as_dataframe : bool = True ) -> Union [ GeoDataFrame , Dict ]: \"\"\" Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Args: search_parameters: The catalog search parameters, see example. as_dataframe: return type, GeoDataFrame if True (default), FeatureCollection if False. Returns: The search results as a GeoDataFrame, optionally as json dict. Example: ```python search_parameters={ \"datetime\": \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\", \"intersects\": { \"type\": \"Polygon\", \"coordinates\": [[[13.32113746,52.73971768],[13.15981158,52.2092959], [13.62204483,52.15632025],[13.78859517,52.68655119],[13.32113746, 52.73971768]]]}, \"limit\": 1, \"sortby\": [{\"field\" : \"properties.cloudCoverage\",\"direction\" : \"asc\"}] } ``` \"\"\" logger . info ( \"Searching catalog with: %r \" , search_parameters ) url = f \" { self . auth . _endpoint () } /catalog/stac/search\" response_json = self . auth . _request ( \"POST\" , url , search_parameters ) logger . info ( \" %d results returned.\" , len ( response_json [ \"features\" ])) dst_crs = \"EPSG:4326\" df = GeoDataFrame . from_features ( response_json , crs = dst_crs ) if df . empty : if as_dataframe : return df else : return df . __geo_interface__ # Filter to actual geometries intersecting the aoi (Sobloo search uses a rectangular # bounds geometry, can contain scenes that touch the aoi bbox, but not the aoi. # So number returned images not consistent with set limit. # TODO: Resolve on backend geometry = search_parameters [ \"intersects\" ] poly = shape ( geometry ) df = df [ df . intersects ( poly )] df = df . reset_index () # Make scene_id more easily accessible # TODO: Add by default to results, independent of sensor. def _get_scene_id ( row ): if row [ \"providerName\" ] == \"oneatlas\" : row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"parentIdentifier\" ] elif row [ \"providerName\" ] in [ \"sobloo-radar\" , \"sobloo-image\" ]: row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"identification\" ][ \"externalId\" ] return row df = df . apply ( _get_scene_id , axis = 1 ) df . crs = dst_crs # apply resets the crs if as_dataframe : return df else : return df . __geo_interface__","title":"Catalog"},{"location":"reference/catalog/#catalog-class","text":"","title":"Catalog class"},{"location":"reference/catalog/#up42.catalog.Catalog","text":"","title":"up42.catalog.Catalog"},{"location":"reference/catalog/#up42.catalog.Catalog.__init__","text":"The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. Source code in up42/catalog.py 54 55 56 57 58 59 60 def __init__ ( self , auth : Auth ): \"\"\" The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. \"\"\" self . auth = auth self . quicklooks = None","title":"__init__()"},{"location":"reference/catalog/#up42.catalog.Catalog.construct_parameters","text":"Follows STAC principles and property names. Parameters: Name Type Description Default geometry Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.point.Point, shapely.geometry.polygon.Polygon] The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. required start_date str Query period starting day, format \"2020-01-01\". '2020-01-01' end_date str Query period ending day, format \"2020-01-01\". '2020-01-30' sensors List[str] The satellite sensors to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] ['pleiades', 'spot', 'sentinel1', 'sentinel2', 'sentinel3', 'sentinel5p'] limit int The maximum number of search results to return. 1 max_cloudcover float Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. 100 sortby str The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" 'cloudCoverage' ascending bool Ascending sort order by default, descending if False. True Returns: Type Description Dict The constructed parameters dictionary. Source code in up42/catalog.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @staticmethod def construct_parameters ( geometry : Union [ Dict , Feature , FeatureCollection , List , GeoDataFrame , Point , Polygon , ], start_date : str = \"2020-01-01\" , end_date : str = \"2020-01-30\" , sensors : List [ str ] = [ \"pleiades\" , \"spot\" , \"sentinel1\" , \"sentinel2\" , \"sentinel3\" , \"sentinel5p\" , ], limit : int = 1 , max_cloudcover : float = 100 , sortby : str = \"cloudCoverage\" , ascending : bool = True , ) -> Dict : \"\"\" Follows STAC principles and property names. Args: geometry: The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". sensors: The satellite sensors to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] limit: The maximum number of search results to return. max_cloudcover: Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. sortby: The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" ascending: Ascending sort order by default, descending if False. Returns: The constructed parameters dictionary. \"\"\" datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" block_filters : List [ str ] = [] for sensor in sensors : if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) block_filters . extend ( supported_sensors [ sensor ][ \"blocks\" ]) query_filters = { \"cloudCoverage\" : { \"lte\" : max_cloudcover }, \"dataBlock\" : { \"in\" : block_filters }, } if ascending : sort_order = \"asc\" else : sort_order = \"desc\" aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_geometry = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = \"intersects\" , squash_multiple_features = \"footprint\" , ) # TODO: cc also contains nan with sentinel 1 etc. ignore? search_parameters = { \"datetime\" : datetime , \"intersects\" : aoi_geometry , \"limit\" : limit , \"query\" : query_filters , \"sortby\" : [{ \"field\" : f \"properties. { sortby } \" , \"direction\" : sort_order }], } return search_parameters","title":"construct_parameters()"},{"location":"reference/catalog/#up42.catalog.Catalog.download_quicklooks","text":"Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Parameters: Name Type Description Default image_ids List[str] provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" required sensor str The satellite sensor of the image_ids, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". required output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of quicklook image output file paths. Source code in up42/catalog.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 def download_quicklooks ( self , image_ids : List [ str ], sensor : str , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Args: image_ids: provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" sensor: The satellite sensor of the image_ids, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". output_directory: The file output directory, defaults to the current working directory. Returns: List of quicklook image output file paths. \"\"\" if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) provider = supported_sensors [ sensor ][ \"provider\" ] logger . info ( \"Getting quicklooks from provider %s for image_ids: %s \" , provider , image_ids ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / \"catalog\" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) if isinstance ( image_ids , str ): image_ids = [ image_ids ] out_paths : List [ str ] = [] for image_id in tqdm ( image_ids ): out_path = output_directory / f \"quicklook_ { image_id } .jpg\" out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /catalog/ { provider } /image/ { image_id } /quicklook\" ) try : response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) response . raise_for_status () except HTTPError as err : raise SystemExit ( err ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/catalog/#up42.catalog.Catalog.search","text":"Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Parameters: Name Type Description Default search_parameters Dict The catalog search parameters, see example. required as_dataframe bool return type, GeoDataFrame if True (default), FeatureCollection if False. True Returns: Type Description Union[geopandas.geodataframe.GeoDataFrame, Dict] The search results as a GeoDataFrame, optionally as json dict. Examples: search_parameters = { \"datetime\" : \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\" , \"intersects\" : { \"type\" : \"Polygon\" , \"coordinates\" : [[[ 13.32113746 , 52.73971768 ],[ 13.15981158 , 52.2092959 ], [ 13.62204483 , 52.15632025 ],[ 13.78859517 , 52.68655119 ],[ 13.32113746 , 52.73971768 ]]]}, \"limit\" : 1 , \"sortby\" : [{ \"field\" : \"properties.cloudCoverage\" , \"direction\" : \"asc\" }] } Source code in up42/catalog.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def search ( self , search_parameters : Dict , as_dataframe : bool = True ) -> Union [ GeoDataFrame , Dict ]: \"\"\" Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Args: search_parameters: The catalog search parameters, see example. as_dataframe: return type, GeoDataFrame if True (default), FeatureCollection if False. Returns: The search results as a GeoDataFrame, optionally as json dict. Example: ```python search_parameters={ \"datetime\": \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\", \"intersects\": { \"type\": \"Polygon\", \"coordinates\": [[[13.32113746,52.73971768],[13.15981158,52.2092959], [13.62204483,52.15632025],[13.78859517,52.68655119],[13.32113746, 52.73971768]]]}, \"limit\": 1, \"sortby\": [{\"field\" : \"properties.cloudCoverage\",\"direction\" : \"asc\"}] } ``` \"\"\" logger . info ( \"Searching catalog with: %r \" , search_parameters ) url = f \" { self . auth . _endpoint () } /catalog/stac/search\" response_json = self . auth . _request ( \"POST\" , url , search_parameters ) logger . info ( \" %d results returned.\" , len ( response_json [ \"features\" ])) dst_crs = \"EPSG:4326\" df = GeoDataFrame . from_features ( response_json , crs = dst_crs ) if df . empty : if as_dataframe : return df else : return df . __geo_interface__ # Filter to actual geometries intersecting the aoi (Sobloo search uses a rectangular # bounds geometry, can contain scenes that touch the aoi bbox, but not the aoi. # So number returned images not consistent with set limit. # TODO: Resolve on backend geometry = search_parameters [ \"intersects\" ] poly = shape ( geometry ) df = df [ df . intersects ( poly )] df = df . reset_index () # Make scene_id more easily accessible # TODO: Add by default to results, independent of sensor. def _get_scene_id ( row ): if row [ \"providerName\" ] == \"oneatlas\" : row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"parentIdentifier\" ] elif row [ \"providerName\" ] in [ \"sobloo-radar\" , \"sobloo-image\" ]: row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"identification\" ][ \"externalId\" ] return row df = df . apply ( _get_scene_id , axis = 1 ) df . crs = dst_crs # apply resets the crs if as_dataframe : return df else : return df . __geo_interface__","title":"search()"},{"location":"reference/cli/","text":"Command Line Interface (CLI) \u00b6 main \u00b6 Usage: main [OPTIONS] COMMAND [ARGS]... Options: -pid, --project-id TEXT Your project ID, get it in the Project settings in the console. -pkey, --project-api-key TEXT Your project API KEY, get in the Project settings in the console. -cfg, --config-file PATH File path to the config.json with {project_id: '...', project_api_key: '...'} --env TEXT auth \u00b6 Check authentication. Usage: main auth [OPTIONS] catalog \u00b6 UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover. Usage: main catalog [OPTIONS] COMMAND [ARGS]... construct-parameters \u00b6 Follows STAC principles and property names to create a filter for catalog search. Usage: main catalog construct-parameters [OPTIONS] GEOM_FILE Options: --start-date [%Y-%m-%d] Query period starting day, format '2020-01-01'. --end-date [%Y-%m-%d] Query period ending day, format '2020-01-01'. --sensors [pleiades|spot|sentinel1|sentinel2|sentinel3|sentinel5p] Imagery sensors to search for. --max-cloud-cover INTEGER RANGE Maximum cloudcover percentage. 100 will return all scenes,8.4 will return all scenes with 8.4 or less cloudcover. --limit INTEGER The maximum number of search results. search \u00b6 Searches the catalog for the search parameters and returns the metadata of the matching scenes. Generate search parameters with 'up42 catalog construct-parameters'. Usage: main catalog search [OPTIONS] SEARCH_PARAMETERS_JSON config \u00b6 Create a config file. Usage: main config [OPTIONS] Options: --env TEXT get-block-details \u00b6 Get details of block by block name. Usage: main get-block-details [OPTIONS] Options: -n, --block-name TEXT Block name to get details. [required] get-blocks \u00b6 Get public blocks information. Usage: main get-blocks [OPTIONS] Options: -t, --block-type [data|processing] Filter by block type. --basic / --full Show basic or full block information. job \u00b6 Get job status, results and more. Usage: main job [OPTIONS] COMMAND [ARGS]... Options: -jid, --job-id TEXT Your job ID, get it by creating a job or running 'up42 project workflow get-jobs' [required] cancel-job \u00b6 Cancel a job that is running. Usage: main job cancel-job [OPTIONS] download-quicklooks \u00b6 Download a job's quicklooks. Usage: main job download-quicklooks [OPTIONS] OUTPUT_DIRECTORY download-results \u00b6 Download and unpack the job results. Usage: main job download-results [OPTIONS] OUTPUT_DIRECTORY get-info \u00b6 Get information about the job. Usage: main job get-info [OPTIONS] get-jobtasks \u00b6 Get the individual items of the job. Usage: main job get-jobtasks [OPTIONS] get-jobtasks-results-json \u00b6 Convenience function to get the resulting data.json of all job tasks. Usage: main job get-jobtasks-results-json [OPTIONS] get-logs \u00b6 Convenience function to print or return the logs of all job tasks. Usage: main job get-logs [OPTIONS] get-results-json \u00b6 Get the job results data.json. Usage: main job get-results-json [OPTIONS] get-status \u00b6 Get the job status. Usage: main job get-status [OPTIONS] track-status \u00b6 Track the job status with regular time intervals. Usage: main job track-status [OPTIONS] Options: -i, --interval INTEGER RANGE Interval between getting job status in seconds. project \u00b6 Create and get workflows, manage project settings and more. Usage: main project [OPTIONS] COMMAND [ARGS]... create-workflow \u00b6 Create a workflow. Usage: main project create-workflow [OPTIONS] NAME get-project-settings \u00b6 Get the project settings. Usage: main project get-project-settings [OPTIONS] get-workflows \u00b6 Get the project workflows. Usage: main project get-workflows [OPTIONS] update-project-settings \u00b6 Update project settings. Usage: main project update-project-settings [OPTIONS] Options: --max-aoi-size INTEGER RANGE The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. --max-concurrent-jobs INTEGER RANGE The maximum number of concurrent jobs, from 1-10, default 1. --number-of-images INTEGER RANGE The maximum number of images returned with each job, from 1-20, default 10. workflow-from-name \u00b6 Use a workflow from name. Usage: main project workflow-from-name [OPTIONS] Options: -n, --workflow-name TEXT Workflow name to use. [required] validate-manifest \u00b6 Validate a block manifest. Usage: main validate-manifest [OPTIONS] MANIFEST_JSON workflow \u00b6 Add workflow tasks, run a job and more. Usage: main workflow [OPTIONS] COMMAND [ARGS]... Options: -wid, --workflow-id TEXT Your workflow ID, get it by creating a workflow or running 'up42 project get-workflows' [required] add-workflow-tasks \u00b6 Adds or overwrites workflow tasks. - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. Usage: main workflow add-workflow-tasks [OPTIONS] INPUT_TASKS_JSON delete \u00b6 Delete the workflow. Usage: main workflow delete [OPTIONS] get-compatible-blocks \u00b6 Get all compatible blocks for the current workflow. Usage: main workflow get-compatible-blocks [OPTIONS] get-info \u00b6 Get information about the workflow. Usage: main workflow get-info [OPTIONS] get-jobs \u00b6 Get the jobs ran with this workflow. Usage: main workflow get-jobs [OPTIONS] get-parameters-info \u00b6 Get info about the parameters of each task in the workflow to make it easy to construct the desired parameters. Usage: main workflow get-parameters-info [OPTIONS] get-workflow-tasks \u00b6 Get the workflow tasks list (DAG). Usage: main workflow get-workflow-tasks [OPTIONS] Options: --basic / --full Show basic or full task information. run-job \u00b6 Creates and runs a new job. Usage: main workflow run-job [OPTIONS] INPUT_PARAMETERS_JSON Options: --track Track status of job in shell. test-job \u00b6 Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Usage: main workflow test-job [OPTIONS] INPUT_PARAMETERS_JSON Options: --track Track status of job in shell. update-name \u00b6 Update the workflow name. Usage: main workflow update-name [OPTIONS] Options: -n, --workflow-name TEXT New name for the workflow. [required] --description TEXT An optional description for the workflow.","title":"CLI"},{"location":"reference/cli/#command-line-interface-cli","text":"","title":"Command Line Interface (CLI)"},{"location":"reference/cli/#main","text":"Usage: main [OPTIONS] COMMAND [ARGS]... Options: -pid, --project-id TEXT Your project ID, get it in the Project settings in the console. -pkey, --project-api-key TEXT Your project API KEY, get in the Project settings in the console. -cfg, --config-file PATH File path to the config.json with {project_id: '...', project_api_key: '...'} --env TEXT","title":"main"},{"location":"reference/cli/#auth","text":"Check authentication. Usage: main auth [OPTIONS]","title":"auth"},{"location":"reference/cli/#catalog","text":"UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover. Usage: main catalog [OPTIONS] COMMAND [ARGS]...","title":"catalog"},{"location":"reference/cli/#construct-parameters","text":"Follows STAC principles and property names to create a filter for catalog search. Usage: main catalog construct-parameters [OPTIONS] GEOM_FILE Options: --start-date [%Y-%m-%d] Query period starting day, format '2020-01-01'. --end-date [%Y-%m-%d] Query period ending day, format '2020-01-01'. --sensors [pleiades|spot|sentinel1|sentinel2|sentinel3|sentinel5p] Imagery sensors to search for. --max-cloud-cover INTEGER RANGE Maximum cloudcover percentage. 100 will return all scenes,8.4 will return all scenes with 8.4 or less cloudcover. --limit INTEGER The maximum number of search results.","title":"construct-parameters"},{"location":"reference/cli/#search","text":"Searches the catalog for the search parameters and returns the metadata of the matching scenes. Generate search parameters with 'up42 catalog construct-parameters'. Usage: main catalog search [OPTIONS] SEARCH_PARAMETERS_JSON","title":"search"},{"location":"reference/cli/#config","text":"Create a config file. Usage: main config [OPTIONS] Options: --env TEXT","title":"config"},{"location":"reference/cli/#get-block-details","text":"Get details of block by block name. Usage: main get-block-details [OPTIONS] Options: -n, --block-name TEXT Block name to get details. [required]","title":"get-block-details"},{"location":"reference/cli/#get-blocks","text":"Get public blocks information. Usage: main get-blocks [OPTIONS] Options: -t, --block-type [data|processing] Filter by block type. --basic / --full Show basic or full block information.","title":"get-blocks"},{"location":"reference/cli/#job","text":"Get job status, results and more. Usage: main job [OPTIONS] COMMAND [ARGS]... Options: -jid, --job-id TEXT Your job ID, get it by creating a job or running 'up42 project workflow get-jobs' [required]","title":"job"},{"location":"reference/cli/#cancel-job","text":"Cancel a job that is running. Usage: main job cancel-job [OPTIONS]","title":"cancel-job"},{"location":"reference/cli/#download-quicklooks","text":"Download a job's quicklooks. Usage: main job download-quicklooks [OPTIONS] OUTPUT_DIRECTORY","title":"download-quicklooks"},{"location":"reference/cli/#download-results","text":"Download and unpack the job results. Usage: main job download-results [OPTIONS] OUTPUT_DIRECTORY","title":"download-results"},{"location":"reference/cli/#get-info","text":"Get information about the job. Usage: main job get-info [OPTIONS]","title":"get-info"},{"location":"reference/cli/#get-jobtasks","text":"Get the individual items of the job. Usage: main job get-jobtasks [OPTIONS]","title":"get-jobtasks"},{"location":"reference/cli/#get-jobtasks-results-json","text":"Convenience function to get the resulting data.json of all job tasks. Usage: main job get-jobtasks-results-json [OPTIONS]","title":"get-jobtasks-results-json"},{"location":"reference/cli/#get-logs","text":"Convenience function to print or return the logs of all job tasks. Usage: main job get-logs [OPTIONS]","title":"get-logs"},{"location":"reference/cli/#get-results-json","text":"Get the job results data.json. Usage: main job get-results-json [OPTIONS]","title":"get-results-json"},{"location":"reference/cli/#get-status","text":"Get the job status. Usage: main job get-status [OPTIONS]","title":"get-status"},{"location":"reference/cli/#track-status","text":"Track the job status with regular time intervals. Usage: main job track-status [OPTIONS] Options: -i, --interval INTEGER RANGE Interval between getting job status in seconds.","title":"track-status"},{"location":"reference/cli/#project","text":"Create and get workflows, manage project settings and more. Usage: main project [OPTIONS] COMMAND [ARGS]...","title":"project"},{"location":"reference/cli/#create-workflow","text":"Create a workflow. Usage: main project create-workflow [OPTIONS] NAME","title":"create-workflow"},{"location":"reference/cli/#get-project-settings","text":"Get the project settings. Usage: main project get-project-settings [OPTIONS]","title":"get-project-settings"},{"location":"reference/cli/#get-workflows","text":"Get the project workflows. Usage: main project get-workflows [OPTIONS]","title":"get-workflows"},{"location":"reference/cli/#update-project-settings","text":"Update project settings. Usage: main project update-project-settings [OPTIONS] Options: --max-aoi-size INTEGER RANGE The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. --max-concurrent-jobs INTEGER RANGE The maximum number of concurrent jobs, from 1-10, default 1. --number-of-images INTEGER RANGE The maximum number of images returned with each job, from 1-20, default 10.","title":"update-project-settings"},{"location":"reference/cli/#workflow-from-name","text":"Use a workflow from name. Usage: main project workflow-from-name [OPTIONS] Options: -n, --workflow-name TEXT Workflow name to use. [required]","title":"workflow-from-name"},{"location":"reference/cli/#validate-manifest","text":"Validate a block manifest. Usage: main validate-manifest [OPTIONS] MANIFEST_JSON","title":"validate-manifest"},{"location":"reference/cli/#workflow","text":"Add workflow tasks, run a job and more. Usage: main workflow [OPTIONS] COMMAND [ARGS]... Options: -wid, --workflow-id TEXT Your workflow ID, get it by creating a workflow or running 'up42 project get-workflows' [required]","title":"workflow"},{"location":"reference/cli/#add-workflow-tasks","text":"Adds or overwrites workflow tasks. - Name is arbitrary but best use the block name. Always use :1 to be able to identify the order when two times the same workflow task is used. - API by itself validates if the underlying block for the selected block-id is available. Usage: main workflow add-workflow-tasks [OPTIONS] INPUT_TASKS_JSON","title":"add-workflow-tasks"},{"location":"reference/cli/#delete","text":"Delete the workflow. Usage: main workflow delete [OPTIONS]","title":"delete"},{"location":"reference/cli/#get-compatible-blocks","text":"Get all compatible blocks for the current workflow. Usage: main workflow get-compatible-blocks [OPTIONS]","title":"get-compatible-blocks"},{"location":"reference/cli/#get-info_1","text":"Get information about the workflow. Usage: main workflow get-info [OPTIONS]","title":"get-info"},{"location":"reference/cli/#get-jobs","text":"Get the jobs ran with this workflow. Usage: main workflow get-jobs [OPTIONS]","title":"get-jobs"},{"location":"reference/cli/#get-parameters-info","text":"Get info about the parameters of each task in the workflow to make it easy to construct the desired parameters. Usage: main workflow get-parameters-info [OPTIONS]","title":"get-parameters-info"},{"location":"reference/cli/#get-workflow-tasks","text":"Get the workflow tasks list (DAG). Usage: main workflow get-workflow-tasks [OPTIONS] Options: --basic / --full Show basic or full task information.","title":"get-workflow-tasks"},{"location":"reference/cli/#run-job","text":"Creates and runs a new job. Usage: main workflow run-job [OPTIONS] INPUT_PARAMETERS_JSON Options: --track Track status of job in shell.","title":"run-job"},{"location":"reference/cli/#test-job","text":"Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Usage: main workflow test-job [OPTIONS] INPUT_PARAMETERS_JSON Options: --track Track status of job in shell.","title":"test-job"},{"location":"reference/cli/#update-name","text":"Update the workflow name. Usage: main workflow update-name [OPTIONS] Options: -n, --workflow-name TEXT New name for the workflow. [required] --description TEXT An optional description for the workflow.","title":"update-name"},{"location":"reference/job/","text":"Job class \u00b6 \u00b6 __init__ ( self , auth , project_id , job_id , order_ids = None ) special \u00b6 The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Source code in up42/job.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , auth : Auth , project_id : str , job_id : str , order_ids : List [ str ] = None , ): \"\"\" The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . quicklooks = None self . results = None if order_ids is None : self . order_ids = [ \"\" ] if self . auth . get_info : self . info = self . _get_info () cancel_job ( self ) \u00b6 Cancels a pending or running job. Source code in up42/job.py 119 120 121 122 123 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . auth . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . auth . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id ) download_quicklooks ( self , output_directory = None ) \u00b6 Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). Source code in up42/job.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). \"\"\" # Currently only the first/data task produces quicklooks. logger . setLevel ( logging . CRITICAL ) data_task = self . get_jobtasks ()[ 0 ] logger . setLevel ( logging . INFO ) out_paths : List [ str ] = data_task . download_quicklooks ( # type: ignore output_directory = output_directory ) # type: ignore self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths download_results ( self , output_directory = None ) \u00b6 Downloads and unpacks the job results. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/job.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def download_results ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads and unpacks the job results. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of job %s \" , self . job_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths get_jobtasks ( self , return_json = False ) \u00b6 Get the individual items of the job as JobTask objects or json. Parameters: Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns: Type Description Union[List[JobTask], List[Dict]] The job task objects in a list. Source code in up42/job.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def get_jobtasks ( self , return_json : bool = False ) -> Union [ List [ \"JobTask\" ], List [ Dict ]]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_json : List [ Dict ] = response_json [ \"data\" ] jobtasks = [ JobTask ( auth = self . auth , project_id = self . project_id , job_id = self . job_id , jobtask_id = task [ \"id\" ], ) for task in jobtasks_json ] if return_json : return jobtasks_json else : return jobtasks get_jobtasks_results_json ( self ) \u00b6 Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: Type Description Dict The data.json of alle single job tasks. Source code in up42/job.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 def get_jobtasks_results_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] jobtasks_results_json = {} for jobtask_id in jobtasks_ids : url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { jobtask_id } /outputs/data-json\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_results_json [ jobtask_id ] = response_json return jobtasks_results_json get_logs ( self , as_print = True , as_return = False ) \u00b6 Convenience function to print or return the logs of all job tasks. Parameters: Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns: Type Description The log strings (only if as_return was selected). Source code in up42/job.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def get_logs ( self , as_print : bool = True , as_return : bool = False ): \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( jobtasks_ids ), jobtasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( jobtasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , jobtask_id in enumerate ( jobtasks_ids ): url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { jobtask_id } /logs\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) job_logs [ jobtask_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with jobtask_id { jobtask_id } : \\n \" ) print ( response_json ) if as_return : return job_logs get_results_json ( self , as_dataframe = False ) \u00b6 Gets the Job results data.json. Parameters: Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. Source code in up42/job.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Job results data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json get_status ( self ) \u00b6 Gets the job status. Returns: Type Description str The job status. Source code in up42/job.py 66 67 68 69 70 71 72 73 74 75 76 77 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status map_results ( self , show_images = True , name_column = 'uid' ) \u00b6 Displays data.json, and if available, one or multiple results geotiffs. Parameters: Name Type Description Default show_images Shows images if True (default), only features if False. True name_column str Name of the column that provides the Feature/Layer name. 'uid' TODO: Make generic with scene_id column integrated. \u00b6 Source code in up42/job.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 def map_results ( self , show_images = True , name_column : str = \"uid\" ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs. Args: show_images: Shows images if True (default), only features if False. name_column: Name of the column that provides the Feature/Layer name. # TODO: Make generic with scene_id column integrated. \"\"\" if self . results is None : raise ValueError ( \"You first need to download the results via job.download_results()!\" ) def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } # Add feature to map. # TODO: Blocks that have results in separate json file. df : GeoDataFrame = self . get_results_json ( as_dataframe = True ) # type: ignore centroid = box ( * df . total_bounds ) . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) for idx , row in df . iterrows (): # type: ignore try : feature_name = row . loc [ name_column ] except KeyError : feature_name = \"\" layer_name = f \"Feature { idx + 1 } - { feature_name } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , style_function = _style_function , highlight_function = _highlight_function , ) folium . Popup ( f \" { layer_name } : { row . drop ( 'geometry' , axis = 0 ) . to_json () } \" ) . add_to ( f ) f . add_to ( m ) # Add image to map. plot_file_format = [ \".tif\" ] raster_filepaths = [ path for path in self . results if Path ( path ) . suffix in plot_file_format ] if show_images and raster_filepaths : try : feature_names = df [ name_column ] . to_list () except KeyError : feature_names = \"\" for idx , ( raster_fp , feature_name ) in enumerate ( zip ( raster_filepaths , feature_names ) ): # Folium requires 4326, streaming blocks are 3857 with rasterio . open ( raster_fp ) as src : with WarpedVRT ( src , crs = \"EPSG:4326\" ) as vrt : # TODO: Make band configuration available dst_array = vrt . read ()[: 3 , :, :] minx , miny , maxx , maxy = vrt . bounds m . add_child ( folium . raster_layers . ImageOverlay ( np . moveaxis ( np . stack ( dst_array ), 0 , 2 ), bounds = [[ miny , minx ], [ maxy , maxx ]], # different order. name = f \"Image { idx + 1 } - { feature_name } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement #type: ignore collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m track_status ( self , report_time = 30 ) \u00b6 Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Parameters: Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 Source code in up42/job.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def track_status ( self , report_time : int = 30 ) -> str : \"\"\" Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" logger . info ( \"Tracking job status continuously, reporting every %s seconds...\" , report_time , ) status = \"NOT STARTED\" time_asleep = 0 while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: if time_asleep != 0 and time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) elif status in [ \"FAILED\" , \"ERROR\" ]: logger . info ( \"Job is %s ! - %s - Printing logs ...\" , status , self . job_id ) self . get_logs ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) elif status in [ \"CANCELLED\" , \"CANCELLING\" ]: logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) raise ValueError ( \"Job has been cancelled!\" ) elif status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully! - %s \" , self . job_id ) sleep ( 5 ) time_asleep += 5 return status upload_results_to_bucket ( self , gs_client , bucket , folder , extension = '.tgz' , version = 'v0' ) \u00b6 Uploads the results to a custom google cloud storage bucket. Source code in up42/job.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def upload_results_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the results to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s results with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s results to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" )","title":"Job"},{"location":"reference/job/#job-class","text":"","title":"Job class"},{"location":"reference/job/#up42.job.Job","text":"","title":"up42.job.Job"},{"location":"reference/job/#up42.job.Job.__init__","text":"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Source code in up42/job.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , auth : Auth , project_id : str , job_id : str , order_ids : List [ str ] = None , ): \"\"\" The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . quicklooks = None self . results = None if order_ids is None : self . order_ids = [ \"\" ] if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/job/#up42.job.Job.cancel_job","text":"Cancels a pending or running job. Source code in up42/job.py 119 120 121 122 123 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . auth . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . auth . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id )","title":"cancel_job()"},{"location":"reference/job/#up42.job.Job.download_quicklooks","text":"Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). Source code in up42/job.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). \"\"\" # Currently only the first/data task produces quicklooks. logger . setLevel ( logging . CRITICAL ) data_task = self . get_jobtasks ()[ 0 ] logger . setLevel ( logging . INFO ) out_paths : List [ str ] = data_task . download_quicklooks ( # type: ignore output_directory = output_directory ) # type: ignore self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/job/#up42.job.Job.download_results","text":"Downloads and unpacks the job results. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/job.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def download_results ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads and unpacks the job results. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of job %s \" , self . job_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths","title":"download_results()"},{"location":"reference/job/#up42.job.Job.get_jobtasks","text":"Get the individual items of the job as JobTask objects or json. Parameters: Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns: Type Description Union[List[JobTask], List[Dict]] The job task objects in a list. Source code in up42/job.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def get_jobtasks ( self , return_json : bool = False ) -> Union [ List [ \"JobTask\" ], List [ Dict ]]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_json : List [ Dict ] = response_json [ \"data\" ] jobtasks = [ JobTask ( auth = self . auth , project_id = self . project_id , job_id = self . job_id , jobtask_id = task [ \"id\" ], ) for task in jobtasks_json ] if return_json : return jobtasks_json else : return jobtasks","title":"get_jobtasks()"},{"location":"reference/job/#up42.job.Job.get_jobtasks_results_json","text":"Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: Type Description Dict The data.json of alle single job tasks. Source code in up42/job.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 def get_jobtasks_results_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] jobtasks_results_json = {} for jobtask_id in jobtasks_ids : url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { jobtask_id } /outputs/data-json\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_results_json [ jobtask_id ] = response_json return jobtasks_results_json","title":"get_jobtasks_results_json()"},{"location":"reference/job/#up42.job.Job.get_logs","text":"Convenience function to print or return the logs of all job tasks. Parameters: Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns: Type Description The log strings (only if as_return was selected). Source code in up42/job.py 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def get_logs ( self , as_print : bool = True , as_return : bool = False ): \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( jobtasks_ids ), jobtasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( jobtasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , jobtask_id in enumerate ( jobtasks_ids ): url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { jobtask_id } /logs\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) job_logs [ jobtask_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with jobtask_id { jobtask_id } : \\n \" ) print ( response_json ) if as_return : return job_logs","title":"get_logs()"},{"location":"reference/job/#up42.job.Job.get_results_json","text":"Gets the Job results data.json. Parameters: Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. Source code in up42/job.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Job results data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"get_results_json()"},{"location":"reference/job/#up42.job.Job.get_status","text":"Gets the job status. Returns: Type Description str The job status. Source code in up42/job.py 66 67 68 69 70 71 72 73 74 75 76 77 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status","title":"get_status()"},{"location":"reference/job/#up42.job.Job.map_results","text":"Displays data.json, and if available, one or multiple results geotiffs. Parameters: Name Type Description Default show_images Shows images if True (default), only features if False. True name_column str Name of the column that provides the Feature/Layer name. 'uid'","title":"map_results()"},{"location":"reference/job/#todo-make-generic-with-scene_id-column-integrated","text":"Source code in up42/job.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 def map_results ( self , show_images = True , name_column : str = \"uid\" ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs. Args: show_images: Shows images if True (default), only features if False. name_column: Name of the column that provides the Feature/Layer name. # TODO: Make generic with scene_id column integrated. \"\"\" if self . results is None : raise ValueError ( \"You first need to download the results via job.download_results()!\" ) def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } # Add feature to map. # TODO: Blocks that have results in separate json file. df : GeoDataFrame = self . get_results_json ( as_dataframe = True ) # type: ignore centroid = box ( * df . total_bounds ) . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) for idx , row in df . iterrows (): # type: ignore try : feature_name = row . loc [ name_column ] except KeyError : feature_name = \"\" layer_name = f \"Feature { idx + 1 } - { feature_name } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , style_function = _style_function , highlight_function = _highlight_function , ) folium . Popup ( f \" { layer_name } : { row . drop ( 'geometry' , axis = 0 ) . to_json () } \" ) . add_to ( f ) f . add_to ( m ) # Add image to map. plot_file_format = [ \".tif\" ] raster_filepaths = [ path for path in self . results if Path ( path ) . suffix in plot_file_format ] if show_images and raster_filepaths : try : feature_names = df [ name_column ] . to_list () except KeyError : feature_names = \"\" for idx , ( raster_fp , feature_name ) in enumerate ( zip ( raster_filepaths , feature_names ) ): # Folium requires 4326, streaming blocks are 3857 with rasterio . open ( raster_fp ) as src : with WarpedVRT ( src , crs = \"EPSG:4326\" ) as vrt : # TODO: Make band configuration available dst_array = vrt . read ()[: 3 , :, :] minx , miny , maxx , maxy = vrt . bounds m . add_child ( folium . raster_layers . ImageOverlay ( np . moveaxis ( np . stack ( dst_array ), 0 , 2 ), bounds = [[ miny , minx ], [ maxy , maxx ]], # different order. name = f \"Image { idx + 1 } - { feature_name } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement #type: ignore collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m","title":"TODO: Make generic with scene_id column integrated."},{"location":"reference/job/#up42.job.Job.track_status","text":"Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Parameters: Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 Source code in up42/job.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def track_status ( self , report_time : int = 30 ) -> str : \"\"\" Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" logger . info ( \"Tracking job status continuously, reporting every %s seconds...\" , report_time , ) status = \"NOT STARTED\" time_asleep = 0 while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: if time_asleep != 0 and time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) elif status in [ \"FAILED\" , \"ERROR\" ]: logger . info ( \"Job is %s ! - %s - Printing logs ...\" , status , self . job_id ) self . get_logs ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) elif status in [ \"CANCELLED\" , \"CANCELLING\" ]: logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) raise ValueError ( \"Job has been cancelled!\" ) elif status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully! - %s \" , self . job_id ) sleep ( 5 ) time_asleep += 5 return status","title":"track_status()"},{"location":"reference/job/#up42.job.Job.upload_results_to_bucket","text":"Uploads the results to a custom google cloud storage bucket. Source code in up42/job.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def upload_results_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the results to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s results with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s results to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" )","title":"upload_results_to_bucket()"},{"location":"reference/jobtask/","text":"JobTask class \u00b6 \u00b6 __init__ ( self , auth , project_id , job_id , jobtask_id ) special \u00b6 The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Source code in up42/jobtask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , auth : Auth , project_id : str , job_id : str , jobtask_id : str , ): \"\"\" The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . jobtask_id = jobtask_id self . quicklooks = None self . results = None if self . auth . get_info : self . info = self . _get_info () download_quicklooks ( self , output_directory = None ) \u00b6 Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] The quicklooks filepaths. Source code in up42/jobtask.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Args: output_directory: The file output directory, defaults to the current working directory. Returns: The quicklooks filepaths. \"\"\" if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) quicklooks_ids = response_json [ \"data\" ] out_paths : List [ str ] = [] for ql_id in tqdm ( quicklooks_ids ): out_path = output_directory / f \"quicklook_ { ql_id } \" # No suffix required. out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/ { ql_id } \" ) response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths download_results ( self , output_directory = None ) \u00b6 Downloads and unpacks the jobtask results. Default download to Desktop. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/jobtask.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def download_results ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Downloads and unpacks the jobtask results. Default download to Desktop. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of jobtask %s \" , self . jobtask_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths get_results_json ( self , as_dataframe = False ) \u00b6 Gets the Jobtask results data.json. Parameters: Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe. Source code in up42/jobtask.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Jobtask results data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . auth . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"JobTask"},{"location":"reference/jobtask/#jobtask-class","text":"","title":"JobTask class"},{"location":"reference/jobtask/#up42.jobtask.JobTask","text":"","title":"up42.jobtask.JobTask"},{"location":"reference/jobtask/#up42.jobtask.JobTask.__init__","text":"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Source code in up42/jobtask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , auth : Auth , project_id : str , job_id : str , jobtask_id : str , ): \"\"\" The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . jobtask_id = jobtask_id self . quicklooks = None self . results = None if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_quicklooks","text":"Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] The quicklooks filepaths. Source code in up42/jobtask.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Args: output_directory: The file output directory, defaults to the current working directory. Returns: The quicklooks filepaths. \"\"\" if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) quicklooks_ids = response_json [ \"data\" ] out_paths : List [ str ] = [] for ql_id in tqdm ( quicklooks_ids ): out_path = output_directory / f \"quicklook_ { ql_id } \" # No suffix required. out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/ { ql_id } \" ) response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_results","text":"Downloads and unpacks the jobtask results. Default download to Desktop. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/jobtask.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def download_results ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Downloads and unpacks the jobtask results. Default download to Desktop. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of jobtask %s \" , self . jobtask_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths","title":"download_results()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.get_results_json","text":"Gets the Jobtask results data.json. Parameters: Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe. Source code in up42/jobtask.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Jobtask results data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . auth . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"get_results_json()"},{"location":"reference/project/","text":"Project class \u00b6 \u00b6 __init__ ( self , auth , project_id ) special \u00b6 The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Source code in up42/project.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , auth : Auth , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. \"\"\" self . auth = auth self . project_id = project_id if self . auth . get_info : self . info = self . _get_info () create_workflow ( self , name , description = '' , use_existing = False ) \u00b6 Creates a new workflow and returns a workflow object. Parameters: Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns: Type Description Workflow The workflow object. Source code in up42/project.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . info ( \"Getting existing workflows in project ...\" ) logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = Workflow ( self . auth , project_id = self . project_id , workflow_id = workflow_id ) return workflow get_jobs ( self , return_json = False ) \u00b6 Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/project.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs in project %s .\" , len ( jobs_json ), self . project_id , ) if return_json : return jobs_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_json ) ] return jobs get_project_settings ( self ) \u00b6 Gets the project settings. Returns: Type Description List The project settings. Source code in up42/project.py 137 138 139 140 141 142 143 144 145 146 147 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings get_workflows ( self , return_json = False ) \u00b6 Gets all workflows in a project as workflow objects or json. Parameters: Name Type Description Default return_json bool True returns Workflow Objects. False Returns: Type Description Union[List[Workflow], Dict] Workflow objects in the project or alternatively json info of the workflows. Source code in up42/project.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ Workflow ( self . auth , project_id = self . project_id , workflow_id = work [ \"id\" ]) for work in tqdm ( workflows_json ) ] return workflows update_project_settings ( self , max_aoi_size = None , max_concurrent_jobs = None , number_of_images = None ) \u00b6 Updates a project's settings. Parameters: Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images The maximum number of images returned with each job, from 1-20, default 10. None Source code in up42/project.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: The maximum number of images returned with each job, from 1-20, default 10. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . auth . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload )","title":"Project"},{"location":"reference/project/#project-class","text":"","title":"Project class"},{"location":"reference/project/#up42.project.Project","text":"","title":"up42.project.Project"},{"location":"reference/project/#up42.project.Project.__init__","text":"The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Source code in up42/project.py 16 17 18 19 20 21 22 23 24 def __init__ ( self , auth : Auth , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. \"\"\" self . auth = auth self . project_id = project_id if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/project/#up42.project.Project.create_workflow","text":"Creates a new workflow and returns a workflow object. Parameters: Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns: Type Description Workflow The workflow object. Source code in up42/project.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . info ( \"Getting existing workflows in project ...\" ) logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = Workflow ( self . auth , project_id = self . project_id , workflow_id = workflow_id ) return workflow","title":"create_workflow()"},{"location":"reference/project/#up42.project.Project.get_jobs","text":"Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/project.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs in project %s .\" , len ( jobs_json ), self . project_id , ) if return_json : return jobs_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_json ) ] return jobs","title":"get_jobs()"},{"location":"reference/project/#up42.project.Project.get_project_settings","text":"Gets the project settings. Returns: Type Description List The project settings. Source code in up42/project.py 137 138 139 140 141 142 143 144 145 146 147 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings","title":"get_project_settings()"},{"location":"reference/project/#up42.project.Project.get_workflows","text":"Gets all workflows in a project as workflow objects or json. Parameters: Name Type Description Default return_json bool True returns Workflow Objects. False Returns: Type Description Union[List[Workflow], Dict] Workflow objects in the project or alternatively json info of the workflows. Source code in up42/project.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ Workflow ( self . auth , project_id = self . project_id , workflow_id = work [ \"id\" ]) for work in tqdm ( workflows_json ) ] return workflows","title":"get_workflows()"},{"location":"reference/project/#up42.project.Project.update_project_settings","text":"Updates a project's settings. Parameters: Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images The maximum number of images returned with each job, from 1-20, default 10. None Source code in up42/project.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: The maximum number of images returned with each job, from 1-20, default 10. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . auth . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload )","title":"update_project_settings()"},{"location":"reference/tools/","text":"Tools class \u00b6 \u00b6 __init__ ( self , auth = None ) special \u00b6 The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Source code in up42/tools.py 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , auth = None ): \"\"\" The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. \"\"\" if auth : self . auth = auth self . quicklooks = None self . results = None draw_aoi ( self ) \u00b6 Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). Source code in up42/tools.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def draw_aoi ( self ): \"\"\" Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). \"\"\" m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m get_block_details ( self , block_id , as_dataframe = False ) \u00b6 Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Parameters: Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Dict A dict of the block details metadata for the specific block. Source code in up42/tools.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . auth . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json get_blocks ( self , block_type = None , basic = True , as_dataframe = False ) \u00b6 Gets a list of all public blocks on the marketplace. Parameters: Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version {block_id : block_name} True as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Union[List[Dict], Dict] A list of the public blocks and their metadata. Optional a simpler version dict. Source code in up42/tools.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ List [ Dict ], Dict ]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting blocks name and id, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json get_example_aoi ( self , location = 'Berlin' , as_dataframe = False ) \u00b6 Gets predefined, small, rectangular example aoi for the selected location. Parameters: Name Type Description Default location str Location, one of Berlin, Washington. 'Berlin' as_dataframe bool Returns a dataframe instead of dict FeatureColletions (default). False Returns: Type Description Union[dict, geopandas.geodataframe.GeoDataFrame] Feature collection json with the selected aoi. Source code in up42/tools.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> Union [ dict , GeoDataFrame ]: \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washington. as_dataframe: Returns a dataframe instead of dict FeatureColletions (default). Returns: Feature collection json with the selected aoi. \"\"\" logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_washington.geojson\" ) else : raise ValueError ( \"Please select one of 'Berlin' or 'Washington' as the \" \"location!\" ) if as_dataframe : df = GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi plot_coverage ( scenes , aoi = None , legend_column = 'scene_id' , figsize = ( 12 , 16 )) staticmethod \u00b6 Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Parameters: Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, results of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. 'scene_id' figsize Matplotlib figure size. (12, 16) Source code in up42/tools.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @staticmethod def plot_coverage ( scenes : GeoDataFrame , aoi : GeoDataFrame = None , legend_column : str = \"scene_id\" , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Args: scenes: GeoDataFrame of scenes, results of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. figsize: Matplotlib figure size. \"\"\" if legend_column not in scenes . columns : legend_column = None # type: ignore logger . info ( \"Given legend_column name not in scene dataframe, \" \"plotting without legend.\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show () plot_quicklooks ( self , figsize = ( 8 , 8 ), filepaths = None , titles = None ) \u00b6 Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] List of titles for the subplots, optional. None Source code in up42/tools.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def plot_quicklooks ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: List of titles for the subplots, optional. \"\"\" if filepaths is None : if self . quicklooks is None : raise ValueError ( \"You first need to download the quicklooks!\" ) filepaths = self . quicklooks plot_file_format = [ \".jpg\" , \".jpeg\" , \".png\" ] warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , ) plot_results ( self , figsize = ( 8 , 8 ), filepaths = None , titles = None ) \u00b6 Plots the downloaded results data. Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[Union[str, pathlib.Path]] Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] Optional list of titles for the subplots. None Source code in up42/tools.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def plot_results ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ Union [ str , Path ]] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded results data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: Optional list of titles for the subplots. \"\"\" if filepaths is None : if self . results is None : raise ValueError ( \"You first need to download the results!\" ) filepaths = self . results plot_file_format = [ \".tif\" ] # TODO: Add other fileformats. _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , ) read_vector_file ( self , filename = 'aoi.geojson' , as_dataframe = False ) \u00b6 Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters: Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Feature Collection Source code in up42/tools.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs . to_string () != \"EPSG:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__ validate_manifest ( self , path_or_json ) \u00b6 Validates the block manifest, input either manifest json string or filepath. Parameters: Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns: Type Description Dict A dictionary with the validation results and potential validation errors. Examples: { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } } Source code in up42/tools.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation results and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /validate-schema/block\" response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ]","title":"Tools"},{"location":"reference/tools/#tools-class","text":"","title":"Tools class"},{"location":"reference/tools/#up42.tools.Tools","text":"","title":"up42.tools.Tools"},{"location":"reference/tools/#up42.tools.Tools.__init__","text":"The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Source code in up42/tools.py 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , auth = None ): \"\"\" The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. \"\"\" if auth : self . auth = auth self . quicklooks = None self . results = None","title":"__init__()"},{"location":"reference/tools/#up42.tools.Tools.draw_aoi","text":"Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). Source code in up42/tools.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def draw_aoi ( self ): \"\"\" Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). \"\"\" m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m","title":"draw_aoi()"},{"location":"reference/tools/#up42.tools.Tools.get_block_details","text":"Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Parameters: Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Dict A dict of the block details metadata for the specific block. Source code in up42/tools.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . auth . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json","title":"get_block_details()"},{"location":"reference/tools/#up42.tools.Tools.get_blocks","text":"Gets a list of all public blocks on the marketplace. Parameters: Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version {block_id : block_name} True as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Union[List[Dict], Dict] A list of the public blocks and their metadata. Optional a simpler version dict. Source code in up42/tools.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ List [ Dict ], Dict ]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting blocks name and id, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json","title":"get_blocks()"},{"location":"reference/tools/#up42.tools.Tools.get_example_aoi","text":"Gets predefined, small, rectangular example aoi for the selected location. Parameters: Name Type Description Default location str Location, one of Berlin, Washington. 'Berlin' as_dataframe bool Returns a dataframe instead of dict FeatureColletions (default). False Returns: Type Description Union[dict, geopandas.geodataframe.GeoDataFrame] Feature collection json with the selected aoi. Source code in up42/tools.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> Union [ dict , GeoDataFrame ]: \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washington. as_dataframe: Returns a dataframe instead of dict FeatureColletions (default). Returns: Feature collection json with the selected aoi. \"\"\" logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_washington.geojson\" ) else : raise ValueError ( \"Please select one of 'Berlin' or 'Washington' as the \" \"location!\" ) if as_dataframe : df = GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi","title":"get_example_aoi()"},{"location":"reference/tools/#up42.tools.Tools.plot_coverage","text":"Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Parameters: Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, results of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. 'scene_id' figsize Matplotlib figure size. (12, 16) Source code in up42/tools.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 @staticmethod def plot_coverage ( scenes : GeoDataFrame , aoi : GeoDataFrame = None , legend_column : str = \"scene_id\" , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Args: scenes: GeoDataFrame of scenes, results of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. figsize: Matplotlib figure size. \"\"\" if legend_column not in scenes . columns : legend_column = None # type: ignore logger . info ( \"Given legend_column name not in scene dataframe, \" \"plotting without legend.\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show ()","title":"plot_coverage()"},{"location":"reference/tools/#up42.tools.Tools.plot_quicklooks","text":"Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] List of titles for the subplots, optional. None Source code in up42/tools.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def plot_quicklooks ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: List of titles for the subplots, optional. \"\"\" if filepaths is None : if self . quicklooks is None : raise ValueError ( \"You first need to download the quicklooks!\" ) filepaths = self . quicklooks plot_file_format = [ \".jpg\" , \".jpeg\" , \".png\" ] warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , )","title":"plot_quicklooks()"},{"location":"reference/tools/#up42.tools.Tools.plot_results","text":"Plots the downloaded results data. Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[Union[str, pathlib.Path]] Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] Optional list of titles for the subplots. None Source code in up42/tools.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def plot_results ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ Union [ str , Path ]] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded results data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: Optional list of titles for the subplots. \"\"\" if filepaths is None : if self . results is None : raise ValueError ( \"You first need to download the results!\" ) filepaths = self . results plot_file_format = [ \".tif\" ] # TODO: Add other fileformats. _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , )","title":"plot_results()"},{"location":"reference/tools/#up42.tools.Tools.read_vector_file","text":"Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters: Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Feature Collection Source code in up42/tools.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs . to_string () != \"EPSG:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__","title":"read_vector_file()"},{"location":"reference/tools/#up42.tools.Tools.validate_manifest","text":"Validates the block manifest, input either manifest json string or filepath. Parameters: Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns: Type Description Dict A dictionary with the validation results and potential validation errors. Examples: { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } } Source code in up42/tools.py 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation results and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /validate-schema/block\" response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ]","title":"validate_manifest()"},{"location":"reference/workflow/","text":"Workflow class \u00b6 \u00b6 __init__ ( self , auth , project_id , workflow_id ) special \u00b6 The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Source code in up42/workflow.py 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , auth : Auth , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. \"\"\" self . auth = auth self . project_id = project_id self . workflow_id = workflow_id if self . auth . get_info : self . info = self . _get_info () add_workflow_tasks ( self , input_tasks ) \u00b6 Adds or overwrites workflow tasks in a workflow on UP42. Parameters: Name Type Description Default input_tasks Union[List[str], List[Dict]] The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the marketplace . required Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Examples: input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] input_tasks = [ \"sobloo-s2-l1c-aoiclipped\" , \"tiling\" ] ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Examples: input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] Source code in up42/workflow.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def add_workflow_tasks ( self , input_tasks : Union [ List [ str ], List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks in a workflow on UP42. Args: input_tasks: The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the [marketplace](https://marketplace.up42.com). !!! Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` ```python input_tasks = [\"sobloo-s2-l1c-aoiclipped\", \"tiling\"] ``` ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] Optional: The input_tasks can also be provided as the full, detailed workflow task definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . auth . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks ) construct_parameters ( self , geometry = None , geometry_operation = None , handle_multiple_features = 'footprint' , start_date = None , end_date = None , limit = None , scene_ids = None , order_ids = None ) \u00b6 Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters: Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, geojson.geometry.Polygon, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon, shapely.geometry.point.Point]] One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns: Type Description Dict Dictionary of constructed input parameters. Source code in up42/workflow.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def construct_parameters ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , geojson_Polygon , List , GeoDataFrame , Polygon , Point , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In case of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , # type: ignore squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters delete ( self ) \u00b6 Deletes the workflow and sets the Python object to None. Source code in up42/workflow.py 499 500 501 502 503 504 505 506 507 508 509 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self get_compatible_blocks ( self ) \u00b6 Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. Source code in up42/workflow.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True ) . keys ())[ - 1 ] # type: ignore url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. compatible_blocks = { block [ \"name\" ]: block [ \"blockId\" ] for block in compatible_blocks } return compatible_blocks get_jobs ( self , return_json = False ) \u00b6 Get all jobs associated with the workflow as job objects or json. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/workflow.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs associated with the workflow as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] jobs_workflow_json = [ j for j in jobs_json if j [ \"workflowId\" ] == self . workflow_id ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_workflow_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_workflow_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_workflow_json ) ] return jobs get_parameters_info ( self ) \u00b6 Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Type Description Dict Workflow parameters info json. Source code in up42/workflow.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def get_parameters_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameters info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info get_workflow_tasks ( self , basic = False ) \u00b6 Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters: Name Type Description Default basic bool If selected returns a simplified task-name : task-id` version. False Returns: Type Description Union[List, Dict] The workflow task info. Source code in up42/workflow.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return { task [ \"name\" ]: task [ \"id\" ] for task in tasks } else : return tasks run_job ( self , input_parameters = None , track_status = False , name = None ) \u00b6 Creates and runs a new job. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned job object. Source code in up42/workflow.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , track_status = track_status , name = name ) test_job ( self , input_parameters = None , track_status = False , name = None ) \u00b6 Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned test job object. Source code in up42/workflow.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def test_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned test job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , test_job = True , track_status = track_status , name = name , ) update_name ( self , name = None , description = None ) \u00b6 Updates the workflow name and description. Parameters: Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None Source code in up42/workflow.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update )","title":"Workflow"},{"location":"reference/workflow/#workflow-class","text":"","title":"Workflow class"},{"location":"reference/workflow/#up42.workflow.Workflow","text":"","title":"up42.workflow.Workflow"},{"location":"reference/workflow/#up42.workflow.Workflow.__init__","text":"The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Source code in up42/workflow.py 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , auth : Auth , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. \"\"\" self . auth = auth self . project_id = project_id self . workflow_id = workflow_id if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/workflow/#up42.workflow.Workflow.add_workflow_tasks","text":"Adds or overwrites workflow tasks in a workflow on UP42. Parameters: Name Type Description Default input_tasks Union[List[str], List[Dict]] The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the marketplace . required Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Examples: input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] input_tasks = [ \"sobloo-s2-l1c-aoiclipped\" , \"tiling\" ] ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Examples: input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] Source code in up42/workflow.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def add_workflow_tasks ( self , input_tasks : Union [ List [ str ], List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks in a workflow on UP42. Args: input_tasks: The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the [marketplace](https://marketplace.up42.com). !!! Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` ```python input_tasks = [\"sobloo-s2-l1c-aoiclipped\", \"tiling\"] ``` ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] Optional: The input_tasks can also be provided as the full, detailed workflow task definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . auth . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks )","title":"add_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.construct_parameters","text":"Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters: Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, geojson.geometry.Polygon, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon, shapely.geometry.point.Point]] One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns: Type Description Dict Dictionary of constructed input parameters. Source code in up42/workflow.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def construct_parameters ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , geojson_Polygon , List , GeoDataFrame , Polygon , Point , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In case of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , # type: ignore squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters","title":"construct_parameters()"},{"location":"reference/workflow/#up42.workflow.Workflow.delete","text":"Deletes the workflow and sets the Python object to None. Source code in up42/workflow.py 499 500 501 502 503 504 505 506 507 508 509 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self","title":"delete()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_compatible_blocks","text":"Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. Source code in up42/workflow.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True ) . keys ())[ - 1 ] # type: ignore url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. compatible_blocks = { block [ \"name\" ]: block [ \"blockId\" ] for block in compatible_blocks } return compatible_blocks","title":"get_compatible_blocks()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_jobs","text":"Get all jobs associated with the workflow as job objects or json. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/workflow.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs associated with the workflow as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] jobs_workflow_json = [ j for j in jobs_json if j [ \"workflowId\" ] == self . workflow_id ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_workflow_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_workflow_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_workflow_json ) ] return jobs","title":"get_jobs()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_parameters_info","text":"Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Type Description Dict Workflow parameters info json. Source code in up42/workflow.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def get_parameters_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameters info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info","title":"get_parameters_info()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_workflow_tasks","text":"Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters: Name Type Description Default basic bool If selected returns a simplified task-name : task-id` version. False Returns: Type Description Union[List, Dict] The workflow task info. Source code in up42/workflow.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return { task [ \"name\" ]: task [ \"id\" ] for task in tasks } else : return tasks","title":"get_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.run_job","text":"Creates and runs a new job. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned job object. Source code in up42/workflow.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , track_status = track_status , name = name )","title":"run_job()"},{"location":"reference/workflow/#up42.workflow.Workflow.test_job","text":"Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned test job object. Source code in up42/workflow.py 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def test_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned test job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , test_job = True , track_status = track_status , name = name , )","title":"test_job()"},{"location":"reference/workflow/#up42.workflow.Workflow.update_name","text":"Updates the workflow name and description. Parameters: Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None Source code in up42/workflow.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update )","title":"update_name()"}]}