{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"30-second-example/","text":"30 Second Example \u00b6 A new workflow is created and filled with tasks ( Sentinel-2 data , Image Sharpening ). The area of interest and workflow parameters are defined. After running the job, the results are downloaded and visualized. import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project # Add blocks/tasks to the workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) print ( up42 . get_blocks ( basic = True )) input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow to run it. aoi = workflow . get_example_aoi ( as_dataframe = True ) #aoi = workflow.read_vector_file(\"data/aoi_berlin.geojson\", as_dataframe=True) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , limit = 1 ) input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters # Run a test job to query data availability and check the configuration. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual job. job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) job . download_results () job . plot_results ()","title":"30 Second Example"},{"location":"30-second-example/#30-second-example","text":"A new workflow is created and filled with tasks ( Sentinel-2 data , Image Sharpening ). The area of interest and workflow parameters are defined. After running the job, the results are downloaded and visualized. import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) # up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project # Add blocks/tasks to the workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) print ( up42 . get_blocks ( basic = True )) input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Define the aoi and input parameters of the workflow to run it. aoi = workflow . get_example_aoi ( as_dataframe = True ) #aoi = workflow.read_vector_file(\"data/aoi_berlin.geojson\", as_dataframe=True) input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , limit = 1 ) input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters # Run a test job to query data availability and check the configuration. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual job. job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) job . download_results () job . plot_results ()","title":"30 Second Example"},{"location":"authentication/","text":"Authentication \u00b6 In order to use the UP42 Python SDK functionality you need to first authenticate with the UP42 servers via your project credentials. Get your Project credentials Log in to UP42.com and create a new project or select an existing one. In the project's \"Developer\" section you can find the project_id and project_api_key . As arguments \u00b6 Authenticate by passing the project credentials directly as arguments : import up42 up42 . authenticate ( project_id = 123 , project_api_key = 456 ) Use a configuration file \u00b6 Alternatively, create a configuration json file and pass its file path: { \"project_id\" : \"...\" , \"project_api_key\" : \"...\" } import up42 up42 . authenticate ( cfg_file = \"config.json\" )","title":"Authentication"},{"location":"authentication/#authentication","text":"In order to use the UP42 Python SDK functionality you need to first authenticate with the UP42 servers via your project credentials. Get your Project credentials Log in to UP42.com and create a new project or select an existing one. In the project's \"Developer\" section you can find the project_id and project_api_key .","title":"Authentication"},{"location":"authentication/#as-arguments","text":"Authenticate by passing the project credentials directly as arguments : import up42 up42 . authenticate ( project_id = 123 , project_api_key = 456 )","title":"As arguments"},{"location":"authentication/#use-a-configuration-file","text":"Alternatively, create a configuration json file and pass its file path: { \"project_id\" : \"...\" , \"project_api_key\" : \"...\" } import up42 up42 . authenticate ( cfg_file = \"config.json\" )","title":"Use a configuration file"},{"location":"catalog/","text":"Catalog Search \u00b6 import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) up42 . authenticate ( cfg_file = \"config.json\" ) catalog = up42 . initialize_catalog () catalog Search scenes in aoi \u00b6 #aoi = up42.read_vector_file(\"data/aoi_washington.geojson\", # as_dataframe=False) aoi = up42 . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) aoi search_paramaters = catalog . construct_parameters ( geometry = aoi , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 5 ) search_results = catalog . search ( search_paramaters = search_paramaters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" ) Quicklooks \u00b6 catalog . download_quicklooks ( image_ids = search_results . id . to_list (), sensor = \"pleiades\" ) catalog . plot_quicklooks ( figsize = ( 20 , 20 ))","title":"Catalog Search"},{"location":"catalog/#catalog-search","text":"import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) up42 . authenticate ( cfg_file = \"config.json\" ) catalog = up42 . initialize_catalog () catalog","title":"Catalog Search"},{"location":"catalog/#search-scenes-in-aoi","text":"#aoi = up42.read_vector_file(\"data/aoi_washington.geojson\", # as_dataframe=False) aoi = up42 . get_example_aoi ( location = \"Berlin\" , as_dataframe = True ) aoi search_paramaters = catalog . construct_parameters ( geometry = aoi , start_date = \"2018-01-01\" , end_date = \"2020-12-31\" , sensors = [ \"pleiades\" ], max_cloudcover = 20 , sortby = \"cloudCoverage\" , limit = 5 ) search_results = catalog . search ( search_paramaters = search_paramaters ) display ( search_results . head ()) catalog . plot_coverage ( scenes = search_results , aoi = aoi , legend_column = \"scene_id\" )","title":"Search scenes in aoi"},{"location":"catalog/#quicklooks","text":"catalog . download_quicklooks ( image_ids = search_results . id . to_list (), sensor = \"pleiades\" ) catalog . plot_quicklooks ( figsize = ( 20 , 20 ))","title":"Quicklooks"},{"location":"cli/","text":"Command Line Interface (CLI) \u00b6 The CLI tool allows you to use the UP42 functionality from the command line. It is installed automatically with and based on the Python SDK. To check whether the tool is installed and functioning correctly, type the following on your terminal or command line. This will print out a summary of the available commands. up42 -h To get help on a specific command, use: up42 command -h Authenticate \u00b6 You can authenticate with a PROJECT_ID and PROJECT_API_KEY up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] auth Or using a config.json file: up42 -cfg [ path to config.json ] auth You can make the authentication persistent by storing either the project key pair or the path to the config file as an environment variable. export UP42_PROJECT_ID =[ PROJECT_ID ] export UP42_PROJECT_API_KEY =[ PROJECT_API_KEY ] Or when using config.json . export UP42_CFG_FILE =[ path to config.json ] To save the authentication for future sessions make sure to append these variables to your bash profile file: # Linux export UP42_CFG_FILE =[ path to config.json ] >> ~/.bashrc # MacOS export UP42_CFG_FILE =[ path to config.json ] >> ~/.bash_profile If you want to create a config.json file from a project key pair, you can use the config command. up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] config Workflows \u00b6 up42 workflow -h Create a new workflow: up42 project create-workflow a_test Or check which workflows already exist: up42 project get-workflows And get a workflow by its name: up42 project workflow-from-name -name a_test After running the command to persist the workflow you can get the workflow tasks: up42 workflow get-workflow-tasks You can also add workflow tasks to the workflow via a json file (see typical usage for an example): up42 workflow add-workflow-tasks new_workflow_tasks.json Jobs \u00b6 up42 job -h First create and run a new test job with parameters defined in a json file (see typical usage for an example): up42 workflow test-job input_parameters.json --track Then run the actual job with parameters: up42 workflow run-job input_parameters.json --track After running the command to persist the job you can download the quicklooks from job in current working directory (note that not all data blocks support quicklooks): up42 job download-quicklooks . Or download and unpack the results: up42 job download-results . You can also print out the logs of the job: up42 job get-log Catalog \u00b6 up42 catalog -h With the catalog commands you can easily search the UP42 catalog for data. First, create a parameter configuration: up42 catalog construct-parameters example_aoi.geojson --sensors pleiades --max-cloud-cover 5 Then get the results: up42 catalog search example_search_params.json General tools \u00b6 Get all public blocks in the platform: up42 get-blocks Get block details by name: up42 get-block-details -name oneatlas-pleiades-aoiclipped","title":"Command Line Interface"},{"location":"cli/#command-line-interface-cli","text":"The CLI tool allows you to use the UP42 functionality from the command line. It is installed automatically with and based on the Python SDK. To check whether the tool is installed and functioning correctly, type the following on your terminal or command line. This will print out a summary of the available commands. up42 -h To get help on a specific command, use: up42 command -h","title":"Command Line Interface (CLI)"},{"location":"cli/#authenticate","text":"You can authenticate with a PROJECT_ID and PROJECT_API_KEY up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] auth Or using a config.json file: up42 -cfg [ path to config.json ] auth You can make the authentication persistent by storing either the project key pair or the path to the config file as an environment variable. export UP42_PROJECT_ID =[ PROJECT_ID ] export UP42_PROJECT_API_KEY =[ PROJECT_API_KEY ] Or when using config.json . export UP42_CFG_FILE =[ path to config.json ] To save the authentication for future sessions make sure to append these variables to your bash profile file: # Linux export UP42_CFG_FILE =[ path to config.json ] >> ~/.bashrc # MacOS export UP42_CFG_FILE =[ path to config.json ] >> ~/.bash_profile If you want to create a config.json file from a project key pair, you can use the config command. up42 -pid [ PROJECT_ID ] -pkey [ PROJECT_API_KEY ] config","title":"Authenticate"},{"location":"cli/#workflows","text":"up42 workflow -h Create a new workflow: up42 project create-workflow a_test Or check which workflows already exist: up42 project get-workflows And get a workflow by its name: up42 project workflow-from-name -name a_test After running the command to persist the workflow you can get the workflow tasks: up42 workflow get-workflow-tasks You can also add workflow tasks to the workflow via a json file (see typical usage for an example): up42 workflow add-workflow-tasks new_workflow_tasks.json","title":"Workflows"},{"location":"cli/#jobs","text":"up42 job -h First create and run a new test job with parameters defined in a json file (see typical usage for an example): up42 workflow test-job input_parameters.json --track Then run the actual job with parameters: up42 workflow run-job input_parameters.json --track After running the command to persist the job you can download the quicklooks from job in current working directory (note that not all data blocks support quicklooks): up42 job download-quicklooks . Or download and unpack the results: up42 job download-results . You can also print out the logs of the job: up42 job get-log","title":"Jobs"},{"location":"cli/#catalog","text":"up42 catalog -h With the catalog commands you can easily search the UP42 catalog for data. First, create a parameter configuration: up42 catalog construct-parameters example_aoi.geojson --sensors pleiades --max-cloud-cover 5 Then get the results: up42 catalog search example_search_params.json","title":"Catalog"},{"location":"cli/#general-tools","text":"Get all public blocks in the platform: up42 get-blocks Get block details by name: up42 get-block-details -name oneatlas-pleiades-aoiclipped","title":"General tools"},{"location":"index_2/","text":"UP42 Python SDK \u00b6 Welcome to UP42 - the developer platform and marketplace for geospatial data and applications: Access commercial and public geospatial datasets , e.g. satellite, aerial, vector, IoT & more. Construct workflows from modular building blocks for geospatial data, processing & analysis. Bring your own algorithms via custom blocks. Integrates into your own product via the API and Python package. Scale your geospatial applications with the powerful UP42 cloud infrastructure . UP42 Python SDK \u00b6 Python package for easy access to UP42's geospatial datasets & processing workflows For geospatial analysis & product builders! Interactive maps & visualization, ideal with Jupyter notebooks Command Line Interface (CLI) Developer tools for UP42 custom blocks (coming soon)","title":"UP42 Python SDK"},{"location":"index_2/#up42-python-sdk","text":"Welcome to UP42 - the developer platform and marketplace for geospatial data and applications: Access commercial and public geospatial datasets , e.g. satellite, aerial, vector, IoT & more. Construct workflows from modular building blocks for geospatial data, processing & analysis. Bring your own algorithms via custom blocks. Integrates into your own product via the API and Python package. Scale your geospatial applications with the powerful UP42 cloud infrastructure .","title":"UP42 Python SDK"},{"location":"index_2/#up42-python-sdk_1","text":"Python package for easy access to UP42's geospatial datasets & processing workflows For geospatial analysis & product builders! Interactive maps & visualization, ideal with Jupyter notebooks Command Line Interface (CLI) Developer tools for UP42 custom blocks (coming soon)","title":"UP42 Python SDK"},{"location":"installation/","text":"Installation \u00b6 User installation \u00b6 The package requires Python version > 3.6. Install via pip: pip install up42-py If you have an existing installation, update to the newest version via: pip install up42-py --upgrade Optional: Jupyter Lab Although optional, the UP42 Python SDK is optimally used in a Jupyter notebook, which makes data exploration even more comfortable! To install: pip install jupyterlab Test the successful installation by importing the package in Python: import up42 Success! Continue with the Authentication chapter ! Development installation \u00b6 Warning The development installation is only necessary if you want to contribute to up42-py, e.g. to fix a bug. Please see the developer readme for the full installation instructions and further information.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#user-installation","text":"The package requires Python version > 3.6. Install via pip: pip install up42-py If you have an existing installation, update to the newest version via: pip install up42-py --upgrade Optional: Jupyter Lab Although optional, the UP42 Python SDK is optimally used in a Jupyter notebook, which makes data exploration even more comfortable! To install: pip install jupyterlab Test the successful installation by importing the package in Python: import up42 Success! Continue with the Authentication chapter !","title":"User installation"},{"location":"installation/#development-installation","text":"Warning The development installation is only necessary if you want to contribute to up42-py, e.g. to fix a bug. Please see the developer readme for the full installation instructions and further information.","title":"Development installation"},{"location":"structure/","text":"Structure \u00b6 Hierachy \u00b6 The Python SDK uses six object classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask Catalog Tools Each object can spawn elements of one level below , e.g. project = up42.initialize_project() workflow = Project().create_workflow() job = workflow.run_job() Functionality \u00b6 An overview of the the functionality of each object (also see the code reference ): Available Functionality Project .get_workflows() .create_workflow() .get_jobs() .get_project_settings() .update_project_settings() .update_project_settings() Workflow .add_workflow_tasks() .get_parameters_info() .construct_parameters() .get_jobs() .test_job() .run_job() .get_workflow_tasks() .add_workflow_tasks() .update_name() .delete() Job .get_status() .track_status() .cancel_job() .get_results() .get_logs() .get_quicklooks() .download_results() .plot_results() .map_results() .upload_results_to_bucket() .get_jobtasks() .get_jobtasks_results() JobTask .get_results_json() .download_results() .get_quicklooks() Catalog .construct_parameters() .search() .download_quicklooks() Tools .read_vector_file() .get_example_aoi() .draw_aoi() .plot_coverage() .plot_quicklooks() .plot_results() .get_blocks() .get_block_details() .validate_manifest() .initialize_project() Object Initialization \u00b6 Here is how initialize each object directly, i.e. when it already exists on UP42 and you want to directly access it: Initialize Object Project up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) project = up42 . initialize_project () Workflow UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) Job UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = up42 . initialize_job ( job_id = UP42_JOB_ID ) JobTask UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = up42 . initialize_jobtask ( jobtask_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) Catalog catalog = up42 . initialize_catalog () Tools The tools' functionalities can be accessed from any of the up42 objects, e.g. up42 . get_example_aoi () # workflow.get_example_aoi() # job.get_example_aoi()","title":"Structure"},{"location":"structure/#structure","text":"","title":"Structure"},{"location":"structure/#hierachy","text":"The Python SDK uses six object classes, representing the hierarchical structure of UP42 : Project > Workflow > Job > JobTask Catalog Tools Each object can spawn elements of one level below , e.g. project = up42.initialize_project() workflow = Project().create_workflow() job = workflow.run_job()","title":"Hierachy"},{"location":"structure/#functionality","text":"An overview of the the functionality of each object (also see the code reference ): Available Functionality Project .get_workflows() .create_workflow() .get_jobs() .get_project_settings() .update_project_settings() .update_project_settings() Workflow .add_workflow_tasks() .get_parameters_info() .construct_parameters() .get_jobs() .test_job() .run_job() .get_workflow_tasks() .add_workflow_tasks() .update_name() .delete() Job .get_status() .track_status() .cancel_job() .get_results() .get_logs() .get_quicklooks() .download_results() .plot_results() .map_results() .upload_results_to_bucket() .get_jobtasks() .get_jobtasks_results() JobTask .get_results_json() .download_results() .get_quicklooks() Catalog .construct_parameters() .search() .download_quicklooks() Tools .read_vector_file() .get_example_aoi() .draw_aoi() .plot_coverage() .plot_quicklooks() .plot_results() .get_blocks() .get_block_details() .validate_manifest() .initialize_project()","title":"Functionality"},{"location":"structure/#object-initialization","text":"Here is how initialize each object directly, i.e. when it already exists on UP42 and you want to directly access it: Initialize Object Project up42 . authenticate ( project_id = \"123\" , project_api_key = \"456\" ) project = up42 . initialize_project () Workflow UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) Job UP42_JOB_ID = \"de5806aa-5ef1-4dc9-ab1d-06d7ec1a5021\" job = up42 . initialize_job ( job_id = UP42_JOB_ID ) JobTask UP42_JOBTASK_ID = \"3f772637-09aa-4164-bded-692fcd746d20\" jobtask = up42 . initialize_jobtask ( jobtask_id = UP42_JOBTASK_ID , job_id = UP42_JOB_ID ) Catalog catalog = up42 . initialize_catalog () Tools The tools' functionalities can be accessed from any of the up42 objects, e.g. up42 . get_example_aoi () # workflow.get_example_aoi() # job.get_example_aoi()","title":"Object Initialization"},{"location":"support-faq/","text":"Support & FAQ \u00b6 Contact \u00b6 Please contact us via Email support@up42.com or open a github issue . Related links \u00b6 UP42 Website UP42 Github Repositories UP42 Python package UP42 Documentation UP42 docs repository FAQ \u00b6 Can I contribute to the SDK? \u00b6 Yes, contributions and bug fixes are very welcome. Please see the developer readme for further instructions.","title":"FAQ"},{"location":"support-faq/#support-faq","text":"","title":"Support &amp; FAQ"},{"location":"support-faq/#contact","text":"Please contact us via Email support@up42.com or open a github issue .","title":"Contact"},{"location":"support-faq/#related-links","text":"UP42 Website UP42 Github Repositories UP42 Python package UP42 Documentation UP42 docs repository","title":"Related links"},{"location":"support-faq/#faq","text":"","title":"FAQ"},{"location":"support-faq/#can-i-contribute-to-the-sdk","text":"Yes, contributions and bug fixes are very welcome. Please see the developer readme for further instructions.","title":"Can I contribute to the SDK?"},{"location":"typical-usage/","text":"Typical Usage \u00b6 This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps. Authenticate & access project \u00b6 import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project Get information about the available blocks to later construct your workflow. up42 . get_blocks ( basic = True ) Create or access the workflow \u00b6 You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) workflow # Add workflow tasks - simple version. See above .get_blocks() result. input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control # about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) #workflow.get_jobs() # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access the existing workflow the id # (has to exist within the accessed project). UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow Select the aoi \u00b6 There are multiple ways to select an aoi: - Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. - Use .draw_aoi() to draw the aoi and export it as a geojson. - Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. - Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) #aoi = workflow.get_example_aoi(location=\"Berlin\") #workflow.draw_aoi() Select the workflow parameters \u00b6 There are also multiple ways to construct the workflow input parameters: - Provide the parameters directly in code as a json string. - Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. - Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameters_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters Test & Run the workflow & download results \u00b6 # Run a test job to query data availability and check the configuration. # With this test query you will not be charged with any data or processing # credits, but have a preview of the job result. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual workflow. job = workflow . run_job ( input_parameters = input_parameters , track_status = True ) Download & Display results \u00b6 # Download job result. results_fp = job . download_results () job . plot_results () job . map_results ()","title":"Typical Usage"},{"location":"typical-usage/#typical-usage","text":"This overview of the most important functions repeats the previous 30-seconds-example, but in more detail and shows additional functionality and alternative steps.","title":"Typical Usage"},{"location":"typical-usage/#authenticate-access-project","text":"import up42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project Get information about the available blocks to later construct your workflow. up42 . get_blocks ( basic = True )","title":"Authenticate &amp; access project"},{"location":"typical-usage/#create-or-access-the-workflow","text":"You can either create a new workflow, use project.get_workflows() to get all existing workflows within the project, or access an exisiting workflow directly via its workflow_id. Example: Sentinel 2 streaming & sharpening filter # Create a new, empty workflow. workflow = project . create_workflow ( name = \"30-seconds-workflow\" , use_existing = True ) workflow # Add workflow tasks - simple version. See above .get_blocks() result. input_tasks = [ 'sobloo-s2-l1c-aoiclipped' , 'sharpening' ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Alternative: Add workflow tasks - complex version, gives you more control # about the block connections. input_tasks = [ { \"name\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"parentName\" : None , \"blockId\" : \"a2daaab4-196d-4226-a018-a810444dcad1\" }, { \"name\" : \"sharpening:1\" , \"parentName\" : \"sobloo-s2-l1c-aoiclipped:1\" , \"blockId\" : \"4ed70368-d4e1-4462-bef6-14e768049471\" } ] workflow . add_workflow_tasks ( input_tasks = input_tasks ) # Check the added tasks. workflow . get_workflow_tasks ( basic = True ) #workflow.get_jobs() # Alternative: Get all existing workflows within the project. all_workflows = project . get_workflows () workflow = all_workflows [ 0 ] workflow # Alternative: Directly access the existing workflow the id # (has to exist within the accessed project). UP42_WORKFLOW_ID = \"7fb2ec8a-45be-41ad-a50f-98ba6b528b98\" workflow = up42 . initialize_workflow ( workflow_id = UP42_WORKFLOW_ID ) workflow","title":"Create or access the workflow"},{"location":"typical-usage/#select-the-aoi","text":"There are multiple ways to select an aoi: - Provide aoi the geometry directly in code as a FeatureCollection, Feature, GeoDataFrame, shapely Polygon or list of bounds coordinates. - Use .draw_aoi() to draw the aoi and export it as a geojson. - Use .read_vector_file() to read a geojson, json, shapefile, kml or wkt file. - Use .get_example_aoi() to read multiple provided sample aois. aoi = [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ] aoi = workflow . read_vector_file ( \"data/aoi_berlin.geojson\" , as_dataframe = True ) aoi . head ( 1 ) #aoi = workflow.get_example_aoi(location=\"Berlin\") #workflow.draw_aoi()","title":"Select the aoi"},{"location":"typical-usage/#select-the-workflow-parameters","text":"There are also multiple ways to construct the workflow input parameters: - Provide the parameters directly in code as a json string. - Use .get_parameters_info() to get a an overview of all potential parameters for the selected workflow and information about the parameter defaults and ranges. - Use .get_input_parameters(aoi_type=\"bbox\", aoi_geometry=aoi) to construct the parameters with the provided aoi and all default parameters. Selecting the aoi_type is independent from the provided aoi, you can e.g. provide a irregular Polygon and still select aoi_type=\"bbox\", then the bounding box of the polygon will be selected. workflow . get_parameters_info () input_parameters = { \"sobloo-s2-l1c-aoiclipped:1\" : { \"bbox\" : [ 13.375966 , 52.515068 , 13.378314 , 52.516639 ], \"ids\" : None , \"time\" : \"2018-01-01T00:00:00+00:00/2020-12-31T23:59:59+00:00\" , \"limit\" : 1 , \"zoom_level\" : 14 , \"time_series\" : None , \"max_cloud_cover\" : 30 }, \"sharpening:1\" : { \"strength\" : \"medium\" } } input_parameters = workflow . construct_parameters ( geometry = aoi , geometry_operation = \"bbox\" , limit = 1 ) # Further update the input_parameters manually input_parameters [ \"sobloo-s2-l1c-aoiclipped:1\" ] . update ({ \"max_cloud_cover\" : 60 }) input_parameters","title":"Select the workflow parameters"},{"location":"typical-usage/#test-run-the-workflow-download-results","text":"# Run a test job to query data availability and check the configuration. # With this test query you will not be charged with any data or processing # credits, but have a preview of the job result. test_job = workflow . test_job ( input_parameters = input_parameters , track_status = True ) test_results = test_job . get_results_json () print ( test_results ) # Run the actual workflow. job = workflow . run_job ( input_parameters = input_parameters , track_status = True )","title":"Test &amp; Run the workflow &amp; download results"},{"location":"typical-usage/#download-display-results","text":"# Download job result. results_fp = job . download_results () job . plot_results () job . map_results ()","title":"Download &amp; Display results"},{"location":"examples/airport-monitoring/","text":"Parallel Jobs \u00b6 Example: Airport monitoring \u00b6 Get a Sentinel-2 clipped image for 10 airports in a country. Run all jobs in parallel Visualize the results import up42 import pandas as pd import geopandas as gpd from pathlib import Path 10 random airports in Spain \u00b6 Airport locations scrapped from: https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) airports = airports . sample ( 10 ) # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) # Buffer airport point locations by roughly 100m airports . geometry = airports . geometry . buffer ( 0.001 ) airports . iloc [ 0 ] . geometry Prepare UP42 workflows \u00b6 # Authenticate with UP42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project # Increase the parallel job limit for the project. # Only works when you have added your credit card information to the UP42 account. project . update_project_settings ( max_concurrent_jobs = 10 ) workflow = project . create_workflow ( \"workflow_demo_airplanes\" , use_existing = True ) workflow # Fill the workflow with tasks blocks = up42 . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ blocks [ selected_block ]]) workflow . get_workflow_tasks ( basic = True ) Run jobs in parallel \u00b6 Queries & downloads one image per airport in parallel. Very crude, this will soon be available in the API in one command! # Run jobs in parallel jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameters ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) # Download results: out_filepaths = [] for job in jobs : fp = job . download_results () out_filepaths . append ( fp [ 0 ]) print ( \"finished\" ) print ( out_filepaths ) # Visualize downloaded results up42 . plot_results ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ())","title":"Airport monitoring"},{"location":"examples/airport-monitoring/#parallel-jobs","text":"","title":"Parallel Jobs"},{"location":"examples/airport-monitoring/#example-airport-monitoring","text":"Get a Sentinel-2 clipped image for 10 airports in a country. Run all jobs in parallel Visualize the results import up42 import pandas as pd import geopandas as gpd from pathlib import Path","title":"Example: Airport monitoring"},{"location":"examples/airport-monitoring/#10-random-airports-in-spain","text":"Airport locations scrapped from: https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat country = \"Spain\" dat = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\" airports = pd . read_table ( dat , sep = \",\" , usecols = [ 0 , 1 , 3 , 6 , 7 ], names = [ \"uid\" , 'airport' , \"country\" , \"lat\" , \"lon\" ]) airports = airports [ airports . country == country ] airports = gpd . GeoDataFrame ( airports , geometry = gpd . points_from_xy ( airports . lon , airports . lat )) world = gpd . read_file ( gpd . datasets . get_path ( 'naturalearth_lowres' )) world = world [ world . name == country ] airports = airports [ airports . within ( world . iloc [ 0 ] . geometry )] display ( airports . head ()) airports = airports . sample ( 10 ) # Visualize locations ax = world . plot ( figsize = ( 10 , 10 ), color = 'white' , edgecolor = 'black' ) airports . plot ( markersize = 20 , ax = ax , color = \"r\" ) # Buffer airport point locations by roughly 100m airports . geometry = airports . geometry . buffer ( 0.001 ) airports . iloc [ 0 ] . geometry","title":"10 random airports in Spain"},{"location":"examples/airport-monitoring/#prepare-up42-workflows","text":"# Authenticate with UP42 up42 . authenticate ( project_id = 12345 , project_api_key = 12345 ) #up42.authenticate(cfg_file=\"config.json\") project = up42 . initialize_project () project # Increase the parallel job limit for the project. # Only works when you have added your credit card information to the UP42 account. project . update_project_settings ( max_concurrent_jobs = 10 ) workflow = project . create_workflow ( \"workflow_demo_airplanes\" , use_existing = True ) workflow # Fill the workflow with tasks blocks = up42 . get_blocks ( basic = True ) selected_block = \"sobloo-s2-l1c-aoiclipped\" workflow . add_workflow_tasks ([ blocks [ selected_block ]]) workflow . get_workflow_tasks ( basic = True )","title":"Prepare UP42 workflows"},{"location":"examples/airport-monitoring/#run-jobs-in-parallel","text":"Queries & downloads one image per airport in parallel. Very crude, this will soon be available in the API in one command! # Run jobs in parallel jobs = [] for airport in airports . geometry : input_parameters = workflow . construct_parameters ( geometry = airport , geometry_operation = \"bbox\" ) input_parameters [ f \" { selected_block } :1\" ][ \"max_cloud_cover\" ] = 10 job = workflow . run_job ( input_parameters = input_parameters ) jobs . append ( job ) # Track status until the last job is finished. for job in jobs : job . track_status ( report_time = 20 ) # Download results: out_filepaths = [] for job in jobs : fp = job . download_results () out_filepaths . append ( fp [ 0 ]) print ( \"finished\" ) print ( out_filepaths ) # Visualize downloaded results up42 . plot_results ( figsize = ( 22 , 22 ), filepaths = out_filepaths , titles = airports . airport . to_list ())","title":"Run jobs in parallel"},{"location":"examples/examples-intro/","text":"Examples \u00b6 This section provides some more extensive examples of usage examples and use cases for the UP42 Python SDK. Also see the Jupyter notebooks in the examples folder . Airport monitoring with parallel jobs","title":"Examples"},{"location":"examples/examples-intro/#examples","text":"This section provides some more extensive examples of usage examples and use cases for the UP42 Python SDK. Also see the Jupyter notebooks in the examples folder . Airport monitoring with parallel jobs","title":"Examples"},{"location":"reference/catalog/","text":"Project class \u00b6 \u00b6 __init__ ( self , auth ) special \u00b6 The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. Public Methods: construct_parameters, search, download_quicklooks Source code in up42/catalog.py 54 55 56 57 58 59 60 61 62 def __init__ ( self , auth : Auth ): \"\"\"The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. Public Methods: construct_parameters, search, download_quicklooks \"\"\" self . auth = auth self . quicklooks = None construct_parameters ( geometry , start_date = '2020-01-01' , end_date = '2020-01-30' , sensors = [ 'pleiades' , 'spot' , 'sentinel1' , 'sentinel2' , 'sentinel3' , 'sentinel5p' ], limit = 1 , max_cloudcover = 100 , sortby = 'cloudCoverage' , ascending = True ) staticmethod \u00b6 Follows STAC principles and property names. Parameters: Name Type Description Default geometry Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.point.Point, shapely.geometry.polygon.Polygon] The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. required start_date str Query period starting day, format \"2020-01-01\". '2020-01-01' end_date str Query period ending day, format \"2020-01-01\". '2020-01-30' sensors List[str] The satellite sensor(s) to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] ['pleiades', 'spot', 'sentinel1', 'sentinel2', 'sentinel3', 'sentinel5p'] limit int The maximum number of search results to return. 1 max_cloudcover float Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. 100 sortby str The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" 'cloudCoverage' ascending bool Ascending sort order by default, descending if False. True Returns: Type Description Dict The constructed parameters dictionary. Source code in up42/catalog.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 @staticmethod def construct_parameters ( geometry : Union [ Dict , Feature , FeatureCollection , List , GeoDataFrame , Point , Polygon , ], start_date : str = \"2020-01-01\" , end_date : str = \"2020-01-30\" , sensors : List [ str ] = [ \"pleiades\" , \"spot\" , \"sentinel1\" , \"sentinel2\" , \"sentinel3\" , \"sentinel5p\" , ], limit : int = 1 , max_cloudcover : float = 100 , sortby : str = \"cloudCoverage\" , ascending : bool = True , ) -> Dict : \"\"\" Follows STAC principles and property names. Args: geometry: The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". sensors: The satellite sensor(s) to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] limit: The maximum number of search results to return. max_cloudcover: Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. sortby: The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" ascending: Ascending sort order by default, descending if False. Returns: The constructed parameters dictionary. \"\"\" datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" block_filters : List [ str ] = [] for sensor in sensors : if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) block_filters . extend ( supported_sensors [ sensor ][ \"blocks\" ]) query_filters = { \"cloudCoverage\" : { \"lte\" : max_cloudcover }, \"dataBlock\" : { \"in\" : block_filters }, } if ascending : sort_order = \"asc\" else : sort_order = \"desc\" aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_geometry = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = \"intersects\" , squash_multiple_features = \"footprint\" , ) # TODO: cc also contains nan with sentinel 1 etc. ignore? search_paramaters = { \"datetime\" : datetime , \"intersects\" : aoi_geometry , \"limit\" : limit , \"query\" : query_filters , \"sortby\" : [{ \"field\" : f \"properties. { sortby } \" , \"direction\" : sort_order }], } return search_paramaters download_quicklooks ( self , image_ids , sensor , output_directory = None ) \u00b6 Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Parameters: Name Type Description Default image_ids List[str] provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" required sensors The satellite sensor(s) to search for, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". required output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of quicklook image output file paths. Source code in up42/catalog.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def download_quicklooks ( self , image_ids : List [ str ], sensor : str , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Args: image_ids: provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" sensors: The satellite sensor(s) to search for, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". output_directory: The file output directory, defaults to the current working directory. Returns: List of quicklook image output file paths. \"\"\" if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) provider = supported_sensors [ sensor ][ \"provider\" ] logger . info ( \"Getting quicklooks from provider %s for image_ids: %s \" , provider , image_ids ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / \"catalog\" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) if isinstance ( image_ids , str ): image_ids = [ image_ids ] out_paths : List [ str ] = [] for image_id in tqdm ( image_ids ): out_path = output_directory / f \"quicklook_ { image_id } .jpg\" out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /catalog/ { provider } /image/ { image_id } /quicklook\" ) try : response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) response . raise_for_status () except HTTPError as err : raise SystemExit ( err ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths search ( self , search_paramaters , as_dataframe = True ) \u00b6 Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Parameters: Name Type Description Default search_params The catalog search parameters, see example. required as_dataframe bool return type, GeoDataFrame if True (default), FeatureCollection if False. True Returns: Type Description Union[geopandas.geodataframe.GeoDataFrame, Dict] The search results as a GeoDataFrame, optionally as json dict. Example search_paramaters = { \"datetime\" : \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\" , \"intersects\" : { \"type\" : \"Polygon\" , \"coordinates\" : [[[ 13.32113746 , 52.73971768 ],[ 13.15981158 , 52.2092959 ], [ 13.62204483 , 52.15632025 ],[ 13.78859517 , 52.68655119 ],[ 13.32113746 , 52.73971768 ]]]}, \"limit\" : 1 , \"sortby\" : [{ \"field\" : \"properties.cloudCoverage\" , \"direction\" : \"asc\" }] } Source code in up42/catalog.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def search ( self , search_paramaters : Dict , as_dataframe : bool = True ) -> Union [ GeoDataFrame , Dict ]: \"\"\" Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Args: search_params: The catalog search parameters, see example. as_dataframe: return type, GeoDataFrame if True (default), FeatureCollection if False. Returns: The search results as a GeoDataFrame, optionally as json dict. Example: ```python search_paramaters={ \"datetime\": \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\", \"intersects\": { \"type\": \"Polygon\", \"coordinates\": [[[13.32113746,52.73971768],[13.15981158,52.2092959], [13.62204483,52.15632025],[13.78859517,52.68655119],[13.32113746, 52.73971768]]]}, \"limit\": 1, \"sortby\": [{\"field\" : \"properties.cloudCoverage\",\"direction\" : \"asc\"}] } ``` \"\"\" logger . info ( \"Searching catalog with: %r \" , search_paramaters ) url = f \" { self . auth . _endpoint () } /catalog/stac/search\" response_json = self . auth . _request ( \"POST\" , url , search_paramaters ) logger . info ( \" %d results returned.\" , len ( response_json [ \"features\" ])) dst_crs = \"EPSG:4326\" df = GeoDataFrame . from_features ( response_json , crs = dst_crs ) if df . empty : if as_dataframe : return df else : return df . __geo_interface__ # Filter to actual geometries intersecting the aoi (Sobloo search uses a rectangular # bounds geometry, can contain scenes that touch the aoi bbox, but not the aoi. # So number returned images not consistent with set limit. # TODO: Resolve on backend geometry = search_paramaters [ \"intersects\" ] poly = shape ( geometry ) df = df [ df . intersects ( poly )] df = df . reset_index () # Make scene_id more easily accessible # TODO: Add by default to results, independent of sensor. def _get_scene_id ( row ): if row [ \"providerName\" ] == \"oneatlas\" : row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"parentIdentifier\" ] elif row [ \"providerName\" ] in [ \"sobloo-radar\" , \"sobloo-image\" ]: row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"identification\" ][ \"externalId\" ] return row df = df . apply ( _get_scene_id , axis = 1 ) df . crs = dst_crs # apply resets the crs if as_dataframe : return df else : return df . __geo_interface__","title":"Catalog"},{"location":"reference/catalog/#project-class","text":"","title":"Project class"},{"location":"reference/catalog/#up42.catalog.Catalog","text":"","title":"up42.catalog.Catalog"},{"location":"reference/catalog/#up42.catalog.Catalog.__init__","text":"The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. Public Methods: construct_parameters, search, download_quicklooks Source code in up42/catalog.py 54 55 56 57 58 59 60 61 62 def __init__ ( self , auth : Auth ): \"\"\"The Catalog class enables access to the UP42 catalog search. You can search for satellite image scenes for different sensors and criteria like cloud cover etc. Public Methods: construct_parameters, search, download_quicklooks \"\"\" self . auth = auth self . quicklooks = None","title":"__init__()"},{"location":"reference/catalog/#up42.catalog.Catalog.construct_parameters","text":"Follows STAC principles and property names. Parameters: Name Type Description Default geometry Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.point.Point, shapely.geometry.polygon.Polygon] The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. required start_date str Query period starting day, format \"2020-01-01\". '2020-01-01' end_date str Query period ending day, format \"2020-01-01\". '2020-01-30' sensors List[str] The satellite sensor(s) to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] ['pleiades', 'spot', 'sentinel1', 'sentinel2', 'sentinel3', 'sentinel5p'] limit int The maximum number of search results to return. 1 max_cloudcover float Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. 100 sortby str The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" 'cloudCoverage' ascending bool Ascending sort order by default, descending if False. True Returns: Type Description Dict The constructed parameters dictionary. Source code in up42/catalog.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 @staticmethod def construct_parameters ( geometry : Union [ Dict , Feature , FeatureCollection , List , GeoDataFrame , Point , Polygon , ], start_date : str = \"2020-01-01\" , end_date : str = \"2020-01-30\" , sensors : List [ str ] = [ \"pleiades\" , \"spot\" , \"sentinel1\" , \"sentinel2\" , \"sentinel3\" , \"sentinel5p\" , ], limit : int = 1 , max_cloudcover : float = 100 , sortby : str = \"cloudCoverage\" , ascending : bool = True , ) -> Dict : \"\"\" Follows STAC principles and property names. Args: geometry: The search geometry, one of Dict, Feature, FeatureCollection, List, GeoDataFrame, Point, Polygon. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". sensors: The satellite sensor(s) to search for, one or multiple of [\"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\"] limit: The maximum number of search results to return. max_cloudcover: Maximum cloudcover % - 100 will return all scenes, 8.4 will return all scenes with 8.4 or less cloudcover. sortby: The property to sort by, \"cloudCoverage\", \"acquisitionDate\", \"acquisitionIdentifier\", \"incidenceAngle\", \"snowCover\" ascending: Ascending sort order by default, descending if False. Returns: The constructed parameters dictionary. \"\"\" datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" block_filters : List [ str ] = [] for sensor in sensors : if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) block_filters . extend ( supported_sensors [ sensor ][ \"blocks\" ]) query_filters = { \"cloudCoverage\" : { \"lte\" : max_cloudcover }, \"dataBlock\" : { \"in\" : block_filters }, } if ascending : sort_order = \"asc\" else : sort_order = \"desc\" aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_geometry = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = \"intersects\" , squash_multiple_features = \"footprint\" , ) # TODO: cc also contains nan with sentinel 1 etc. ignore? search_paramaters = { \"datetime\" : datetime , \"intersects\" : aoi_geometry , \"limit\" : limit , \"query\" : query_filters , \"sortby\" : [{ \"field\" : f \"properties. { sortby } \" , \"direction\" : sort_order }], } return search_paramaters","title":"construct_parameters()"},{"location":"reference/catalog/#up42.catalog.Catalog.download_quicklooks","text":"Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Parameters: Name Type Description Default image_ids List[str] provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" required sensors The satellite sensor(s) to search for, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". required output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of quicklook image output file paths. Source code in up42/catalog.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def download_quicklooks ( self , image_ids : List [ str ], sensor : str , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Gets the quicklooks of scenes from a single sensor. After download, can be plotted via catalog.plot_quicklooks(). Args: image_ids: provider image_id in the form \"6dffb8be-c2ab-46e3-9c1c-6958a54e4527\" sensors: The satellite sensor(s) to search for, one of \"pleiades\", \"spot\", \"sentinel1\", \"sentinel2\", \"sentinel3\", \"sentinel5p\". output_directory: The file output directory, defaults to the current working directory. Returns: List of quicklook image output file paths. \"\"\" if sensor not in list ( supported_sensors . keys ()): raise ValueError ( f \"Currently only these sensors are supported: \" f \" { list ( supported_sensors . keys ()) } \" ) provider = supported_sensors [ sensor ][ \"provider\" ] logger . info ( \"Getting quicklooks from provider %s for image_ids: %s \" , provider , image_ids ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / \"catalog\" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) if isinstance ( image_ids , str ): image_ids = [ image_ids ] out_paths : List [ str ] = [] for image_id in tqdm ( image_ids ): out_path = output_directory / f \"quicklook_ { image_id } .jpg\" out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /catalog/ { provider } /image/ { image_id } /quicklook\" ) try : response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) response . raise_for_status () except HTTPError as err : raise SystemExit ( err ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/catalog/#up42.catalog.Catalog.search","text":"Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Parameters: Name Type Description Default search_params The catalog search parameters, see example. required as_dataframe bool return type, GeoDataFrame if True (default), FeatureCollection if False. True Returns: Type Description Union[geopandas.geodataframe.GeoDataFrame, Dict] The search results as a GeoDataFrame, optionally as json dict. Example search_paramaters = { \"datetime\" : \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\" , \"intersects\" : { \"type\" : \"Polygon\" , \"coordinates\" : [[[ 13.32113746 , 52.73971768 ],[ 13.15981158 , 52.2092959 ], [ 13.62204483 , 52.15632025 ],[ 13.78859517 , 52.68655119 ],[ 13.32113746 , 52.73971768 ]]]}, \"limit\" : 1 , \"sortby\" : [{ \"field\" : \"properties.cloudCoverage\" , \"direction\" : \"asc\" }] } Source code in up42/catalog.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def search ( self , search_paramaters : Dict , as_dataframe : bool = True ) -> Union [ GeoDataFrame , Dict ]: \"\"\" Searches the catalog for the the search parameters and returns the metadata of the matching scenes. Args: search_params: The catalog search parameters, see example. as_dataframe: return type, GeoDataFrame if True (default), FeatureCollection if False. Returns: The search results as a GeoDataFrame, optionally as json dict. Example: ```python search_paramaters={ \"datetime\": \"2019-01-01T00:00:00Z/2019-01-15T00:00:00Z\", \"intersects\": { \"type\": \"Polygon\", \"coordinates\": [[[13.32113746,52.73971768],[13.15981158,52.2092959], [13.62204483,52.15632025],[13.78859517,52.68655119],[13.32113746, 52.73971768]]]}, \"limit\": 1, \"sortby\": [{\"field\" : \"properties.cloudCoverage\",\"direction\" : \"asc\"}] } ``` \"\"\" logger . info ( \"Searching catalog with: %r \" , search_paramaters ) url = f \" { self . auth . _endpoint () } /catalog/stac/search\" response_json = self . auth . _request ( \"POST\" , url , search_paramaters ) logger . info ( \" %d results returned.\" , len ( response_json [ \"features\" ])) dst_crs = \"EPSG:4326\" df = GeoDataFrame . from_features ( response_json , crs = dst_crs ) if df . empty : if as_dataframe : return df else : return df . __geo_interface__ # Filter to actual geometries intersecting the aoi (Sobloo search uses a rectangular # bounds geometry, can contain scenes that touch the aoi bbox, but not the aoi. # So number returned images not consistent with set limit. # TODO: Resolve on backend geometry = search_paramaters [ \"intersects\" ] poly = shape ( geometry ) df = df [ df . intersects ( poly )] df = df . reset_index () # Make scene_id more easily accessible # TODO: Add by default to results, independent of sensor. def _get_scene_id ( row ): if row [ \"providerName\" ] == \"oneatlas\" : row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"parentIdentifier\" ] elif row [ \"providerName\" ] in [ \"sobloo-radar\" , \"sobloo-image\" ]: row [ \"scene_id\" ] = row [ \"providerProperties\" ][ \"identification\" ][ \"externalId\" ] return row df = df . apply ( _get_scene_id , axis = 1 ) df . crs = dst_crs # apply resets the crs if as_dataframe : return df else : return df . __geo_interface__","title":"search()"},{"location":"reference/job/","text":"Job class \u00b6 \u00b6 __init__ ( self , auth , project_id , job_id , order_ids = None ) special \u00b6 The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json Source code in up42/job.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , auth : Auth , project_id : str , job_id : str , order_ids : List [ str ] = None , ): \"\"\"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . quicklooks = None self . results = None if order_ids is None : self . order_ids = [ \"\" ] if self . auth . get_info : self . info = self . _get_info () cancel_job ( self ) \u00b6 Cancels a pending or running job. Source code in up42/job.py 120 121 122 123 124 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . auth . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . auth . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id ) download_quicklooks ( self , output_directory = None ) \u00b6 Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). Source code in up42/job.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). \"\"\" # Currently only the first/data task produces quicklooks. logger . setLevel ( logging . CRITICAL ) data_task = self . get_jobtasks ()[ 0 ] logger . setLevel ( logging . INFO ) out_paths : List [ str ] = data_task . download_quicklooks ( # type: ignore output_directory = output_directory ) # type: ignore self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths download_results ( self , output_directory = None ) \u00b6 Downloads and unpacks the job results. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/job.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def download_results ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads and unpacks the job results. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of job %s \" , self . job_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths get_jobtasks ( self , return_json = False ) \u00b6 Get the individual items of the job as JobTask objects or json. Parameters: Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns: Type Description Union[List[JobTask], List[Dict]] The job task objects in a list. Source code in up42/job.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def get_jobtasks ( self , return_json : bool = False ) -> Union [ List [ \"JobTask\" ], List [ Dict ]]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_json : List [ Dict ] = response_json [ \"data\" ] jobtasks = [ JobTask ( auth = self . auth , project_id = self . project_id , job_id = self . job_id , jobtask_id = task [ \"id\" ], ) for task in jobtasks_json ] if return_json : return jobtasks_json else : return jobtasks get_jobtasks_results_json ( self ) \u00b6 Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: Type Description Dict The data.json of alle single job tasks. Source code in up42/job.py 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 def get_jobtasks_results_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] jobtasks_results_json = {} for jobtask_id in jobtasks_ids : url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { jobtask_id } /outputs/data-json\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_results_json [ jobtask_id ] = response_json return jobtasks_results_json get_logs ( self , as_print = True , as_return = False ) \u00b6 Convenience function to print or return the logs of all job tasks. Parameters: Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns: Type Description The log strings (only if as_return was selected). Source code in up42/job.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def get_logs ( self , as_print : bool = True , as_return : bool = False ): \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( jobtasks_ids ), jobtasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( jobtasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , jobtask_id in enumerate ( jobtasks_ids ): url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { jobtask_id } /logs\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) job_logs [ jobtask_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with jobtask_id { jobtask_id } : \\n \" ) print ( response_json ) if as_return : return job_logs get_results_json ( self , as_dataframe = False ) \u00b6 Gets the Job results data.json. Parameters: Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. Source code in up42/job.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Job results data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json get_status ( self ) \u00b6 Gets the job status. Returns: Type Description str The job status. Source code in up42/job.py 67 68 69 70 71 72 73 74 75 76 77 78 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status map_results ( self , show_images = True , name_column = 'uid' ) \u00b6 Displays data.json, and if available, one or multiple results geotiffs. Parameters: Name Type Description Default show_images Shows images if True (default), only features if False. True name_column str Name of the column that provides the Feature/Layer name. 'uid' TODO: Make generic with scene_id column integrated. \u00b6 Source code in up42/job.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def map_results ( self , show_images = True , name_column : str = \"uid\" ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs. Args: show_images: Shows images if True (default), only features if False. name_column: Name of the column that provides the Feature/Layer name. # TODO: Make generic with scene_id column integrated. \"\"\" if self . results is None : raise ValueError ( \"You first need to download the results via job.download_results()!\" ) def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } # Add feature to map. df : GeoDataFrame = self . get_results_json ( as_dataframe = True ) # type: ignore centroid = box ( * df . total_bounds ) . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) for idx , row in df . iterrows (): # type: ignore try : feature_name = row . loc [ name_column ] except KeyError : feature_name = \"\" layer_name = f \"Feature { idx + 1 } - { feature_name } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , style_function = _style_function , highlight_function = _highlight_function , ) folium . Popup ( f \" { layer_name } : { row . drop ( 'geometry' , axis = 0 ) . to_json () } \" ) . add_to ( f ) f . add_to ( m ) # Same: folium.GeoJson(df, name=name_column, style_function=style_function, # highlight_function=highlight_function).add_to(map) # Add image to map. if show_images and self . results is not None : plot_file_format = [ \".tif\" ] plottable_images = [ path for path in self . results if Path ( path ) . suffix in plot_file_format ] if plottable_images : dst_crs = \"EPSG:4326\" filepaths : List [ Path ] = self . results try : feature_names = df [ name_column ] . to_list () except KeyError : feature_names = [ \"\" for i in range ( df . shape [ 0 ])] for idx , ( raster_fp , feature_name ) in enumerate ( zip ( filepaths , feature_names ) ): # TODO: Not ideal, streaming images are webmercator, folium requires wgs 84.0 # TODO: Switch to ipyleaflet! # This requires reprojecting on the user pc, not via the api. # Reproject raster and add to map with rasterio . open ( raster_fp ) as src : dst_profile = src . meta . copy () if src . crs == dst_crs : dst_array = src . read ()[: 3 , :, :] minx , miny , maxx , maxy = src . bounds else : transform , width , height = calculate_default_transform ( src . crs , dst_crs , src . width , src . height , * src . bounds ) dst_profile . update ( { \"crs\" : dst_crs , \"transform\" : transform , \"width\" : width , \"height\" : height , } ) with MemoryFile () as memfile : with memfile . open ( ** dst_profile ) as mem : for i in range ( 1 , src . count + 1 ): reproject ( source = rasterio . band ( src , i ), destination = rasterio . band ( mem , i ), src_transform = src . transform , src_crs = src . crs , dst_transform = transform , dst_crs = dst_crs , resampling = Resampling . nearest , ) dst_array = mem . read ()[: 3 , :, :] minx , miny , maxx , maxy = mem . bounds # TODO: Make band configuration available m . add_child ( folium . raster_layers . ImageOverlay ( np . moveaxis ( np . stack ( dst_array ), 0 , 2 ), bounds = [[ miny , minx ], [ maxy , maxx ]], # different order. name = f \"Image { idx + 1 } - { feature_name } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement #type: ignore collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m track_status ( self , report_time = 30 ) \u00b6 Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Parameters: Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 Source code in up42/job.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def track_status ( self , report_time : int = 30 ) -> str : \"\"\" Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" logger . info ( \"Tracking job status continuously, reporting every %s seconds...\" , report_time , ) status = \"NOT STARTED\" time_asleep = 0 while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: if time_asleep != 0 and time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) elif status in [ \"FAILED\" , \"ERROR\" ]: logger . info ( \"Job is %s ! - %s - Printing logs ...\" , status , self . job_id ) self . get_logs ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) elif status in [ \"CANCELLED\" , \"CANCELLING\" ]: logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) raise ValueError ( \"Job has been cancelled!\" ) elif status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully! - %s \" , self . job_id ) sleep ( 5 ) time_asleep += 5 return status upload_results_to_bucket ( self , gs_client , bucket , folder , extension = '.tgz' , version = 'v0' ) \u00b6 Uploads the results to a custom google cloud storage bucket. Source code in up42/job.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def upload_results_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the results to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s results with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s results to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" )","title":"Job"},{"location":"reference/job/#job-class","text":"","title":"Job class"},{"location":"reference/job/#up42.job.Job","text":"","title":"up42.job.Job"},{"location":"reference/job/#up42.job.Job.__init__","text":"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json Source code in up42/job.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , auth : Auth , project_id : str , job_id : str , order_ids : List [ str ] = None , ): \"\"\"The Job class provides access to the results, parameters and tasks of UP42 Jobs (Workflows that have been run as Jobs). Public Methods: get_status, track_status, cancel_job, download_quicklooks, get_results_json download_results, upload_results_to_bucket, map_results, get_logs, get_jobtasks, get_jobtasks_results_json \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . quicklooks = None self . results = None if order_ids is None : self . order_ids = [ \"\" ] if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/job/#up42.job.Job.cancel_job","text":"Cancels a pending or running job. Source code in up42/job.py 120 121 122 123 124 def cancel_job ( self ) -> None : \"\"\"Cancels a pending or running job.\"\"\" url = f \" { self . auth . _endpoint () } /jobs/ { self . job_id } /cancel/\" self . auth . _request ( request_type = \"POST\" , url = url ) logger . info ( \"Job canceled: %s \" , self . job_id )","title":"cancel_job()"},{"location":"reference/job/#up42.job.Job.download_quicklooks","text":"Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). Source code in up42/job.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Conveniance function that downloads the quicklooks of the data (dirst) jobtask. After download, can be plotted via job.plot_quicklooks(). \"\"\" # Currently only the first/data task produces quicklooks. logger . setLevel ( logging . CRITICAL ) data_task = self . get_jobtasks ()[ 0 ] logger . setLevel ( logging . INFO ) out_paths : List [ str ] = data_task . download_quicklooks ( # type: ignore output_directory = output_directory ) # type: ignore self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/job/#up42.job.Job.download_results","text":"Downloads and unpacks the job results. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/job.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def download_results ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads and unpacks the job results. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of job %s \" , self . job_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths","title":"download_results()"},{"location":"reference/job/#up42.job.Job.get_jobtasks","text":"Get the individual items of the job as JobTask objects or json. Parameters: Name Type Description Default return_json bool If True returns the json information of the job tasks. False Returns: Type Description Union[List[JobTask], List[Dict]] The job task objects in a list. Source code in up42/job.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def get_jobtasks ( self , return_json : bool = False ) -> Union [ List [ \"JobTask\" ], List [ Dict ]]: \"\"\" Get the individual items of the job as JobTask objects or json. Args: return_json: If True returns the json information of the job tasks. Returns: The job task objects in a list. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/\" ) logger . info ( \"Getting job tasks: %s \" , self . job_id ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_json : List [ Dict ] = response_json [ \"data\" ] jobtasks = [ JobTask ( auth = self . auth , project_id = self . project_id , job_id = self . job_id , jobtask_id = task [ \"id\" ], ) for task in jobtasks_json ] if return_json : return jobtasks_json else : return jobtasks","title":"get_jobtasks()"},{"location":"reference/job/#up42.job.Job.get_jobtasks_results_json","text":"Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: Type Description Dict The data.json of alle single job tasks. Source code in up42/job.py 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 def get_jobtasks_results_json ( self ) -> Dict : \"\"\" Convenience function to get the resulting data.json of all job tasks in a dictionary of strings. Returns: The data.json of alle single job tasks. \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] jobtasks_results_json = {} for jobtask_id in jobtasks_ids : url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { jobtask_id } /outputs/data-json\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobtasks_results_json [ jobtask_id ] = response_json return jobtasks_results_json","title":"get_jobtasks_results_json()"},{"location":"reference/job/#up42.job.Job.get_logs","text":"Convenience function to print or return the logs of all job tasks. Parameters: Name Type Description Default as_print bool Prints the logs, no return. True as_return bool Also returns the log strings. False Returns: Type Description The log strings (only if as_return was selected). Source code in up42/job.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def get_logs ( self , as_print : bool = True , as_return : bool = False ): \"\"\" Convenience function to print or return the logs of all job tasks. Args: as_print: Prints the logs, no return. as_return: Also returns the log strings. Returns: The log strings (only if as_return was selected). \"\"\" jobtasks : List [ Dict ] = self . get_jobtasks ( return_json = True ) # type: ignore jobtasks_ids = [ task [ \"id\" ] for task in jobtasks ] logger . info ( \"Getting logs for %s job tasks: %s \" , len ( jobtasks_ids ), jobtasks_ids ) job_logs = {} if as_print : print ( f \"Printing logs of { len ( jobtasks_ids ) } JobTasks in Job with job_id \" f \" { self . job_id } : \\n \" ) for idx , jobtask_id in enumerate ( jobtasks_ids ): url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/\" f \" { self . job_id } /tasks/ { jobtask_id } /logs\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) job_logs [ jobtask_id ] = response_json if as_print : print ( \"----------------------------------------------------------\" ) print ( f \"JobTask { idx + 1 } with jobtask_id { jobtask_id } : \\n \" ) print ( response_json ) if as_return : return job_logs","title":"get_logs()"},{"location":"reference/job/#up42.job.Job.get_results_json","text":"Gets the Job results data.json. Parameters: Name Type Description Default as_dataframe bool Return type, Default Feature Collection. GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] The job data.json json. Source code in up42/job.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Job results data.json. Args: as_dataframe: Return type, Default Feature Collection. GeoDataFrame if True. Returns: The job data.json json. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"get_results_json()"},{"location":"reference/job/#up42.job.Job.get_status","text":"Gets the job status. Returns: Type Description str The job status. Source code in up42/job.py 67 68 69 70 71 72 73 74 75 76 77 78 def get_status ( self ) -> str : \"\"\" Gets the job status. Returns: The job status. \"\"\" # logger.info(\"Getting job status: %s\", self.job_id) info = self . _get_info () status = info [ \"status\" ] logger . info ( \"Job is %s \" , status ) return status","title":"get_status()"},{"location":"reference/job/#up42.job.Job.map_results","text":"Displays data.json, and if available, one or multiple results geotiffs. Parameters: Name Type Description Default show_images Shows images if True (default), only features if False. True name_column str Name of the column that provides the Feature/Layer name. 'uid'","title":"map_results()"},{"location":"reference/job/#todo-make-generic-with-scene_id-column-integrated","text":"Source code in up42/job.py 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def map_results ( self , show_images = True , name_column : str = \"uid\" ) -> None : \"\"\" Displays data.json, and if available, one or multiple results geotiffs. Args: show_images: Shows images if True (default), only features if False. name_column: Name of the column that provides the Feature/Layer name. # TODO: Make generic with scene_id column integrated. \"\"\" if self . results is None : raise ValueError ( \"You first need to download the results via job.download_results()!\" ) def _style_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#5288c4\" , \"color\" : \"blue\" , \"weight\" : 2.5 , \"dashArray\" : \"5, 5\" , } def _highlight_function ( feature ): # pylint: disable=unused-argument return { \"fillColor\" : \"#ffaf00\" , \"color\" : \"red\" , \"weight\" : 3.5 , \"dashArray\" : \"5, 5\" , } # Add feature to map. df : GeoDataFrame = self . get_results_json ( as_dataframe = True ) # type: ignore centroid = box ( * df . total_bounds ) . centroid m = folium_base_map ( lat = centroid . y , lon = centroid . x ,) for idx , row in df . iterrows (): # type: ignore try : feature_name = row . loc [ name_column ] except KeyError : feature_name = \"\" layer_name = f \"Feature { idx + 1 } - { feature_name } \" f = folium . GeoJson ( row [ \"geometry\" ], name = layer_name , style_function = _style_function , highlight_function = _highlight_function , ) folium . Popup ( f \" { layer_name } : { row . drop ( 'geometry' , axis = 0 ) . to_json () } \" ) . add_to ( f ) f . add_to ( m ) # Same: folium.GeoJson(df, name=name_column, style_function=style_function, # highlight_function=highlight_function).add_to(map) # Add image to map. if show_images and self . results is not None : plot_file_format = [ \".tif\" ] plottable_images = [ path for path in self . results if Path ( path ) . suffix in plot_file_format ] if plottable_images : dst_crs = \"EPSG:4326\" filepaths : List [ Path ] = self . results try : feature_names = df [ name_column ] . to_list () except KeyError : feature_names = [ \"\" for i in range ( df . shape [ 0 ])] for idx , ( raster_fp , feature_name ) in enumerate ( zip ( filepaths , feature_names ) ): # TODO: Not ideal, streaming images are webmercator, folium requires wgs 84.0 # TODO: Switch to ipyleaflet! # This requires reprojecting on the user pc, not via the api. # Reproject raster and add to map with rasterio . open ( raster_fp ) as src : dst_profile = src . meta . copy () if src . crs == dst_crs : dst_array = src . read ()[: 3 , :, :] minx , miny , maxx , maxy = src . bounds else : transform , width , height = calculate_default_transform ( src . crs , dst_crs , src . width , src . height , * src . bounds ) dst_profile . update ( { \"crs\" : dst_crs , \"transform\" : transform , \"width\" : width , \"height\" : height , } ) with MemoryFile () as memfile : with memfile . open ( ** dst_profile ) as mem : for i in range ( 1 , src . count + 1 ): reproject ( source = rasterio . band ( src , i ), destination = rasterio . band ( mem , i ), src_transform = src . transform , src_crs = src . crs , dst_transform = transform , dst_crs = dst_crs , resampling = Resampling . nearest , ) dst_array = mem . read ()[: 3 , :, :] minx , miny , maxx , maxy = mem . bounds # TODO: Make band configuration available m . add_child ( folium . raster_layers . ImageOverlay ( np . moveaxis ( np . stack ( dst_array ), 0 , 2 ), bounds = [[ miny , minx ], [ maxy , maxx ]], # different order. name = f \"Image { idx + 1 } - { feature_name } \" , ) ) # Collapse layer control with too many features. if df . shape [ 0 ] > 4 : # pylint: disable=simplifiable-if-statement #type: ignore collapsed = True else : collapsed = False folium . LayerControl ( position = \"bottomleft\" , collapsed = collapsed ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m","title":"TODO: Make generic with scene_id column integrated."},{"location":"reference/job/#up42.job.Job.track_status","text":"Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Parameters: Name Type Description Default report_time int The intervall (in seconds) when to query the job status. 30 Source code in up42/job.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def track_status ( self , report_time : int = 30 ) -> str : \"\"\" Continuously gets the job status until job has finished or failed. Internally checks every five seconds for the status, prints the log every time interval given in report_time argument. Args: report_time: The intervall (in seconds) when to query the job status. \"\"\" logger . info ( \"Tracking job status continuously, reporting every %s seconds...\" , report_time , ) status = \"NOT STARTED\" time_asleep = 0 while status != \"SUCCEEDED\" : logger . setLevel ( logging . CRITICAL ) status = self . get_status () logger . setLevel ( logging . INFO ) if status in [ \"NOT STARTED\" , \"PENDING\" , \"RUNNING\" ]: if time_asleep != 0 and time_asleep % report_time == 0 : logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) elif status in [ \"FAILED\" , \"ERROR\" ]: logger . info ( \"Job is %s ! - %s - Printing logs ...\" , status , self . job_id ) self . get_logs ( as_print = True ) raise ValueError ( \"Job has failed! See the above log.\" ) elif status in [ \"CANCELLED\" , \"CANCELLING\" ]: logger . info ( \"Job is %s ! - %s \" , status , self . job_id ) raise ValueError ( \"Job has been cancelled!\" ) elif status == \"SUCCEEDED\" : logger . info ( \"Job finished successfully! - %s \" , self . job_id ) sleep ( 5 ) time_asleep += 5 return status","title":"track_status()"},{"location":"reference/job/#up42.job.Job.upload_results_to_bucket","text":"Uploads the results to a custom google cloud storage bucket. Source code in up42/job.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def upload_results_to_bucket ( self , gs_client , bucket , folder , extension : str = \".tgz\" , version : str = \"v0\" ) -> None : \"\"\"Uploads the results to a custom google cloud storage bucket.\"\"\" download_url = self . _get_download_url () r = requests . get ( download_url ) if self . order_ids != [ \"\" ]: blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . order_ids [ 0 ] + extension )) ) logger . info ( \"Upload job %s results with order_ids to %s ...\" , self . job_id , blob . name ) else : blob = bucket . blob ( str ( Path ( version ) / Path ( folder ) / Path ( self . job_id + extension )) ) logger . info ( \"Upload job %s results to %s ...\" , self . job_id , blob . name ) blob . upload_from_string ( data = r . content , content_type = \"application/octet-stream\" , client = gs_client , ) logger . info ( \"Uploaded!\" )","title":"upload_results_to_bucket()"},{"location":"reference/jobtask/","text":"JobTask class \u00b6 \u00b6 __init__ ( self , auth , project_id , job_id , jobtask_id ) special \u00b6 The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks Source code in up42/jobtask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , auth : Auth , project_id : str , job_id : str , jobtask_id : str , ): \"\"\"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . jobtask_id = jobtask_id self . quicklooks = None self . results = None if self . auth . get_info : self . info = self . _get_info () download_quicklooks ( self , output_directory = None ) \u00b6 Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] The quicklooks filepaths. Source code in up42/jobtask.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Args: output_directory: The file output directory, defaults to the current working directory. Returns: The quicklooks filepaths. \"\"\" if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) quicklooks_ids = response_json [ \"data\" ] out_paths : List [ str ] = [] for ql_id in tqdm ( quicklooks_ids ): out_path = output_directory / f \"quicklook_ { ql_id } \" # No suffix required. out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/ { ql_id } \" ) response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths download_results ( self , output_directory = None ) \u00b6 Downloads and unpacks the jobtask results. Default download to Desktop. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/jobtask.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def download_results ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Downloads and unpacks the jobtask results. Default download to Desktop. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of jobtask %s \" , self . jobtask_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths get_results_json ( self , as_dataframe = False ) \u00b6 Gets the Jobtask results data.json. Parameters: Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe. Source code in up42/jobtask.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Jobtask results data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . auth . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"JobTask"},{"location":"reference/jobtask/#jobtask-class","text":"","title":"JobTask class"},{"location":"reference/jobtask/#up42.jobtask.JobTask","text":"","title":"up42.jobtask.JobTask"},{"location":"reference/jobtask/#up42.jobtask.JobTask.__init__","text":"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks Source code in up42/jobtask.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , auth : Auth , project_id : str , job_id : str , jobtask_id : str , ): \"\"\"The JobTask class provides access to the results and parameters of single Tasks of UP42 Jobs (each Job contains one or multiple Jobtasks, one for each block in the workflow). Public Methods: get_results_json, download_results, download_quicklooks \"\"\" self . auth = auth self . project_id = project_id self . job_id = job_id self . jobtask_id = jobtask_id self . quicklooks = None self . results = None if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_quicklooks","text":"Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] The quicklooks filepaths. Source code in up42/jobtask.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def download_quicklooks ( self , output_directory : Union [ str , Path , None ] = None , ) -> List [ str ]: \"\"\" Downloads quicklooks of the job task to disk. After download, can be plotted via jobtask.plot_quicklooks(). Args: output_directory: The file output directory, defaults to the current working directory. Returns: The quicklooks filepaths. \"\"\" if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) quicklooks_ids = response_json [ \"data\" ] out_paths : List [ str ] = [] for ql_id in tqdm ( quicklooks_ids ): out_path = output_directory / f \"quicklook_ { ql_id } \" # No suffix required. out_paths . append ( str ( out_path )) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/quicklooks/ { ql_id } \" ) response = self . auth . _request ( request_type = \"GET\" , url = url , return_text = False ) with open ( out_path , \"wb\" ) as dst : for chunk in response : dst . write ( chunk ) self . quicklooks = out_paths # pylint: disable=attribute-defined-outside-init return out_paths","title":"download_quicklooks()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.download_results","text":"Downloads and unpacks the jobtask results. Default download to Desktop. Parameters: Name Type Description Default output_directory Optional[Union[str, pathlib.Path]] The file output directory, defaults to the current working directory. None Returns: Type Description List[str] List of the downloaded results' filepaths. Source code in up42/jobtask.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def download_results ( self , output_directory : Union [ str , Path , None ] = None ) -> List [ str ]: \"\"\" Downloads and unpacks the jobtask results. Default download to Desktop. Args: output_directory: The file output directory, defaults to the current working directory. Returns: List of the downloaded results' filepaths. \"\"\" # TODO: Overwrite argument logger . info ( \"Downloading results of jobtask %s \" , self . jobtask_id ) if output_directory is None : output_directory = ( Path . cwd () / f \"project_ { self . auth . project_id } \" / f \"job_ { self . job_id } \" ) else : output_directory = Path ( output_directory ) output_directory . mkdir ( parents = True , exist_ok = True ) logger . info ( \"Download directory: %s \" , str ( output_directory )) download_url = self . _get_download_url () out_filepaths = download_results_from_gcs ( download_url = download_url , output_directory = output_directory , ) self . results = out_filepaths return out_filepaths","title":"download_results()"},{"location":"reference/jobtask/#up42.jobtask.JobTask.get_results_json","text":"Gets the Jobtask results data.json. Parameters: Name Type Description Default as_dataframe bool \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Json of the results, alternatively geodataframe. Source code in up42/jobtask.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_results_json ( self , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Gets the Jobtask results data.json. Args: as_dataframe: \"fc\" for FeatureCollection dict, \"df\" for GeoDataFrame. Returns: Json of the results, alternatively geodataframe. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . auth . project_id } /jobs/ { self . job_id } \" f \"/tasks/ { self . jobtask_id } /outputs/data-json/\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) logger . info ( \"Retrieved %s features.\" , len ( response_json [ \"features\" ])) if as_dataframe : # UP42 results are always in EPSG 4326 df = GeoDataFrame . from_features ( response_json , crs = 4326 ) return df else : return response_json","title":"get_results_json()"},{"location":"reference/project/","text":"Project class \u00b6 \u00b6 __init__ ( self , auth , project_id ) special \u00b6 The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_jobs, get_project_settings, update_project_settings Source code in up42/project.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , auth : Auth , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_jobs, get_project_settings, update_project_settings \"\"\" self . auth = auth self . project_id = project_id if self . auth . get_info : self . info = self . _get_info () create_workflow ( self , name , description = '' , use_existing = False ) \u00b6 Creates a new workflow and returns a workflow object. Parameters: Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns: Type Description Workflow The workflow object. Source code in up42/project.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . info ( \"Getting existing workflows in project ...\" ) logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = Workflow ( self . auth , project_id = self . project_id , workflow_id = workflow_id ) return workflow get_jobs ( self , return_json = False ) \u00b6 Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/project.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs in project %s .\" , len ( jobs_json ), self . project_id , ) if return_json : return jobs_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_json ) ] return jobs get_project_settings ( self ) \u00b6 Gets the project settings. Returns: Type Description List The project settings. Source code in up42/project.py 141 142 143 144 145 146 147 148 149 150 151 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings get_workflows ( self , return_json = False ) \u00b6 Gets all workflows in a project as workflow objects or json. Parameters: Name Type Description Default return_json bool True returns Workflow Objects. False Returns: Type Description Union[List[Workflow], Dict] Workflow objects in the project or alternatively json info of the workflows. Source code in up42/project.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ Workflow ( self . auth , project_id = self . project_id , workflow_id = work [ \"id\" ]) for work in tqdm ( workflows_json ) ] return workflows update_project_settings ( self , max_aoi_size = None , max_concurrent_jobs = None , number_of_images = None ) \u00b6 Updates a project's settings. Parameters: Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images The maximum number of images returned with each job, from 1-20, default 10. None Source code in up42/project.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: The maximum number of images returned with each job, from 1-20, default 10. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . auth . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload )","title":"Project"},{"location":"reference/project/#project-class","text":"","title":"Project class"},{"location":"reference/project/#up42.project.Project","text":"","title":"up42.project.Project"},{"location":"reference/project/#up42.project.Project.__init__","text":"The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_jobs, get_project_settings, update_project_settings Source code in up42/project.py 16 17 18 19 20 21 22 23 24 25 26 27 28 def __init__ ( self , auth : Auth , project_id : str ): \"\"\" The Project class can query all available workflows and spawn new workflows within an UP42 project. Also handles project user settings. Public Methods: create_workflow, get_workflows, get_jobs, get_project_settings, update_project_settings \"\"\" self . auth = auth self . project_id = project_id if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/project/#up42.project.Project.create_workflow","text":"Creates a new workflow and returns a workflow object. Parameters: Name Type Description Default name str Name of the new workflow. required description str Description of the new workflow. '' use_existing bool If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. False Returns: Type Description Workflow The workflow object. Source code in up42/project.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def create_workflow ( self , name : str , description : str = \"\" , use_existing : bool = False ) -> \"Workflow\" : \"\"\" Creates a new workflow and returns a workflow object. Args: name: Name of the new workflow. description: Description of the new workflow. use_existing: If True, instead of creating a new workflow, uses the most recent workflow with the same name & description. Returns: The workflow object. \"\"\" if use_existing : logger . info ( \"Getting existing workflows in project ...\" ) logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . CRITICAL ) existing_workflows = self . get_workflows () logging . getLogger ( \"up42.workflow\" ) . setLevel ( logging . INFO ) matching_workflows = [ workflow for workflow in existing_workflows if workflow . info [ \"name\" ] == name and workflow . info [ \"description\" ] == description ] if matching_workflows : existing_workflow = matching_workflows [ 0 ] logger . info ( \"Using existing workflow: %s , %s .\" , name , existing_workflow . workflow_id , ) return existing_workflow url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" payload = { \"name\" : name , \"description\" : description } response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = payload ) workflow_id = response_json [ \"data\" ][ \"id\" ] logger . info ( \"Created new workflow: %s .\" , workflow_id ) workflow = Workflow ( self . auth , project_id = self . project_id , workflow_id = workflow_id ) return workflow","title":"create_workflow()"},{"location":"reference/project/#up42.project.Project.get_jobs","text":"Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/project.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs in the project as job objects or json. Use Workflow().get_job() to get jobs associated with a specific workflow. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] logger . info ( \"Got %s jobs in project %s .\" , len ( jobs_json ), self . project_id , ) if return_json : return jobs_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_json ) ] return jobs","title":"get_jobs()"},{"location":"reference/project/#up42.project.Project.get_project_settings","text":"Gets the project settings. Returns: Type Description List The project settings. Source code in up42/project.py 141 142 143 144 145 146 147 148 149 150 151 def get_project_settings ( self ) -> List : \"\"\" Gets the project settings. Returns: The project settings. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) project_settings = response_json [ \"data\" ] return project_settings","title":"get_project_settings()"},{"location":"reference/project/#up42.project.Project.get_workflows","text":"Gets all workflows in a project as workflow objects or json. Parameters: Name Type Description Default return_json bool True returns Workflow Objects. False Returns: Type Description Union[List[Workflow], Dict] Workflow objects in the project or alternatively json info of the workflows. Source code in up42/project.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_workflows ( self , return_json : bool = False ) -> Union [ List [ \"Workflow\" ], Dict ]: \"\"\" Gets all workflows in a project as workflow objects or json. Args: return_json: True returns Workflow Objects. Returns: Workflow objects in the project or alternatively json info of the workflows. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) workflows_json = response_json [ \"data\" ] logger . info ( \"Got %s workflows for project %s .\" , len ( workflows_json ), self . project_id ) if return_json : return workflows_json else : workflows = [ Workflow ( self . auth , project_id = self . project_id , workflow_id = work [ \"id\" ]) for work in tqdm ( workflows_json ) ] return workflows","title":"get_workflows()"},{"location":"reference/project/#up42.project.Project.update_project_settings","text":"Updates a project's settings. Parameters: Name Type Description Default max_aoi_size int The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. None max_concurrent_jobs int The maximum number of concurrent jobs, from 1-10, default 1. None number_of_images The maximum number of images returned with each job, from 1-20, default 10. None Source code in up42/project.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 def update_project_settings ( self , max_aoi_size : int = None , max_concurrent_jobs : int = None , number_of_images = None , ) -> None : \"\"\" Updates a project's settings. Args: max_aoi_size: The maximum area of interest geometry size, from 1-1000 sqkm, default 10 sqkm. max_concurrent_jobs: The maximum number of concurrent jobs, from 1-10, default 1. number_of_images: The maximum number of images returned with each job, from 1-20, default 10. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /settings\" payload = [ { \"name\" : \"JOB_QUERY_MAX_AOI_SIZE\" , \"value\" : f \" { 100 if max_aoi_size is None else max_aoi_size } \" , }, { \"name\" : \"MAX_CONCURRENT_JOBS\" , \"value\" : f \" { 10 if max_concurrent_jobs is None else max_concurrent_jobs } \" , }, { \"name\" : \"JOB_QUERY_LIMIT_PARAMETER_MAX_VALUE\" , \"value\" : f \" { 10 if number_of_images is None else number_of_images } \" , }, ] self . auth . _request ( request_type = \"PUT\" , url = url , data = payload ) logger . info ( \"Updated project settings: %s \" , payload )","title":"update_project_settings()"},{"location":"reference/tools/","text":"Tools class \u00b6 \u00b6 __init__ ( self , auth = None ) special \u00b6 The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks Source code in up42/tools.py 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , auth = None ): \"\"\" The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks \"\"\" if auth : self . auth = auth self . quicklooks = None self . results = None draw_aoi ( self ) \u00b6 Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). Source code in up42/tools.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def draw_aoi ( self ): \"\"\" Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). \"\"\" m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m get_block_details ( self , block_id , as_dataframe = False ) \u00b6 Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Parameters: Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Dict A dict of the block details metadata for the specific block. Source code in up42/tools.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . auth . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json get_blocks ( self , block_type = None , basic = True , as_dataframe = False ) \u00b6 Gets a list of all public blocks on the marketplace. Parameters: Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version {block_id : block_name} True as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Union[List[Dict], Dict] A list of the public blocks and their metadata. Optional a simpler version dict. Source code in up42/tools.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ List [ Dict ], Dict ]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting blocks name and id, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json get_example_aoi ( self , location = 'Berlin' , as_dataframe = False ) \u00b6 Gets predefined, small, rectangular example aoi for the selected location. Parameters: Name Type Description Default location str Location, one of Berlin, Washington. 'Berlin' as_dataframe bool Returns a dataframe instead of dict FeatureColletions (default). False Returns: Type Description Union[dict, geopandas.geodataframe.GeoDataFrame] Feature collection json with the selected aoi. Source code in up42/tools.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> Union [ dict , GeoDataFrame ]: \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washington. as_dataframe: Returns a dataframe instead of dict FeatureColletions (default). Returns: Feature collection json with the selected aoi. \"\"\" logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_washington.geojson\" ) else : raise ValueError ( \"Please select one of 'Berlin' or 'Washington' as the \" \"location!\" ) if as_dataframe : df = GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi plot_coverage ( scenes , aoi = None , legend_column = 'scene_id' , figsize = ( 12 , 16 )) staticmethod \u00b6 Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Parameters: Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, results of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. 'scene_id' figsize Matplotlib figure size. (12, 16) Source code in up42/tools.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 @staticmethod def plot_coverage ( scenes : GeoDataFrame , aoi : GeoDataFrame = None , legend_column : str = \"scene_id\" , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Args: scenes: GeoDataFrame of scenes, results of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. figsize: Matplotlib figure size. \"\"\" if legend_column not in scenes . columns : legend_column = None # type: ignore logger . info ( \"Given legend_column name not in scene dataframe, \" \"plotting without legend.\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show () plot_quicklooks ( self , figsize = ( 8 , 8 ), filepaths = None , titles = None ) \u00b6 Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] List of titles for the subplots, optional. None Source code in up42/tools.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def plot_quicklooks ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: List of titles for the subplots, optional. \"\"\" if filepaths is None : if self . quicklooks is None : raise ValueError ( \"You first need to download the quicklooks!\" ) filepaths = self . quicklooks plot_file_format = [ \".jpg\" , \".jpeg\" , \".png\" ] warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , ) plot_results ( self , figsize = ( 8 , 8 ), filepaths = None , titles = None ) \u00b6 Plots the downloaded results data. Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[Union[str, pathlib.Path]] Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] Optional list of titles for the subplots. None Source code in up42/tools.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def plot_results ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ Union [ str , Path ]] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded results data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: Optional list of titles for the subplots. \"\"\" if filepaths is None : if self . results is None : raise ValueError ( \"You first need to download the results!\" ) filepaths = self . results plot_file_format = [ \".tif\" ] # TODO: Add other fileformats. _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , ) read_vector_file ( self , filename = 'aoi.geojson' , as_dataframe = False ) \u00b6 Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters: Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Feature Collection Source code in up42/tools.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs . to_string () != \"EPSG:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__ validate_manifest ( self , path_or_json ) \u00b6 Validates the block manifest, input either manifest json string or filepath. Parameters: Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns: Type Description Dict A dictionary with the validation results and potential validation errors. Example { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } } Source code in up42/tools.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation results and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /validate-schema/block\" response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ]","title":"Tools"},{"location":"reference/tools/#tools-class","text":"","title":"Tools class"},{"location":"reference/tools/#up42.tools.Tools","text":"","title":"up42.tools.Tools"},{"location":"reference/tools/#up42.tools.Tools.__init__","text":"The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks Source code in up42/tools.py 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , auth = None ): \"\"\" The tools class contains functionality that is not bound to a specific UP42 object, e.g. for aoi handling etc., UP42 block information, validatin a block manifest etc. They can be accessed from every object and also from the imported up42 package directly. Public methods: read_vector_file, get_example_aoi, draw_aoi, plot_coverage, plot_quicklooks \"\"\" if auth : self . auth = auth self . quicklooks = None self . results = None","title":"__init__()"},{"location":"reference/tools/#up42.tools.Tools.draw_aoi","text":"Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). Source code in up42/tools.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def draw_aoi ( self ): \"\"\" Displays an interactive map to draw an aoi by hand, returns the folium object if not run in a Jupyter notebook. Export the drawn aoi via the export button, then read the geometries via read_aoi_file(). \"\"\" m = folium_base_map ( layer_control = True ) DrawFoliumOverride ( export = True , filename = \"aoi.geojson\" , position = \"topleft\" , draw_options = { \"rectangle\" : { \"repeatMode\" : False , \"showArea\" : True }, \"polygon\" : { \"showArea\" : True , \"allowIntersection\" : False }, \"polyline\" : False , \"circle\" : False , \"marker\" : False , \"circlemarker\" : False , }, edit_options = { \"polygon\" : { \"allowIntersection\" : False }}, ) . add_to ( m ) try : assert get_ipython () is not None display ( m ) except ( AssertionError , NameError ): logger . info ( \"Returning folium map object. To display it directly run in a \" \"Jupyter notebook!\" ) return m","title":"draw_aoi()"},{"location":"reference/tools/#up42.tools.Tools.get_block_details","text":"Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Parameters: Name Type Description Default block_id str The block id. required as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Dict A dict of the block details metadata for the specific block. Source code in up42/tools.py 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def get_block_details ( self , block_id : str , as_dataframe = False ) -> Dict : \"\"\" Gets the detailed information about a specific public block from the server, includes all manifest.json and marketplace.json contents. Args: block_id: The block id. as_dataframe: Returns a dataframe instead of json (default). Returns: A dict of the block details metadata for the specific block. \"\"\" if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks/ { block_id } \" # public blocks response_json = self . auth . _request ( request_type = \"GET\" , url = url ) details_json = response_json [ \"data\" ] if as_dataframe : return pd . DataFrame . from_dict ( details_json , orient = \"index\" ) . transpose () else : return details_json","title":"get_block_details()"},{"location":"reference/tools/#up42.tools.Tools.get_blocks","text":"Gets a list of all public blocks on the marketplace. Parameters: Name Type Description Default block_type Optionally filters to \"data\" or \"processing\" blocks, default None. None basic bool Optionally returns simple version {block_id : block_name} True as_dataframe Returns a dataframe instead of json (default). False Returns: Type Description Union[List[Dict], Dict] A list of the public blocks and their metadata. Optional a simpler version dict. Source code in up42/tools.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def get_blocks ( self , block_type = None , basic : bool = True , as_dataframe = False , ) -> Union [ List [ Dict ], Dict ]: \"\"\" Gets a list of all public blocks on the marketplace. Args: block_type: Optionally filters to \"data\" or \"processing\" blocks, default None. basic: Optionally returns simple version {block_id : block_name} as_dataframe: Returns a dataframe instead of json (default). Returns: A list of the public blocks and their metadata. Optional a simpler version dict. \"\"\" try : block_type = block_type . lower () except AttributeError : pass if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /blocks\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) public_blocks_json = response_json [ \"data\" ] if block_type == \"data\" : logger . info ( \"Getting only data blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"DATA\" ] elif block_type == \"processing\" : logger . info ( \"Getting only processing blocks.\" ) blocks_json = [ block for block in public_blocks_json if block [ \"type\" ] == \"PROCESSING\" ] else : blocks_json = public_blocks_json if basic : logger . info ( \"Getting blocks name and id, use basic=False for all block details.\" ) blocks_basic = { block [ \"name\" ]: block [ \"id\" ] for block in blocks_json } if as_dataframe : return pd . DataFrame . from_dict ( blocks_basic , orient = \"index\" ) else : return blocks_basic else : if as_dataframe : return pd . DataFrame ( blocks_json ) else : return blocks_json","title":"get_blocks()"},{"location":"reference/tools/#up42.tools.Tools.get_example_aoi","text":"Gets predefined, small, rectangular example aoi for the selected location. Parameters: Name Type Description Default location str Location, one of Berlin, Washington. 'Berlin' as_dataframe bool Returns a dataframe instead of dict FeatureColletions (default). False Returns: Type Description Union[dict, geopandas.geodataframe.GeoDataFrame] Feature collection json with the selected aoi. Source code in up42/tools.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_example_aoi ( self , location : str = \"Berlin\" , as_dataframe : bool = False ) -> Union [ dict , GeoDataFrame ]: \"\"\" Gets predefined, small, rectangular example aoi for the selected location. Args: location: Location, one of Berlin, Washington. as_dataframe: Returns a dataframe instead of dict FeatureColletions (default). Returns: Feature collection json with the selected aoi. \"\"\" logger . info ( \"Getting small example aoi in %s .\" , location ) if location == \"Berlin\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_berlin.geojson\" ) elif location == \"Washington\" : example_aoi = self . read_vector_file ( f \" { str ( Path ( __file__ ) . resolve () . parent ) } /data/aoi_washington.geojson\" ) else : raise ValueError ( \"Please select one of 'Berlin' or 'Washington' as the \" \"location!\" ) if as_dataframe : df = GeoDataFrame . from_features ( example_aoi , crs = 4326 ) return df else : return example_aoi","title":"get_example_aoi()"},{"location":"reference/tools/#up42.tools.Tools.plot_coverage","text":"Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Parameters: Name Type Description Default scenes GeoDataFrame GeoDataFrame of scenes, results of catalog.search() required aoi GeoDataFrame GeoDataFrame of aoi. None legend_column str Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. 'scene_id' figsize Matplotlib figure size. (12, 16) Source code in up42/tools.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 @staticmethod def plot_coverage ( scenes : GeoDataFrame , aoi : GeoDataFrame = None , legend_column : str = \"scene_id\" , figsize = ( 12 , 16 ), ) -> None : \"\"\" Plots a coverage map of a dataframe with geometries e.g. the results of catalog.search()) Args: scenes: GeoDataFrame of scenes, results of catalog.search() aoi: GeoDataFrame of aoi. legend_column: Dataframe column set to legend, default is \"scene_id\". Legend entries are sorted and this determines plotting order. figsize: Matplotlib figure size. \"\"\" if legend_column not in scenes . columns : legend_column = None # type: ignore logger . info ( \"Given legend_column name not in scene dataframe, \" \"plotting without legend.\" ) ax = scenes . plot ( legend_column , categorical = True , figsize = figsize , cmap = \"Set3\" , legend = True , alpha = 0.7 , legend_kwds = dict ( loc = \"upper left\" , bbox_to_anchor = ( 1 , 1 )), ) if aoi is not None : aoi . plot ( color = \"r\" , ax = ax , fc = \"None\" , edgecolor = \"r\" , lw = 1 ) # TODO: Add aoi to legend. # from matplotlib.patches import Patch # patch = Patch(label=\"aoi\", facecolor='None', edgecolor='r') # ax.legend(handles=handles, labels=labels) # TODO: Overlay quicklooks on geometry. ax . set_axis_off () plt . show ()","title":"plot_coverage()"},{"location":"reference/tools/#up42.tools.Tools.plot_quicklooks","text":"Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] List of titles for the subplots, optional. None Source code in up42/tools.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def plot_quicklooks ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded quicklooks (filepaths saved to self.quicklooks of the respective object, e.g. job, catalog). Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: List of titles for the subplots, optional. \"\"\" if filepaths is None : if self . quicklooks is None : raise ValueError ( \"You first need to download the quicklooks!\" ) filepaths = self . quicklooks plot_file_format = [ \".jpg\" , \".jpeg\" , \".png\" ] warnings . filterwarnings ( \"ignore\" , category = rasterio . errors . NotGeoreferencedWarning ) _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , )","title":"plot_quicklooks()"},{"location":"reference/tools/#up42.tools.Tools.plot_results","text":"Plots the downloaded results data. Parameters: Name Type Description Default figsize Tuple[int, int] matplotlib figure size. (8, 8) filepaths List[Union[str, pathlib.Path]] Paths to images to plot. Optional, by default picks up the last downloaded results. None titles List[str] Optional list of titles for the subplots. None Source code in up42/tools.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def plot_results ( self , figsize : Tuple [ int , int ] = ( 8 , 8 ), filepaths : List [ Union [ str , Path ]] = None , titles : List [ str ] = None , ) -> None : \"\"\" Plots the downloaded results data. Args: figsize: matplotlib figure size. filepaths: Paths to images to plot. Optional, by default picks up the last downloaded results. titles: Optional list of titles for the subplots. \"\"\" if filepaths is None : if self . results is None : raise ValueError ( \"You first need to download the results!\" ) filepaths = self . results plot_file_format = [ \".tif\" ] # TODO: Add other fileformats. _plot_images ( plot_file_format = plot_file_format , figsize = figsize , filepaths = filepaths , titles = titles , )","title":"plot_results()"},{"location":"reference/tools/#up42.tools.Tools.read_vector_file","text":"Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Parameters: Name Type Description Default filename str File path of the vector file. 'aoi.geojson' as_dataframe bool Return type, default FeatureCollection, GeoDataFrame if True. False Returns: Type Description Union[Dict, geopandas.geodataframe.GeoDataFrame] Feature Collection Source code in up42/tools.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def read_vector_file ( self , filename : str = \"aoi.geojson\" , as_dataframe : bool = False ) -> Union [ Dict , GeoDataFrame ]: \"\"\" Reads vector files (geojson, shapefile, kml, wkt) to a feature collection, for use as the aoi geometry in the workflow input parameters (see get_input_parameters). Example aoi fiels are provided, e.g. example/data/aoi_Berlin.geojson Args: filename: File path of the vector file. as_dataframe: Return type, default FeatureCollection, GeoDataFrame if True. Returns: Feature Collection \"\"\" suffix = Path ( filename ) . suffix if suffix == \".kml\" : gpd . io . file . fiona . drvsupport . supported_drivers [ \"KML\" ] = \"rw\" df = gpd . read_file ( filename , driver = \"KML\" ) elif suffix == \".wkt\" : with open ( filename ) as wkt_file : wkt = wkt_file . read () df = pd . DataFrame ({ \"geometry\" : [ wkt ]}) df [ \"geometry\" ] = df [ \"geometry\" ] . apply ( shapely . wkt . loads ) df = GeoDataFrame ( df , geometry = \"geometry\" , crs = 4326 ) else : df = gpd . read_file ( filename ) if df . crs . to_string () != \"EPSG:4326\" : df = df . to_crs ( epsg = 4326 ) df . geometry = df . geometry . buffer ( 0 ) # TODO: Explode multipolygons (if neccessary as union in aoi anyway most often). # TODO: Have both bboxes for each feature and overall? if as_dataframe : return df else : return df . __geo_interface__","title":"read_vector_file()"},{"location":"reference/tools/#up42.tools.Tools.validate_manifest","text":"Validates the block manifest, input either manifest json string or filepath. Parameters: Name Type Description Default path_or_json Union[str, pathlib.Path, Dict] The input manifest, either filepath or json string, see example. required Returns: Type Description Dict A dictionary with the validation results and potential validation errors. Example { \"_up42_specification_version\" : 2 , \"name\" : \"sharpening\" , \"type\" : \"processing\" , \"tags\" : [ \"imagery\" , \"processing\" ], \"display_name\" : \"Sharpening Filter\" , \"description\" : \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\" , \"parameters\" : { \"strength\" : { \"type\" : \"string\" , \"default\" : \"medium\" } }, \"machine\" : { \"type\" : \"large\" }, \"input_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" } } }, \"output_capabilities\" : { \"raster\" : { \"up42_standard\" : { \"format\" : \"GTiff\" , \"bands\" : \">\" , \"sensor\" : \">\" , \"resolution\" : \">\" , \"dtype\" : \">\" , \"processing_level\" : \">\" } } } } Source code in up42/tools.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def validate_manifest ( self , path_or_json : Union [ str , Path , Dict ]) -> Dict : \"\"\" Validates the block manifest, input either manifest json string or filepath. Args: path_or_json: The input manifest, either filepath or json string, see example. Returns: A dictionary with the validation results and potential validation errors. Example: ```json { \"_up42_specification_version\": 2, \"name\": \"sharpening\", \"type\": \"processing\", \"tags\": [ \"imagery\", \"processing\" ], \"display_name\": \"Sharpening Filter\", \"description\": \"This block enhances the sharpness of a raster image by applying an unsharp mask filter algorithm.\", \"parameters\": { \"strength\": {\"type\": \"string\", \"default\": \"medium\"} }, \"machine\": { \"type\": \"large\" }, \"input_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\" } } }, \"output_capabilities\": { \"raster\": { \"up42_standard\": { \"format\": \"GTiff\", \"bands\": \">\", \"sensor\": \">\", \"resolution\": \">\", \"dtype\": \">\", \"processing_level\": \">\" } } } } ``` \"\"\" if isinstance ( path_or_json , ( str , Path )): with open ( path_or_json ) as src : manifest_json = json . load ( src ) else : manifest_json = path_or_json if not hasattr ( self , \"auth\" ): raise Exception ( \"Requires authentication with UP42, use up42.authenticate()!\" ) url = f \" { self . auth . _endpoint () } /validate-schema/block\" response_json = self . auth . _request ( request_type = \"POST\" , url = url , data = manifest_json ) logger . info ( \"The manifest is valid.\" ) return response_json [ \"data\" ]","title":"validate_manifest()"},{"location":"reference/workflow/","text":"Workflow class \u00b6 \u00b6 __init__ ( self , auth , project_id , workflow_id ) special \u00b6 The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, test_job, run_job, get_jobs, update_name, delete Source code in up42/workflow.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , auth : Auth , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, test_job, run_job, get_jobs, update_name, delete \"\"\" self . auth = auth self . project_id = project_id self . workflow_id = workflow_id if self . auth . get_info : self . info = self . _get_info () add_workflow_tasks ( self , input_tasks ) \u00b6 Adds or overwrites workflow tasks in a workflow on UP42. Parameters: Name Type Description Default input_tasks Union[List[str], List[Dict]] The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the marketplace . required Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Example input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] input_tasks = [ \"sobloo-s2-l1c-aoiclipped\" , \"tiling\" ] ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] Optional: The input_tasks can also be provided as the full, detailed workflow task definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Example: input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] Source code in up42/workflow.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def add_workflow_tasks ( self , input_tasks : Union [ List [ str ], List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks in a workflow on UP42. Args: input_tasks: The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the [marketplace](https://marketplace.up42.com). !!! Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` ```python input_tasks = [\"sobloo-s2-l1c-aoiclipped\", \"tiling\"] ``` ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] Optional: The input_tasks can also be provided as the full, detailed workflow task definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . auth . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks ) construct_parameters ( self , geometry = None , geometry_operation = None , handle_multiple_features = 'footprint' , start_date = None , end_date = None , limit = None , scene_ids = None , order_ids = None ) \u00b6 Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters: Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, geojson.geometry.Polygon, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon, shapely.geometry.point.Point]] One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns: Type Description Dict Dictionary of constructed input parameters. Source code in up42/workflow.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def construct_parameters ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , geojson_Polygon , List , GeoDataFrame , Polygon , Point , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In case of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , # type: ignore squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters delete ( self ) \u00b6 Deletes the workflow and sets the Python object to None. Source code in up42/workflow.py 503 504 505 506 507 508 509 510 511 512 513 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self get_compatible_blocks ( self ) \u00b6 Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. Source code in up42/workflow.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True ) . keys ())[ - 1 ] # type: ignore url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. compatible_blocks = { block [ \"name\" ]: block [ \"blockId\" ] for block in compatible_blocks } return compatible_blocks get_jobs ( self , return_json = False ) \u00b6 Get all jobs associated with the workflow as job objects or json. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/workflow.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs associated with the workflow as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] jobs_workflow_json = [ j for j in jobs_json if j [ \"workflowId\" ] == self . workflow_id ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_workflow_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_workflow_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_workflow_json ) ] return jobs get_parameters_info ( self ) \u00b6 Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Type Description Dict Workflow parameters info json. Source code in up42/workflow.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def get_parameters_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameters info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info get_workflow_tasks ( self , basic = False ) \u00b6 Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters: Name Type Description Default basic bool If selected returns a simplified task-name : task-id` version. False Returns: Type Description Union[List, Dict] The workflow task info. Source code in up42/workflow.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return { task [ \"name\" ]: task [ \"id\" ] for task in tasks } else : return tasks run_job ( self , input_parameters = None , track_status = False , name = None ) \u00b6 Creates and runs a new job. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned job object. Source code in up42/workflow.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , track_status = track_status , name = name ) test_job ( self , input_parameters = None , track_status = False , name = None ) \u00b6 Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned test job object. Source code in up42/workflow.py 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 def test_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned test job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , test_job = True , track_status = track_status , name = name , ) update_name ( self , name = None , description = None ) \u00b6 Updates the workflow name and description. Parameters: Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None Source code in up42/workflow.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update )","title":"Workflow"},{"location":"reference/workflow/#workflow-class","text":"","title":"Workflow class"},{"location":"reference/workflow/#up42.workflow.Workflow","text":"","title":"up42.workflow.Workflow"},{"location":"reference/workflow/#up42.workflow.Workflow.__init__","text":"The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, test_job, run_job, get_jobs, update_name, delete Source code in up42/workflow.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , auth : Auth , project_id : str , workflow_id : str ): \"\"\" The Workflow class can query all available and spawn new jobs for an UP42 Workflow and helps to find and set the the workflow tasks, parameters and aoi. Public Methods: get_compatible_blocks, get_workflow_tasks, add_workflow_tasks, get_parameters_info, construct_parameters, test_job, run_job, get_jobs, update_name, delete \"\"\" self . auth = auth self . project_id = project_id self . workflow_id = workflow_id if self . auth . get_info : self . info = self . _get_info ()","title":"__init__()"},{"location":"reference/workflow/#up42.workflow.Workflow.add_workflow_tasks","text":"Adds or overwrites workflow tasks in a workflow on UP42. Parameters: Name Type Description Default input_tasks Union[List[str], List[Dict]] The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the marketplace . required Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Example input_tasks_simple = [ 'a2daaab4-196d-4226-a018-a810444dcad1' , '4ed70368-d4e1-4462-bef6-14e768049471' ] input_tasks = [ \"sobloo-s2-l1c-aoiclipped\" , \"tiling\" ] ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] Optional: The input_tasks can also be provided as the full, detailed workflow task definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Example: input_tasks_full = [{ 'name' : 'sobloo-s2-l1c-aoiclipped:1' , 'parentName' : None , 'blockId' : 'a2daaab4-196d-4226-a018-a810444dcad1' }, { 'name' : 'sharpening:1' , 'parentName' : 'sobloo-s2-l1c-aoiclipped' , 'blockId' : '4ed70368-d4e1-4462-bef6-14e768049471' }] Source code in up42/workflow.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def add_workflow_tasks ( self , input_tasks : Union [ List [ str ], List [ Dict ]]) -> None : \"\"\" Adds or overwrites workflow tasks in a workflow on UP42. Args: input_tasks: The input tasks, specifying the blocks. Can be a list of the block ids, block names or block display names (The name shown on the [marketplace](https://marketplace.up42.com). !!! Info Using block ids specifies a specific version of the block that will be added to the workflow. With block names or block display names, the most recent version of a block will always be added. Example: ```python input_tasks_simple = ['a2daaab4-196d-4226-a018-a810444dcad1', '4ed70368-d4e1-4462-bef6-14e768049471'] ``` ```python input_tasks = [\"sobloo-s2-l1c-aoiclipped\", \"tiling\"] ``` ```python input_tasks = [\"Sentinel-2 L1C MSI AOI clipped\", \"Raster Tiling\"] Optional: The input_tasks can also be provided as the full, detailed workflow task definition (dict of block id, block name and parent block name). Always use :1 to be able to identify the order when two times the same workflow task is used. The name is arbitrary, but best use the block name. Example: ```python input_tasks_full = [{'name': 'sobloo-s2-l1c-aoiclipped:1', 'parentName': None, 'blockId': 'a2daaab4-196d-4226-a018-a810444dcad1'}, {'name': 'sharpening:1', 'parentName': 'sobloo-s2-l1c-aoiclipped', 'blockId': '4ed70368-d4e1-4462-bef6-14e768049471'}] ``` \"\"\" # Construct proper task definition from simplified input. if isinstance ( input_tasks [ 0 ], str ) and not isinstance ( input_tasks [ 0 ], dict ): input_tasks = self . _construct_full_workflow_tasks_dict ( input_tasks ) url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks/\" ) self . auth . _request ( request_type = \"POST\" , url = url , data = input_tasks ) logger . info ( \"Added tasks to workflow: %r \" , input_tasks )","title":"add_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.construct_parameters","text":"Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Parameters: Name Type Description Default geometry Optional[Union[Dict, geojson.feature.Feature, geojson.feature.FeatureCollection, geojson.geometry.Polygon, List, geopandas.geodataframe.GeoDataFrame, shapely.geometry.polygon.Polygon, shapely.geometry.point.Point]] One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. None geometry_operation Optional[str] Desired operation, One of \"bbox\", \"intersects\", \"contains\". None limit int Maximum number of expected results. None start_date str Query period starting day, format \"2020-01-01\". None end_date str Query period ending day, format \"2020-01-01\". None scene_ids List List of scene_ids, if given ignores all other parameters except geometry. None order_ids List[str] Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. None Returns: Type Description Dict Dictionary of constructed input parameters. Source code in up42/workflow.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def construct_parameters ( self , geometry : Optional [ Union [ Dict , Feature , FeatureCollection , geojson_Polygon , List , GeoDataFrame , Polygon , Point , ] ] = None , geometry_operation : Optional [ str ] = None , handle_multiple_features : str = \"footprint\" , start_date : str = None , # TODO: Other format? More time options? end_date : str = None , limit : int = None , scene_ids : List = None , order_ids : List [ str ] = None , ) -> Dict : \"\"\" Constructs workflow input parameters with a specified aoi, the default input parameters, and optionally limit and order-ids. Further parameter editing needs to be done manually via dict.update({key:value}). Args: geometry: One of Dict, FeatureCollection, Feature, List, GeoDataFrame, shapely.geometry.Polygon, shapely.geometry.Point. All assume EPSG 4326. geometry_operation: Desired operation, One of \"bbox\", \"intersects\", \"contains\". limit: Maximum number of expected results. start_date: Query period starting day, format \"2020-01-01\". end_date: Query period ending day, format \"2020-01-01\". scene_ids: List of scene_ids, if given ignores all other parameters except geometry. order_ids: Optional, can be used to incorporate existing bought imagery on UP42 into new workflows. Returns: Dictionary of constructed input parameters. \"\"\" # TODO: Add ipy slide widget option? One for each block. input_parameters = self . _get_default_parameters () data_block_name = list ( input_parameters . keys ())[ 0 ] if order_ids is not None : # Needs to be handled in this function(not run_job) as it is only # relevant for the data block. # TODO: Check for order-id correct schema, should be handled on backend? input_parameters [ data_block_name ] = { \"order_ids\" : order_ids } else : if limit is not None : input_parameters [ data_block_name ][ \"limit\" ] = limit if scene_ids is not None : if not isinstance ( scene_ids , list ): scene_ids = [ scene_ids ] input_parameters [ data_block_name ][ \"ids\" ] = scene_ids input_parameters [ data_block_name ][ \"limit\" ] = len ( scene_ids ) input_parameters [ data_block_name ] . pop ( \"time\" ) # TODO: In case of ids remove all non-relevant parameters. Cleaner. elif start_date is not None and end_date is not None : datetime = f \" { start_date } T00:00:00Z/ { end_date } T00:00:00Z\" input_parameters [ data_block_name ][ \"time\" ] = datetime aoi_fc = any_vector_to_fc ( vector = geometry ,) aoi_feature = fc_to_query_geometry ( fc = aoi_fc , geometry_operation = geometry_operation , # type: ignore squash_multiple_features = handle_multiple_features , ) input_parameters [ data_block_name ][ geometry_operation ] = aoi_feature return input_parameters","title":"construct_parameters()"},{"location":"reference/workflow/#up42.workflow.Workflow.delete","text":"Deletes the workflow and sets the Python object to None. Source code in up42/workflow.py 503 504 505 506 507 508 509 510 511 512 513 def delete ( self ) -> None : \"\"\" Deletes the workflow and sets the Python object to None. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"DELETE\" , url = url , return_text = False ) logger . info ( \"Successfully deleted workflow: %s \" , self . workflow_id ) del self","title":"delete()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_compatible_blocks","text":"Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. Source code in up42/workflow.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def get_compatible_blocks ( self ) -> Dict : \"\"\" Gets all compatible blocks for the current workflow. If the workflow is empty it will provide all data blocks, if the workflow already has workflow tasks, it will provide the compatible blocks for the last workflow task in the workflow. Currently no data blocks can be attached to other data blocks. \"\"\" last_task = list ( self . get_workflow_tasks ( basic = True ) . keys ())[ - 1 ] # type: ignore url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/ { self . workflow_id } /\" f \"compatible-blocks?parentTaskName= { last_task } \" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) compatible_blocks = response_json [ \"data\" ][ \"blocks\" ] # TODO: Plot diagram of current workflow in green, attachable blocks in red. compatible_blocks = { block [ \"name\" ]: block [ \"blockId\" ] for block in compatible_blocks } return compatible_blocks","title":"get_compatible_blocks()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_jobs","text":"Get all jobs associated with the workflow as job objects or json. Parameters: Name Type Description Default return_json bool If true, returns the job info jsons instead of job objects. False Returns: Type Description Union[List[Job], Dict] All job objects as a list, or alternatively the jobs info as json. Source code in up42/workflow.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def get_jobs ( self , return_json : bool = False ) -> Union [ List [ \"Job\" ], Dict ]: \"\"\" Get all jobs associated with the workflow as job objects or json. Args: return_json: If true, returns the job info jsons instead of job objects. Returns: All job objects as a list, or alternatively the jobs info as json. \"\"\" url = f \" { self . auth . _endpoint () } /projects/ { self . project_id } /jobs\" response_json = self . auth . _request ( request_type = \"GET\" , url = url ) jobs_json = response_json [ \"data\" ] jobs_workflow_json = [ j for j in jobs_json if j [ \"workflowId\" ] == self . workflow_id ] logger . info ( \"Got %s jobs for workflow %s in project %s .\" , len ( jobs_workflow_json ), self . workflow_id , self . project_id , ) if return_json : return jobs_workflow_json else : jobs = [ Job ( self . auth , job_id = job [ \"id\" ], project_id = self . project_id ) for job in tqdm ( jobs_workflow_json ) ] return jobs","title":"get_jobs()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_parameters_info","text":"Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Type Description Dict Workflow parameters info json. Source code in up42/workflow.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def get_parameters_info ( self ) -> Dict : \"\"\" Gets infos about the workflow parameters of each block in the workflow to make it easy to construct the desired parameters. Returns: Workflow parameters info json. \"\"\" workflow_parameters_info = {} workflow_tasks = self . get_workflow_tasks () for task in workflow_tasks : task_name = task [ \"name\" ] task_default_parameters = task [ \"block\" ][ \"parameters\" ] workflow_parameters_info [ task_name ] = task_default_parameters return workflow_parameters_info","title":"get_parameters_info()"},{"location":"reference/workflow/#up42.workflow.Workflow.get_workflow_tasks","text":"Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Parameters: Name Type Description Default basic bool If selected returns a simplified task-name : task-id` version. False Returns: Type Description Union[List, Dict] The workflow task info. Source code in up42/workflow.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_workflow_tasks ( self , basic : bool = False ) -> Union [ List , Dict ]: \"\"\" Get the workflow-tasks of the workflow (Blocks in a workflow are called workflow_tasks) Args: basic: If selected returns a simplified task-name : task-id` version. Returns: The workflow task info. \"\"\" url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } /tasks\" ) response_json = self . auth . _request ( request_type = \"GET\" , url = url ) tasks = response_json [ \"data\" ] logger . info ( \"Got %s tasks/blocks in workflow %s .\" , len ( tasks ), self . workflow_id ) if basic : return { task [ \"name\" ]: task [ \"id\" ] for task in tasks } else : return tasks","title":"get_workflow_tasks()"},{"location":"reference/workflow/#up42.workflow.Workflow.run_job","text":"Creates and runs a new job. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned job object. Source code in up42/workflow.py 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def run_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Creates and runs a new job. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , track_status = track_status , name = name )","title":"run_job()"},{"location":"reference/workflow/#up42.workflow.Workflow.test_job","text":"Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Parameters: Name Type Description Default input_parameters Union[Dict, str, pathlib.Path] Either json string of workflow parameters or filepath to json. None track_status bool Automatically attaches workflow.track_status which queries the job status every 30 seconds. False name str The job name. Optional, by default the workflow name is assigned. None Returns: Type Description Job The spawned test job object. Source code in up42/workflow.py 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 def test_job ( self , input_parameters : Union [ Dict , str , Path ] = None , track_status : bool = False , name : str = None , ) -> \"Job\" : \"\"\" Create a run a new test job (Test Query). With this test query you will not be charged with any data or processing credits, but have a preview of the job result. Args: input_parameters: Either json string of workflow parameters or filepath to json. track_status: Automatically attaches workflow.track_status which queries the job status every 30 seconds. name: The job name. Optional, by default the workflow name is assigned. Returns: The spawned test job object. \"\"\" return self . _helper_run_job ( input_parameters = input_parameters , test_job = True , track_status = track_status , name = name , )","title":"test_job()"},{"location":"reference/workflow/#up42.workflow.Workflow.update_name","text":"Updates the workflow name and description. Parameters: Name Type Description Default name str New name of the workflow. None description str New description of the workflow. None Source code in up42/workflow.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 def update_name ( self , name : str = None , description : str = None ) -> None : \"\"\" Updates the workflow name and description. Args: name: New name of the workflow. description: New description of the workflow. \"\"\" properties_to_update = { \"name\" : name , \"description\" : description } url = ( f \" { self . auth . _endpoint () } /projects/ { self . project_id } /workflows/\" f \" { self . workflow_id } \" ) self . auth . _request ( request_type = \"PUT\" , url = url , data = properties_to_update ) logger . info ( \"Updated workflow name: %r \" , properties_to_update )","title":"update_name()"}]}